{"user_input": "What is the role of the OpenAI Client in this project?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "The OpenAI Client is used to showcase how custom models can be integrated with the OpenAI API, demonstrating LLM, Embedding, and Audio OpenAI API while running Huggingface Models in the background.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What OpenAI do?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "OpenAI API is for integrating custom models and it shows LLM, Embedding and Audio using the OpenAI Client.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What LLM do in this project?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "LLM is used in this project to show how custom models can be integrated with OpenAI API, showcasing its functionality alongside Embedding and Audio OpenAI API.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How is Audio utilized in this project?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "Audio is utilized in this project to showcase how custom models can be integrated with the OpenAI API, specifically demonstrating the Audio OpenAI API while running Huggingface Models in the background.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Can you elaborate on how the OpenAI API is utilized in this project to serve custom models, and what types of models are mentioned?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "The OpenAI API is utilized in this project to showcase the integration of custom models, specifically demonstrating the use of LLM, Embedding, and Audio OpenAI API through the OpenAI Client while running Huggingface Models in the background. Any model can be served as long as it conforms to the OpenAI API rules, which can be found at https://platform.openai.com/docs/api-reference/introduction.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How can the Audio OpenAI API be utilized in a project that integrates custom models with the OpenAI API?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "The Audio OpenAI API can be utilized in a project designed to showcase how custom models are integrated with the OpenAI API. This project demonstrates the use of LLM, Embedding, and Audio OpenAI API while running Huggingface Models in the background. Any model can be served as long as it conforms to OpenAI API rules, which can be found at https://platform.openai.com/docs/api-reference/introduction.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How is Audio integrated into the OpenAI API in this project?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "The project is designed to showcase how custom models can be integrated with the OpenAI API, specifically demonstrating the use of Audio OpenAI API alongside LLM and Embedding, while running Huggingface Models in the background.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What does embedding refer to in the context of the OpenAI API?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "In the context of the OpenAI API, embedding refers to one of the functionalities showcased in the project, which integrates custom models with the OpenAI API.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What does LLM stand for in the context of this project?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "In the context of this project, LLM stands for Large Language Model.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Wut is the role of OpenAI Client in this project?", "reference_contexts": ["# Serve Model through OpenAI API\nThis project is desinged to showcase how custom models can be integrated with OpenAI API.\n\nIt is desinged to show LLM, Embedding and Audio OpenAI API using the OpenAI Client while running Huggingface Models in background. \n\nAny model in servable as long as it conforms to OpenAI API rules which can be fined at\n\nhttps://platform.openai.com/docs/api-reference/introduction\n"], "reference": "The OpenAI Client is used to showcase how custom models can be integrated with the OpenAI API, demonstrating LLM, Embedding, and Audio OpenAI API while running Huggingface Models in the background.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of SentenseTransformer in the code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "The SentenceTransformer is used to load models for generating embeddings from text input in the provided code.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the significance of the API_TOKEN in the provided code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "The API_TOKEN is used for authentication in the code. It is retrieved from the environment variable LOCAL_API_TOKEN and is compared against the token provided in the Authorization header of incoming requests. If the token does not match, the request is denied with an unauthorized response.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What Bearer do in this code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "Bearer is used in the code to check the Authorization header for a valid token. If the header is missing or does not start with 'Bearer ', it returns a response indicating a missing or invalid token.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Wht is the purpose of API_TOKEN in the code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "API_TOKEN is used to store the local API token, which is retrieved from the environment variable 'LOCAL_API_TOKEN'. It is utilized in the authentication process to ensure that requests to the API are authorized.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of Bearer token in the authentication process in the code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "The Bearer token is used in the authentication process to verify the identity of the user making the request. In the code, the 'requires_auth' function checks for the presence of an 'Authorization' header that starts with 'Bearer '. If the header is missing or does not start with 'Bearer ', it returns a response indicating a missing or invalid token. The token is then extracted and compared to the expected API token. If the token does not match, a response indicating 'Unauthorized' is returned.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of API_TOKEN in the code and how it is used for authorization?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "API_TOKEN is used in the code to validate the authorization of requests. It is retrieved from the environment variable LOCAL_API_TOKEN, defaulting to 'metacentrum'. In the requires_auth function, the token is checked against the Authorization header of incoming requests. If the token is missing or does not match the API_TOKEN, the request is denied with an appropriate error response.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of SentenceTransformer in the provided code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "In the provided code, SentenceTransformer is used to load and utilize pre-trained models for generating embeddings from input text. The models are stored in a dictionary and are loaded onto the appropriate device (CPU or GPU) for processing.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How is Flask utilized in the provided code context?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "Flask is utilized in the provided code context to create a web application, specifically to define an API endpoint for generating embeddings. The application is initialized with 'app = Flask(__name__)' and includes a route '/v1/embeddings' that handles POST requests to process input data and return generated embeddings.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What are embdings used for in the provided code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "In the provided code, embeddings are generated using a model from the SentenceTransformer library. The '/v1/embeddings' endpoint accepts a POST request with input text and a model ID, and it returns the generated embeddings for the input text.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How is the SentenceTransformer utilized in the provided Flask application?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom sentence_transformers import SentenceTransformer\nfrom functools import wraps\nimport torch\nimport os\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"all-mpnet-base-v1\": \"sentence-transformers/all-mpnet-base-v1\",\n}\nmodels = {\"all-MiniLM-L6-v2\": None, \"all-mpnet-base-v1\": None}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        model = SentenceTransformer(model_name)\n        model = model.to(DEVICE)\n        models[model_id] = model\n\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            return Response(\"Unauthorized\", 403)\n\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/embeddings\", methods=[\"POST\"])\n# @requires_auth\ndef generate_embedding():\n    # Extract data from the request\n    data = request.json\n    text = data.get(\"input\", \"\")\n    model_id = data.get(\"model\", None)\n    if text == \"\":\n        return Response(\"Missing input parameter\", 400)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    if not isinstance(text, list):\n        text = [text]\n    else:\n        text = [str(t).strip() if str(t).strip() else \"NONE\" for t in text]\n\n    # Generate embedding\n    embeddings = model.encode(text)\n    response = {\n        \"object\": \"list\",\n        \"data\": [\n            {\"object\": \"embedding\", \"embedding\": embedding.tolist(), \"index\": i}\n            for i, embedding in enumerate(embeddings)\n        ],\n    }\n\n    # Initial response\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5001)\n"], "reference": "In the provided Flask application, the SentenceTransformer is utilized to generate embeddings from input text. The application loads models defined in the model_dict and uses the selected model to encode the input text into embeddings, which are then returned in the response.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How is the FastLanguageModel utilized in the codebase?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "The FastLanguageModel is utilized in the codebase by attempting to import it from the unsloth package. If the import is successful, it is used to load models for inference. If the import fails, the code reverts to using the AutoModelForCausalLM and AutoTokenizer from the transformers library. The FastLanguageModel is specifically used in the load_models function to load models with the option to load them in 4-bit format.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of Flask in the provided code context?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "Flask is used in the provided code as a web framework to create a web application. It is instantiated with 'app = Flask(__name__)' and is utilized to define routes and handle HTTP requests, such as generating completions and chat completions.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How does Flask facilitate the development of web applications in the provided code context?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "Flask is utilized in the provided code to create a web application that serves as an API for generating text completions. It sets up routes for handling POST requests, such as '/v1/completions/' and '/v1/chat/completions', where it processes incoming JSON data, extracts prompts, and manages model interactions. The application also incorporates logging for monitoring and debugging purposes, and it uses decorators to enforce authentication for API access.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Can you explain the role of FastLanguageModel in the context of loading models for a Flask application?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "FastLanguageModel is used in the Flask application to load models efficiently. If the 'unsloth' module is available, it imports FastLanguageModel and utilizes it to load models with specific parameters, such as 'load_in_4bit=True' for optimized memory usage. If 'unsloth' is not available, the application falls back to using Hugging Face Transformers to load the models.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of AutoModelForCausalLM in the provided code context?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "In the provided code context, AutoModelForCausalLM is used to load a pre-trained language model from the Hugging Face Transformers library. It is utilized in the load_models function to create a model instance that can generate text based on input prompts, allowing for the implementation of a language model server.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the significance of the llm_server.log file in the context of the application?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "The llm_server.log file is used for logging information related to the llm server's operations. It records events such as attempts to use the 'unsloth' library, loading models, input tokens, output tokens, and authorization requests. The logging configuration specifies the format and level of detail for the logs, which helps in monitoring and debugging the application.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the significance of Llama-3.2-3B in the provided code context?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "Llama-3.2-3B is listed in the model_dict as a model identifier, associated with the model name 'unsloth/Llama-3.2-3B-Instruct'. It is part of a collection of models that the application is set to load and utilize for generating responses.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of FastLanguageModel in the code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "FastLanguageModel is used to load and manage language models for inference. If the 'unsloth' module is available, it imports FastLanguageModel to load models with specific configurations; otherwise, it falls back to using Hugging Face Transformers.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the model identifier for Llama-3.2-1B?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "The model identifier for Llama-3.2-1B is \"unsloth/Llama-3.2-1B-Instruct\".", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How does the code utilize Hugging Face Transformers for model loading and generation?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import TextIteratorStreamer\nfrom functools import wraps\nfrom threading import Thread\nfrom datetime import date\nimport sys\nimport torch\nimport json\nimport os\nimport logging\n\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"Llama-3.2-1B\": \"unsloth/Llama-3.2-1B-Instruct\",\n    \"Llama-3.2-3B\": \"unsloth/Llama-3.2-3B-Instruct\",\n}\nmodels = {\"Llama-3.2-1B\": None, \"Llama-3.2-3B\": None}\n\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlogging.basicConfig(\n    filename=\"llm_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"llm server\")\n\ntry:\n    logger.log(logging.INFO, \"Trying to use unsloth\")\n    from unsloth import FastLanguageModel\nexcept ImportError:\n    logger.log(logging.WARNING, \"Reverting to Hugging Face Transformers\")\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        if \"unsloth\" not in sys.modules:\n            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        else:\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                model_name, dtype=None, load_in_4bit=True\n            )\n            model = FastLanguageModel.for_inference(model)\n        models[model_id] = (model, tokenizer)\n\n\ndef generate_stream(prompt, max_tokens, temperature, model_id):\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return \"Invalid model parameter\"\n\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {input_ids['input_ids'].shape[-1]}\")\n\n    streamer = TextIteratorStreamer(\n        tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\n    generate_kwargs = dict(\n        input_ids,\n        streamer=streamer,\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        top_p=0.99,\n        top_k=1000,\n        temperature=temperature,\n        tokenizer=tokenizer,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n    )\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\n    t.start()\n\n    tok_count = 0\n    for token in streamer:\n        tok_count += 1\n        # Yield each token in OpenAI-compatible JSON format\n        yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {'content': token}, 'finish_reason': None} ]})}\\n\\n\"\n\n    logger.info(f\"Output Tokens: {tok_count}\")\n    # Final message to signal end of streaming\n    yield f\"data: {json.dumps({'object': 'chat.completion.chunk','choices': [{'delta': {}, 'finish_reason': 'stop'}]})}\\n\\n\"\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/completions/\", methods=[\"POST\"])\n@requires_auth\ndef generate_completion():\n    # Extract data from the request\n    data = request.json\n    prompt = \"<|begin_of_text|>\" + data.get(\"prompt\", \"\")\n    logger.info(f\"Prompt: {prompt}\")\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    logger.info(f\"Request for {max_tokens} tokens\")\n    model_id = data.get(\"model\", None)\n    logger.info(f\"Model: {model_id}\")\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n    )\n\n    logger.info(f\"Output Tokens: {len(outputs[0][inputs['input_ids'].shape[-1]:])}\")\n    completion = tokenizer.decode(\n        outputs[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"text\": completion,\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\n@app.route(\"/v1/chat/completions\", methods=[\"POST\"])\n@requires_auth\ndef generate_chat_completion():\n    # Extract chat history and max_tokens from the request\n    data = request.json\n    messages = data.get(\"messages\", [])\n    max_tokens = min(data.get(\"max_tokens\", 100), 4096)\n    model_id = data.get(\"model\", None)\n    stream = data.get(\"stream\", False)\n    logprobs = data.get(\"logprobs\", False)\n    temperature = data.get(\"temperature\", 0.7)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model, tokenizer = models.get(model_id, (None, None))\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Prepare prompt from messages (chat history)\n    prompt = \"<|begin_of_text|>\"\n    for i, message in enumerate(messages):\n        role = message.get(\"role\")\n        content = message.get(\"content\")\n        if i == 0 and role != \"system\":\n            today = date.today().strftime(\"%B %d, %Y\")\n            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\nToday Date: {today}\\nYou are a helpful assistant. You can answer questions about any topic.<|eot_id|>\"\n        if role and content:\n            prompt += (\n                f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n {content}<|eot_id|>\"\n            )\n    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    logger.info(f\"Prompt: {prompt}\")\n    logger.info(f\"Request for {max_tokens} tokens\")\n    logger.info(f\"Model: {model_id}\")\n\n    if stream:\n        # Stream the completion in OpenAI-compatible format\n        return Response(\n            generate_stream(prompt, max_tokens, temperature, model_id),\n            content_type=\"text/event-stream\",\n        )\n    # Tokenize and generate response\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    logger.info(f\"Input Tokens: {inputs['input_ids'].shape[-1]}\")\n    outputs = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_new_tokens=max_tokens,\n        do_sample=True,\n        temperature=temperature,\n        top_p=0.99,\n        top_k=1000,\n        tokenizer=tokenizer,\n        renormalize_logits=True,\n        output_scores=True,\n        return_legacy_cache=False,\n        stop_strings=[\"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"],\n        return_dict_in_generate=True,\n    )\n    scores = None\n    if logprobs:\n        scores = {\n            tokenizer.decode(\n                outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ): outputs[\"scores\"][i][\n                0, outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] + i]\n            ]\n            .cpu()\n            .item()\n            for i in range(\n                outputs[\"sequences\"][0].shape[-1] - inputs[\"input_ids\"].shape[-1]\n            )\n        }\n    logger.info(\n        f\"Output Tokens: {len(outputs['sequences'][0][inputs['input_ids'].shape[-1]:])}\"\n    )\n    completion = tokenizer.decode(\n        outputs[\"sequences\"][0][inputs[\"input_ids\"].shape[-1] :],\n        skip_special_tokens=True,\n    )\n\n    # Format response similar to OpenAI's API\n    response = {\n        \"choices\": [\n            {\n                \"message\": {\"role\": \"assistant\", \"content\": completion},\n                \"logprobs\": scores,\n                \"index\": 0,\n                \"finish_reason\": \"length\",\n            }\n        ]\n    }\n    return jsonify(response)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5000)\n"], "reference": "The code attempts to import a model from the 'unsloth' library first. If that fails, it reverts to using Hugging Face Transformers by importing 'AutoModelForCausalLM' and 'AutoTokenizer'. The 'load_models' function loads models specified in 'model_dict' using Hugging Face's methods, initializing them for inference. The 'generate_completion' and 'generate_chat_completion' functions utilize these models to generate responses based on input prompts, tokenizing the input and generating outputs with specified parameters.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the significance of the stt_server.log file in the context of the provided code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "The stt_server.log file is used for logging information and events in the STT server application. It is configured to log messages with a specific format, including the timestamp, logger name, log level, and message content. The logging level is set to INFO, which means that informational messages, warnings, and errors will be recorded. This helps in monitoring the application's behavior and troubleshooting issues.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Can you explain how to jsonify the response in the transcribe_audio function?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "In the transcribe_audio function, the response is jsonified by creating a dictionary with the transcription text and then passing it to the jsonify function. This is done in the line: response = {\"text\": transcription}, followed by return jsonify(response).", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the defult device used for processing in the code, cpu or cuda?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "The default device used for processing in the code is 'cpu' if 'cuda' is not available.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "what openai/whisper-small do in code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "The openai/whisper-small is used in the code as a model for automatic speech recognition. It is loaded from the model_dict and utilized in the transcribe_audio function to transcribe audio files.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of the Bearer token in the authentication process?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "The Bearer token is used in the authentication process to verify the identity of the requester. The code checks for the presence of an Authorization header that starts with 'Bearer '. If the token is missing or invalid, a warning is logged, and a 401 response is returned. If the token does not match the expected API_TOKEN, a 403 response is returned, indicating unauthorized access.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is Flask and how is it used in the code provided for the audio transcription service?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "Flask is a web framework used to create web applications in Python. In the provided code, Flask is used to set up a web server that handles audio transcription requests. The application is initialized with 'app = Flask(__name__)'. It defines a route '/v1/audio/transcriptions' that accepts POST requests, requiring authorization to transcribe audio files. The code also includes functions to load models and handle authentication for incoming requests.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What device is used if cuda not available in the code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "If cuda is not available, the device used in the code is 'cpu'.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Can you explain how the audio transcription process is handled in the provided Flask application code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "In the provided Flask application code, the audio transcription process is handled by the `transcribe_audio` function, which is decorated with the `@requires_auth` decorator to ensure that requests are authenticated. When a POST request is made to the `/v1/audio/transcriptions` endpoint, the function first checks if an audio file is included in the request. If not, it returns a 400 response indicating that no audio file was provided. It then retrieves the model ID from the request form; if the model ID is missing or invalid, it returns appropriate error responses. If the audio file is present, it is saved to a temporary location with a `.wav` suffix. The function then attempts to transcribe the audio file using the specified model and returns the transcription as a JSON response. In case of any errors during transcription, it logs the error and returns a 500 response. Finally, it ensures that the temporary audio file is deleted after processing.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What role does automatic-speech-recognition play in the provided code?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "In the provided code, automatic-speech-recognition is utilized to load a model for transcribing audio files. The model is loaded using the pipeline function from the transformers library, and it is used in the transcribe_audio function to convert audio input into text.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How is the term 'cuda' utilized in the provided code context?", "reference_contexts": ["from flask import Flask, request, jsonify, Response\nfrom transformers import pipeline\nfrom functools import wraps\nimport tempfile\nimport torch\nimport os\nimport logging\n\napp = Flask(__name__)\n\nmodel_dict = {\n    \"whisper-small\": \"openai/whisper-small\",\n}\nmodels = {\n    \"whisper-small\": None,\n}\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nAPI_TOKEN = os.getenv(\"LOCAL_API_TOKEN\", \"metacentrum\")\n\nlogging.basicConfig(\n    filename=\"stt_server.log\",\n    filemode=\"a\",\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=logging.INFO,\n)\n\nlogger = logging.getLogger(\"STT server\")\n\n\ndef load_models():\n    for model_id, model_name in model_dict.items():\n        logger.info(f\"Loading model {model_id} from {model_name}\")\n        model = pipeline(\n            \"automatic-speech-recognition\", model=model_name, device=DEVICE\n        )\n        models[model_id] = model\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        logger.info(f\"Request from: {request.remote_addr}\")\n        auth_header = request.headers.get(\"Authorization\")\n        if not auth_header or not auth_header.startswith(\"Bearer \"):\n            logger.warning(\"Missing or invalid token\")\n            return Response(\"Missing or invalid token\", 401)\n\n        token = auth_header.split(\" \")[1]\n        if token != API_TOKEN:\n            logger.warning(\"Unauthorized\")\n            return Response(\"Unauthorized\", 403)\n\n        logger.info(\"Authorized\")\n        return f(*args, **kwargs)\n\n    return decorated\n\n\n@app.route(\"/v1/audio/transcriptions\", methods=[\"POST\"])\n@requires_auth\ndef transcribe_audio():\n    # Check for audio file in the request\n    if \"file\" not in request.files:\n        return Response(\"No audio file provided\", 400)\n    model_id = request.form.get(\"model\", None)\n    if model_id is None:\n        return Response(\"Missing model parameter\", 400)\n    model = models.get(model_id, None)\n    if model is None:\n        return Response(\"Invalid model parameter\", 400)\n\n    # Save the file to a temporary location\n    audio_file = request.files[\"file\"]\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n        audio_path = temp_audio.name\n        audio_file.save(audio_path)\n\n    try:\n        # Transcribe audio file\n        transcription = model(audio_path)[\"text\"]\n        response = {\n            \"text\": transcription,\n        }\n        return jsonify(response)\n\n    except Exception as e:\n        logger.error(f\"Error transcribing audio: {e}\")\n        return Response(f\"Internal Error\", 500)\n\n    finally:\n        # Cleanup temporary file\n        if os.path.exists(audio_path):\n            os.remove(audio_path)\n\n\nif __name__ == \"__main__\":\n    load_models()\n    app.run(host=\"0.0.0.0\", port=5002)\n"], "reference": "In the provided code context, 'cuda' is used to determine the device on which the models will run. The line 'DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"' checks if a CUDA-capable GPU is available; if so, it sets the device to 'cuda', otherwise, it defaults to 'cpu'. This allows the application to leverage GPU acceleration for model inference when possible.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}

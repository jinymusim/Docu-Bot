{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "from docu_bot.document_loaders.git_document_loader import GitDocumentLoader\n",
    "from docu_bot.document_loaders.zip_document_loader import ZipDocumentLoader\n",
    "from docu_bot.document_loaders.utils import LoadedRepositoriesAndFiles\n",
    "from docu_bot.retrievals.document_retrival import DocumentRetrieval\n",
    "from docu_bot.constants import PROMPTS\n",
    "from docu_bot.stores.utils import LoadedVectorStores, create_vector_store_from_document_loader\n",
    "from docu_bot.stores.docstore import DocumentStore\n",
    "from docu_bot.stores.vectorstore import MultiVectorStore\n",
    "from docu_bot.chat.answer_openai import prepare_retriever, get_documents, rag, RETRIEVAL_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/co_dal.txt', 'source': 'git'}, page_content='Do dat fonetika (Do radku)\\n\\nEarly stopping na validation loss. + Pridat test set\\n\\nKdybychom delali dalsi sloky, tak spojit Unicode a Syllable tokenizer (Dohromady pod 28 000 tokenu) \\na kodovat Syllable s posunem o len(Unicode). Kontext Syllable tokenizrem, aktualni sloka Unicodem.'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/README.md', 'source': 'git'}, page_content='# GPT Czech Poet\\nLearning models for Czech Poet Generation\\n\\n## Getting training data\\nTraining data needs to be separately downloaded from github project `https://github.com/versotym/corpusCzechVerse`  \\nThis project by Institute of Czech Literature, Czech Academy of Sciences, incorporates 1~305 books of poetry.  \\nFor use, all modeling scripts automatically converts data to needed format.  \\n\\n## GPT Models\\nFor learning Czech GPT Models use script `PoetGen\\\\lm_finetune_torch_trainer_api.py`.  \\nThe script allows multiple different types of models and tokenizers.  \\nFor example, if custom tokenizer is used, model will update its embeddings to match new vocabulary.\\n\\n```\\nparser.add_argument(\"--default_hf_model\", default=\"lchaloupsky/czech-gpt2-oscar\", type=str, help=\"Default Model from HF to use\")\\nparser.add_argument(\"--use_default_model\",  default=True, type=bool, help=\"Use Default Model\")\\nparser.add_argument(\"--tokenizer\", default=\\'lchaloupsky/czech-gpt2-oscar\\', type=str, help=\"Tokenizer to use\")\\nparser.add_argument(\"--model_type\",  default=\"base\", type=str, choices=[\"base\", \"secondary_tasks\", \"half\", \"verse\", \"context\", \"year\", \"all\"], help=\"What type of Model is to be constructed\")\\nparser.add_argument(\"--model_path\", default=os.path.abspath(os.path.join(os.path.dirname(__file__), \"Base-Tokenizer-NormalText-gpt-cz-poetry-all-e4e8\")),  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--max_len\", default=1024, type=int, help=\"Max length for tokenizer\")\\nparser.add_argument(\"--context_max_len\", default=8, type=int, help=\"Max length of context for tokenizer\")\\nparser.add_argument(\"--verse_len\", default=[4,6], type=list, help=\"Lengths of verses\")\\n\\n\\nparser.add_argument(\"--prompt_rhyme\", default=True, type=bool, help=\"Rhyme is prompted into training data\")\\nparser.add_argument(\"--prompt_length\", default=True, type=bool, help=\"Verse length is prompted into training data\")\\nparser.add_argument(\"--prompt_ending\", default=True, type=bool, help=\"Ending of Verse is prompted into training data\")\\n\\nparser.add_argument(\"--syllables\", default=False, type=bool, help=\"If inputs should be parsed by syllables\")\\nparser.add_argument(\"--lower_case\", default=True, type=bool, help=\"If to lower case data\")\\n\\nparser.add_argument(\"--mirror_imbed\", default=True, type=bool, help=\"If to mirror input embedding to output ones\")\\n\\nparser.add_argument(\"--val_data_rate\", default=0.05, type=float, help=\"Rate of validation data\")\\n```\\n\\n### Generation\\nEach model can be generated for in standard way by calling `model.model.generate(tokenized_start, **kwarg)`  \\nor by using the Forced Generation function `model.generate_forced(poem_parameters)` that iteratively generates to match rhyme schema.  \\n\\n### Testing Model\\nFor model testing use script `test_load_model.py` that load the model and tests it on empty inputs.\\nScript will generate strophe using both type of model generation and output it to specified file\\n\\n## Validators\\nFor multiple parameters, it\\'s possible to train validators to check if they are followed.  \\nFor this use `train_validator.py` script to train validators for rhyme schemes, meters or years of publishing.  \\n\\n### Model Validation\\nTo validate model using validators, use script `model_validator.py` with your model and validators.  \\nDo not forget to specify if validators used syllables in training.\\n\\n# License\\nCC-BY-SA\\n'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/data_analysis\\\\analysis_loss.py', 'source': 'git'}, page_content='import json\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport os\\n\\nFILENAME_E4E16 = os.path.join(os.path.dirname(__file__), \\'e4e16.txt\\')\\nFILENAME_E0E24 = os.path.join(os.path.dirname(__file__), \\'e0e24.txt\\')\\n\\nwith open(FILENAME_E4E16, \\'r\\') as file_e4e16:\\n    loss_e4e16 = []\\n    epoch_e4e16 = []\\n    lr_e4e16 = []\\n    for line in file_e4e16.readlines():\\n        try:\\n            temp_json = json.loads(line.strip())\\n            loss_e4e16.append(float(temp_json[\\'loss\\']))\\n            epoch_e4e16.append(float(temp_json[\\'epoch\\']))\\n            lr_e4e16.append(float(temp_json[\\'learning_rate\\']))\\n        except:\\n            continue\\n\\nwith open(FILENAME_E0E24, \\'r\\') as file_e4e16:\\n    loss_e0e24 = []\\n    epoch_e0e24 = []\\n    lr_e0e24 = []\\n    for line in file_e4e16.readlines():\\n        try:\\n            temp_json = json.loads(line.strip())\\n            loss_e0e24.append(float(temp_json[\\'loss\\']))\\n            epoch_e0e24.append(float(temp_json[\\'epoch\\']))\\n            lr_e0e24.append(float(temp_json[\\'learning_rate\\']))\\n        except:\\n            continue\\n\\nplt.plot(epoch_e4e16, loss_e4e16, color=\\'red\\', ls=\\'--\\', label=\\'E4E16 Loss - With Language Learning\\')\\nplt.plot(epoch_e0e24, loss_e0e24, color=\\'blue\\', ls=\\':\\', label=\\'E0E24 Loss - Without Language Learning\\')\\nplt.title(\"OUR Tokenize Model Loss Comparision\")\\nplt.yscale(\"log\")\\nplt.legend()\\nplt.show()\\nplt.figure()\\nplt.plot(epoch_e4e16, lr_e4e16, color=\\'red\\', ls=\\'--\\',label=\\'E4E16 Learning Rate - With Language Learning\\')\\nplt.plot(epoch_e0e24, lr_e0e24, color=\\'blue\\', ls=\\':\\',label=\\'E0E24 Learning Rate - Without Language Learning\\')\\nplt.title(\"OUR Tokenize Model Showcase\")\\nplt.legend()\\nplt.show()'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/data_analysis\\\\data_analysis.py', 'source': 'git'}, page_content=\"import csv\\nimport seaborn as sns\\nimport os\\nimport numpy as np\\nimport pandas as pd\\nfrom scipy import stats\\nfrom matplotlib import pyplot as plt\\n\\nFILENAME = os.path.join(os.path.dirname(__file__), 'significance_test_data.csv')\\n\\ndef display_comparision(comparision_header: str, rhyme_data: np.ndarray = None, meter_data: np.ndarray = None, year_data:np.ndarray =None, \\n                        rhyme_base:float = None, rhyme_improved:float = None, meter_base:float = None, meter_improved:float = None, \\n                        year_base:float = None, year_improved:float = None, p_value_checks:list = [1, 5, 25, 50, 75, 95, 99]):\\n    # CMAP\\n    _CMAP = plt.get_cmap('Dark2')\\n    SLICED_CMAP = _CMAP(np.linspace(0, 1, len(p_value_checks) + 1))\\n\\n    # Plot Rhyme Data\\n    if not rhyme_data is  None:\\n        sns.histplot(data=pd.DataFrame({'rhyme_acc': rhyme_data}), x='rhyme_acc').set(title=f'RHYME {comparision_header}')\\n        rhyme_lows_pos = np.percentile(rhyme_data, p_value_checks)\\n        for i, (p_value, value) in enumerate(zip(p_value_checks, rhyme_lows_pos)):\\n            plt.axvline(x=value, color= SLICED_CMAP[i], ls='--', label= f'{p_value} %')\\n        if rhyme_base != None and rhyme_improved != None:\\n            percentiles = stats.percentileofscore(rhyme_data, [rhyme_base, rhyme_improved])\\n            plt.axvline(x=rhyme_base, color='r' , ls=':', label=f'BASE: {percentiles[0]} %', linewidth=5)\\n            plt.axvline(x=rhyme_improved, color='black' , ls=':', label=f'IMPROVED: {percentiles[1]} %', linewidth=5)\\n        plt.legend()\\n        plt.show()\\n        plt.figure()\\n\\n    # Plot Meter Data\\n    if  not meter_data is None:\\n        sns.histplot(data=pd.DataFrame({'meter_acc': meter_data}), x='meter_acc').set(title=f'METER {comparision_header}')\\n        meter_lows_pos = np.percentile(meter_data, p_value_checks)\\n        for i, (p_value, value) in enumerate(zip(p_value_checks, meter_lows_pos)):\\n            plt.axvline(x=value, color= SLICED_CMAP[i], ls='--', label= f'{p_value} %')\\n        if meter_base != None and meter_improved != None:\\n            percentiles = stats.percentileofscore(meter_data, [meter_base, meter_improved])\\n            plt.axvline(x=meter_base, color='r' , ls=':', label=f'BASE: {percentiles[0]} %', linewidth=5)\\n            plt.axvline(x=meter_improved, color='black' , ls=':', label=f'IMPROVED: {percentiles[1]} %', linewidth=5)\\n        plt.legend()\\n        plt.show()\\n        plt.figure()\\n\\n    # Plot Year Data\\n    if not year_data is None:\\n        sns.histplot(data=pd.DataFrame({'year_acc': year_data}), x='year_acc').set(title=f'YEAR {comparision_header}')\\n        year_lows_pos = np.percentile(year_data, p_value_checks)\\n        for i, (p_value, value) in enumerate(zip(p_value_checks, year_lows_pos)):\\n            plt.axvline(x=value, color= SLICED_CMAP[i], ls='--', label= f'{p_value} %')\\n        if year_base != None and year_improved != None:\\n            percentiles = stats.percentileofscore(year_data, [year_base, year_improved])\\n            plt.axvline(x=year_base, color='r' , ls=':', label=f'BASE: {percentiles[0]} %', linewidth=5)\\n            plt.axvline(x=year_improved, color='black' , ls=':', label=f'IMPROVED: {percentiles[1]} %', linewidth=5)\\n        plt.legend()\\n        plt.show()\\n        plt.figure()\\n\\n\\n\\nwith open(FILENAME, 'r') as file:\\n    rows = []\\n    spamreader = csv.reader(file, delimiter=',')\\n    for row in spamreader:\\n        rows.append(row)\\n    # Display RHYME Distil RAW to Disil Syllable \\n    display_comparision(comparision_header= rows[0][1], \\n                        rhyme_data=np.asarray( list(map(float,rows[0][2:])) ), rhyme_base=0.9682, rhyme_improved=0.9689)\\n    # Display RHYME RobeCzech RAW to RobeCzech Syllable \\n    display_comparision(comparision_header= rows[1][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[1][2:])) ), rhyme_base=0.4806, rhyme_improved=0.9468)\\n    # Display RHYME RobeCzech SYLLABLE to Disil Syllable \\n    display_comparision(comparision_header= rows[2][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[2][2:])) ), rhyme_base=0.9468, rhyme_improved=0.9689)\\n    # Display YEAR RobeCzech SYLLABLE to RobeCzech Raw \\n    display_comparision(comparision_header= rows[3][1],\\n                        year_data=np.asarray( list(map(float,rows[3][2:])) ), year_base=0.4255, year_improved=0.4745)\\n    # Display YEAR Roberta Raw to RobeCzech Raw \\n    display_comparision(comparision_header= rows[4][1],\\n                        year_data=np.asarray( list(map(float,rows[4][2:])) ), year_base=0.4315, year_improved=0.4745)\\n    # Display RHYME Roberta Raw Unweighted to Roberta Raw Weighted\\n    display_comparision(comparision_header= rows[5][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[5][2:])) ), rhyme_base=0.9678, rhyme_improved=0.9693)\\n    # Display METER Distil RAW to Disil Syllable \\n    display_comparision(comparision_header= rows[6][1],\\n                        meter_data=np.asarray( list(map(float,rows[6][2:])) ), meter_base=0.8987, meter_improved=0.8952)\\n    # Display METER Distil Syllable to Disil Syllable Context\\n    display_comparision(comparision_header= rows[7][1],\\n                        meter_data=np.asarray( list(map(float,rows[7][2:])) ), meter_base=0.8952, meter_improved=0.9494)\\n    # Display METER Roberta Syllable Context to Disil Syllable Context\\n    display_comparision(comparision_header= rows[8][1],\\n                        meter_data=np.asarray( list(map(float,rows[8][2:])) ), meter_base=0.9434, meter_improved=0.9494)\\n    # Display METER Roberta Raw Unweighted to Roberta Raw Weighted\\n    display_comparision(comparision_header= rows[9][1],\\n                        meter_data=np.asarray( list(map(float,rows[9][2:])) ), meter_base=0.8973, meter_improved=0.8928)\\n        \\n    # Display RHYME and METER Verse-Par input base and pretarined\\n    display_comparision(comparision_header= rows[10][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[10][2:])) ), rhyme_base=0.898, rhyme_improved=0.883, \\n                        meter_data=np.asarray( list(map(float,rows[11][2:])) ), meter_base=0.944, meter_improved=0.946)\\n    # Display RHYME and METER BASIC input base and pretarined\\n    display_comparision(comparision_header= rows[12][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[12][2:])) ), rhyme_base=0.496, rhyme_improved=0.411, \\n                        meter_data=np.asarray( list(map(float,rows[13][2:])) ), meter_base=0.944, meter_improved=0.922)\\n    # Display RHYME BASE tok Basic to Forced Generation\\n    display_comparision(comparision_header= rows[14][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[14][2:])) ), rhyme_base=0.865, rhyme_improved=0.869,\\n                        meter_data=np.asarray( list(map(float,rows[15][2:])) ), meter_base=0.945, meter_improved=0.938)\\n    # Display RHYME OUR tok Basic to Forced Generation\\n    display_comparision(comparision_header= rows[16][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[16][2:])) ), rhyme_base=0.806, rhyme_improved=0.806,\\n                        meter_data=np.asarray( list(map(float,rows[17][2:])) ), meter_base=0.946, meter_improved=0.948)\\n    # Display RHYME SYLLABLE tok Basic to Forced Generation\\n    display_comparision(comparision_header= rows[18][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[18][2:])) ), rhyme_base=0.887, rhyme_improved=0.877)\\n    # Display RHYME BASIC Input to METER\\\\_VERSE Input\\n    display_comparision(comparision_header= rows[19][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[19][2:])) ), rhyme_base=0.496, rhyme_improved=0.868)\\n    # Display RHYME BASE Tok to OUR Tok\\n    display_comparision(comparision_header= rows[20][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[20][2:])) ), rhyme_base=0.869, rhyme_improved=0.806)\\n    # Display RHYME UNICODE Tok Basic to Forced Gen \\n    display_comparision(comparision_header= rows[21][1],\\n                       rhyme_data=np.asarray( list(map(float,rows[21][2:])) ), rhyme_base=0.738, rhyme_improved=0.940)\\n    # Display RHYME BASE Tok to SYLLABLE Tok\\n    display_comparision(comparision_header= rows[22][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[22][2:])) ), rhyme_base=0.869, rhyme_improved=0.877)\\n        \\n    # Display RHYME, METER BASE Tok Base to BASE tok Secondary\\n    display_comparision(comparision_header= rows[23][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[23][2:])) ), rhyme_base=0.868, rhyme_improved=0.865,\\n                        meter_data=np.asarray( list(map(float,rows[24][2:])) ), meter_base=0.946, meter_improved=0.945)\\n    # Display RHYME, METER OUR Tok Base to OUR tok Secondary\\n    display_comparision(comparision_header= rows[25][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[25][2:])) ), rhyme_base=0.815, rhyme_improved=0.806,\\n                        meter_data=np.asarray( list(map(float,rows[26][2:])) ), meter_base=0.945, meter_improved=0.946)\\n    # Display RHYME, METER SYLLABLE Tok Base to SYLLABLE tok Secondary\\n    display_comparision(comparision_header= rows[27][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[27][2:])) ), rhyme_base=0.877, rhyme_improved=0.887,\\n                        meter_data=np.asarray( list(map(float,rows[28][2:])) ), meter_base=0.939, meter_improved=0.946)\\n    # Display RHYME, METER SYLLABLE Tok to UNICODE Tok\\n    display_comparision(comparision_header= rows[29][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[29][2:])) ), rhyme_base=0.877, rhyme_improved=0.940,\\n                        meter_data=np.asarray( list(map(float,rows[30][2:])) ), meter_base=0.942, meter_improved=0.940)\\n    display_comparision(comparision_header= rows[31][1],\\n                        rhyme_data=np.asarray( list(map(float,rows[31][2:])) ), rhyme_base=0.857, rhyme_improved=0.887,)\\n    \"),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/data_analysis\\\\e0e24.txt', 'source': 'git'}, page_content='{\"loss\": 7.0219, \"learning_rate\": 1.464043101428906e-06, \"epoch\": 0.06}\\n{\"loss\": 5.94, \"learning_rate\": 2.928086202857812e-06, \"epoch\": 0.12}\\n{\"loss\": 5.0979, \"learning_rate\": 4.392129304286718e-06, \"epoch\": 0.18}\\n{\"loss\": 4.2553, \"learning_rate\": 5.856172405715624e-06, \"epoch\": 0.23}\\n{\"loss\": 4.0539, \"learning_rate\": 7.32021550714453e-06, \"epoch\": 0.29}\\n{\"loss\": 3.9711, \"learning_rate\": 8.784258608573437e-06, \"epoch\": 0.35}\\n{\"loss\": 3.8907, \"learning_rate\": 1.0248301710002343e-05, \"epoch\": 0.41}\\n{\"loss\": 3.8038, \"learning_rate\": 1.1712344811431248e-05, \"epoch\": 0.47}\\n{\"loss\": 3.7318, \"learning_rate\": 1.3176387912860156e-05, \"epoch\": 0.53}\\n{\"loss\": 3.6648, \"learning_rate\": 1.464043101428906e-05, \"epoch\": 0.59}\\n{\"loss\": 3.6091, \"learning_rate\": 1.610447411571797e-05, \"epoch\": 0.64}\\n{\"loss\": 3.5425, \"learning_rate\": 1.7568517217146874e-05, \"epoch\": 0.7}\\n{\"loss\": 3.5029, \"learning_rate\": 1.9032560318575778e-05, \"epoch\": 0.76}\\n{\"loss\": 3.4577, \"learning_rate\": 2.0496603420004686e-05, \"epoch\": 0.82}\\n{\"loss\": 3.4125, \"learning_rate\": 2.196064652143359e-05, \"epoch\": 0.88}\\n{\"loss\": 3.369, \"learning_rate\": 2.3424689622862496e-05, \"epoch\": 0.94}\\n{\"loss\": 3.3312, \"learning_rate\": 2.4888732724291404e-05, \"epoch\": 1.0}\\n{\"loss\": 3.2718, \"learning_rate\": 2.6352775825720312e-05, \"epoch\": 1.05}\\n{\"loss\": 3.2333, \"learning_rate\": 2.781681892714922e-05, \"epoch\": 1.11}\\n{\"loss\": 3.2061, \"learning_rate\": 2.928086202857812e-05, \"epoch\": 1.17}\\n{\"loss\": 3.1655, \"learning_rate\": 3.0744905130007026e-05, \"epoch\": 1.23}\\n{\"loss\": 3.1445, \"learning_rate\": 3.220894823143594e-05, \"epoch\": 1.29}\\n{\"loss\": 3.1233, \"learning_rate\": 3.367299133286484e-05, \"epoch\": 1.35}\\n{\"loss\": 3.0995, \"learning_rate\": 3.513703443429375e-05, \"epoch\": 1.41}\\n{\"loss\": 3.0803, \"learning_rate\": 3.660107753572266e-05, \"epoch\": 1.46}\\n{\"loss\": 3.0613, \"learning_rate\": 3.8065120637151556e-05, \"epoch\": 1.52}\\n{\"loss\": 3.0362, \"learning_rate\": 3.952916373858047e-05, \"epoch\": 1.58}\\n{\"loss\": 3.032, \"learning_rate\": 4.099320684000937e-05, \"epoch\": 1.64}\\n{\"loss\": 3.0082, \"learning_rate\": 4.245724994143828e-05, \"epoch\": 1.7}\\n{\"loss\": 2.9883, \"learning_rate\": 4.392129304286718e-05, \"epoch\": 1.76}\\n{\"loss\": 2.9786, \"learning_rate\": 4.5385336144296094e-05, \"epoch\": 1.82}\\n{\"loss\": 2.9583, \"learning_rate\": 4.684937924572499e-05, \"epoch\": 1.87}\\n{\"loss\": 2.9476, \"learning_rate\": 4.83134223471539e-05, \"epoch\": 1.93}\\n{\"loss\": 2.9341, \"learning_rate\": 4.977746544858281e-05, \"epoch\": 1.99}\\n{\"loss\": 2.8794, \"learning_rate\": 4.9999371549586935e-05, \"epoch\": 2.05}\\n{\"loss\": 2.8627, \"learning_rate\": 4.9997015466736896e-05, \"epoch\": 2.11}\\n{\"loss\": 2.85, \"learning_rate\": 4.99929117246903e-05, \"epoch\": 2.17}\\n{\"loss\": 2.848, \"learning_rate\": 4.998706061035911e-05, \"epoch\": 2.23}\\n{\"loss\": 2.8358, \"learning_rate\": 4.997946253282231e-05, \"epoch\": 2.28}\\n{\"loss\": 2.8324, \"learning_rate\": 4.997011802329732e-05, \"epoch\": 2.34}\\n{\"loss\": 2.8214, \"learning_rate\": 4.995902773510286e-05, \"epoch\": 2.4}\\n{\"loss\": 2.8128, \"learning_rate\": 4.9946192443613256e-05, \"epoch\": 2.46}\\n{\"loss\": 2.8042, \"learning_rate\": 4.993161304620425e-05, \"epoch\": 2.52}\\n{\"loss\": 2.7995, \"learning_rate\": 4.991529056219023e-05, \"epoch\": 2.58}\\n{\"loss\": 2.7863, \"learning_rate\": 4.989722613275303e-05, \"epoch\": 2.63}\\n{\"loss\": 2.7929, \"learning_rate\": 4.987742102086207e-05, \"epoch\": 2.69}\\n{\"loss\": 2.7721, \"learning_rate\": 4.985587661118607e-05, \"epoch\": 2.75}\\n{\"loss\": 2.7768, \"learning_rate\": 4.98325944099963e-05, \"epoch\": 2.81}\\n{\"loss\": 2.7585, \"learning_rate\": 4.98075760450612e-05, \"epoch\": 2.87}\\n{\"loss\": 2.7677, \"learning_rate\": 4.9780823265532604e-05, \"epoch\": 2.93}\\n{\"loss\": 2.7538, \"learning_rate\": 4.975233794182346e-05, \"epoch\": 2.99}\\n{\"loss\": 2.695, \"learning_rate\": 4.9722122065477025e-05, \"epoch\": 3.04}\\n{\"loss\": 2.6746, \"learning_rate\": 4.9690177749027675e-05, \"epoch\": 3.1}\\n{\"loss\": 2.6859, \"learning_rate\": 4.965650722585318e-05, \"epoch\": 3.16}\\n{\"loss\": 2.6843, \"learning_rate\": 4.9621112850018526e-05, \"epoch\": 3.22}\\n{\"loss\": 2.6879, \"learning_rate\": 4.95839970961114e-05, \"epoch\": 3.28}\\n{\"loss\": 2.6819, \"learning_rate\": 4.9545162559069146e-05, \"epoch\": 3.34}\\n{\"loss\": 2.679, \"learning_rate\": 4.9504611953997315e-05, \"epoch\": 3.4}\\n{\"loss\": 2.6755, \"learning_rate\": 4.9462348115979885e-05, \"epoch\": 3.45}\\n{\"loss\": 2.6666, \"learning_rate\": 4.941837399988102e-05, \"epoch\": 3.51}\\n{\"loss\": 2.6704, \"learning_rate\": 4.937269268013849e-05, \"epoch\": 3.57}\\n{\"loss\": 2.6668, \"learning_rate\": 4.93253073505487e-05, \"epoch\": 3.63}\\n{\"loss\": 2.6587, \"learning_rate\": 4.927622132404345e-05, \"epoch\": 3.69}\\n{\"loss\": 2.6602, \"learning_rate\": 4.922543803245826e-05, \"epoch\": 3.75}\\n{\"loss\": 2.6572, \"learning_rate\": 4.9172961026292457e-05, \"epoch\": 3.81}\\n{\"loss\": 2.6563, \"learning_rate\": 4.911879397446093e-05, \"epoch\": 3.86}\\n{\"loss\": 2.6515, \"learning_rate\": 4.906294066403765e-05, \"epoch\": 3.92}\\n{\"loss\": 2.6522, \"learning_rate\": 4.900540499999085e-05, \"epoch\": 3.98}\\n{\"loss\": 2.5891, \"learning_rate\": 4.894619100491005e-05, \"epoch\": 4.04}\\n{\"loss\": 2.5733, \"learning_rate\": 4.888530281872481e-05, \"epoch\": 4.1}\\n{\"loss\": 2.5793, \"learning_rate\": 4.882274469841527e-05, \"epoch\": 4.16}\\n{\"loss\": 2.5797, \"learning_rate\": 4.8758521017714544e-05, \"epoch\": 4.22}\\n{\"loss\": 2.5792, \"learning_rate\": 4.8692636266802924e-05, \"epoch\": 4.27}\\n{\"loss\": 2.5876, \"learning_rate\": 4.8625095051993955e-05, \"epoch\": 4.33}\\n{\"loss\": 2.5781, \"learning_rate\": 4.855590209541238e-05, \"epoch\": 4.39}\\n{\"loss\": 2.5803, \"learning_rate\": 4.8485062234663983e-05, \"epoch\": 4.45}\\n{\"loss\": 2.5819, \"learning_rate\": 4.84125804224974e-05, \"epoch\": 4.51}\\n{\"loss\": 2.5776, \"learning_rate\": 4.833846172645784e-05, \"epoch\": 4.57}\\n{\"loss\": 2.5779, \"learning_rate\": 4.8262711328532745e-05, \"epoch\": 4.63}\\n{\"loss\": 2.5704, \"learning_rate\": 4.818533452478956e-05, \"epoch\": 4.68}\\n{\"loss\": 2.5728, \"learning_rate\": 4.810633672500543e-05, \"epoch\": 4.74}\\n{\"loss\": 2.5731, \"learning_rate\": 4.802572345228894e-05, \"epoch\": 4.8}\\n{\"loss\": 2.5708, \"learning_rate\": 4.794350034269404e-05, \"epoch\": 4.86}\\n{\"loss\": 2.5732, \"learning_rate\": 4.785967314482594e-05, \"epoch\": 4.92}\\n{\"loss\": 2.5687, \"learning_rate\": 4.777424771943926e-05, \"epoch\": 4.98}\\n{\"loss\": 2.5141, \"learning_rate\": 4.768723003902818e-05, \"epoch\": 5.04}\\n{\"loss\": 2.4955, \"learning_rate\": 4.7598626187409014e-05, \"epoch\": 5.09}\\n{\"loss\": 2.4941, \"learning_rate\": 4.7508442359294716e-05, \"epoch\": 5.15}\\n{\"loss\": 2.4981, \"learning_rate\": 4.741668485986187e-05, \"epoch\": 5.21}\\n{\"loss\": 2.493, \"learning_rate\": 4.7323360104309836e-05, \"epoch\": 5.27}\\n{\"loss\": 2.5056, \"learning_rate\": 4.7228474617412255e-05, \"epoch\": 5.33}\\n{\"loss\": 2.5054, \"learning_rate\": 4.713203503306084e-05, \"epoch\": 5.39}\\n{\"loss\": 2.4978, \"learning_rate\": 4.7034048093801576e-05, \"epoch\": 5.45}\\n{\"loss\": 2.5021, \"learning_rate\": 4.6934520650363346e-05, \"epoch\": 5.5}\\n{\"loss\": 2.4992, \"learning_rate\": 4.6833459661178915e-05, \"epoch\": 5.56}\\n{\"loss\": 2.4977, \"learning_rate\": 4.673087219189849e-05, \"epoch\": 5.62}\\n{\"loss\": 2.4952, \"learning_rate\": 4.6626765414895674e-05, \"epoch\": 5.68}\\n{\"loss\": 2.5052, \"learning_rate\": 4.652114660876605e-05, \"epoch\": 5.74}\\n{\"loss\": 2.5, \"learning_rate\": 4.641402315781829e-05, \"epoch\": 5.8}\\n{\"loss\": 2.5003, \"learning_rate\": 4.6305402551557854e-05, \"epoch\": 5.86}\\n{\"loss\": 2.5013, \"learning_rate\": 4.619529238416341e-05, \"epoch\": 5.91}\\n{\"loss\": 2.4938, \"learning_rate\": 4.6083700353955865e-05, \"epoch\": 5.97}\\n{\"loss\": 2.4502, \"learning_rate\": 4.597063426286014e-05, \"epoch\": 6.03}\\n{\"loss\": 2.4186, \"learning_rate\": 4.5856102015859684e-05, \"epoch\": 6.09}\\n{\"loss\": 2.4244, \"learning_rate\": 4.574011162044384e-05, \"epoch\": 6.15}\\n{\"loss\": 2.4287, \"learning_rate\": 4.562267118604798e-05, \"epoch\": 6.21}\\n{\"loss\": 2.4275, \"learning_rate\": 4.5503788923486524e-05, \"epoch\": 6.27}\\n{\"loss\": 2.4331, \"learning_rate\": 4.538347314437891e-05, \"epoch\": 6.32}\\n{\"loss\": 2.4314, \"learning_rate\": 4.5261732260568465e-05, \"epoch\": 6.38}\\n{\"loss\": 2.4323, \"learning_rate\": 4.5138574783534306e-05, \"epoch\": 6.44}\\n{\"loss\": 2.4377, \"learning_rate\": 4.501400932379626e-05, \"epoch\": 6.5}\\n{\"loss\": 2.4356, \"learning_rate\": 4.4888044590312875e-05, \"epoch\": 6.56}\\n{\"loss\": 2.4388, \"learning_rate\": 4.4760689389872504e-05, \"epoch\": 6.62}\\n{\"loss\": 2.4372, \"learning_rate\": 4.4631952626477614e-05, \"epoch\": 6.68}\\n{\"loss\": 2.4333, \"learning_rate\": 4.450184330072225e-05, \"epoch\": 6.73}\\n{\"loss\": 2.4363, \"learning_rate\": 4.437037050916275e-05, \"epoch\": 6.79}\\n{\"loss\": 2.4386, \"learning_rate\": 4.42375434436818e-05, \"epoch\": 6.85}\\n{\"loss\": 2.4341, \"learning_rate\": 4.410337139084573e-05, \"epoch\": 6.91}\\n{\"loss\": 2.4282, \"learning_rate\": 4.39678637312553e-05, \"epoch\": 6.97}\\n{\"loss\": 2.3924, \"learning_rate\": 4.3831029938889815e-05, \"epoch\": 7.03}\\n{\"loss\": 2.3556, \"learning_rate\": 4.36928795804448e-05, \"epoch\": 7.09}\\n{\"loss\": 2.3587, \"learning_rate\": 4.355342231466309e-05, \"epoch\": 7.14}\\n{\"loss\": 2.3714, \"learning_rate\": 4.34126678916596e-05, \"epoch\": 7.2}\\n{\"loss\": 2.3681, \"learning_rate\": 4.327062615223961e-05, \"epoch\": 7.26}\\n{\"loss\": 2.3729, \"learning_rate\": 4.312730702721076e-05, \"epoch\": 7.32}\\n{\"loss\": 2.3685, \"learning_rate\": 4.298272053668873e-05, \"epoch\": 7.38}\\n{\"loss\": 2.3728, \"learning_rate\": 4.283687678939672e-05, \"epoch\": 7.44}\\n{\"loss\": 2.3794, \"learning_rate\": 4.268978598195867e-05, \"epoch\": 7.5}\\n{\"loss\": 2.3726, \"learning_rate\": 4.254145839818636e-05, \"epoch\": 7.55}\\n{\"loss\": 2.3685, \"learning_rate\": 4.239190440836044e-05, \"epoch\": 7.61}\\n{\"loss\": 2.3747, \"learning_rate\": 4.224113446850539e-05, \"epoch\": 7.67}\\n{\"loss\": 2.3714, \"learning_rate\": 4.2089159119658506e-05, \"epoch\": 7.73}\\n{\"loss\": 2.3794, \"learning_rate\": 4.193598898713288e-05, \"epoch\": 7.79}\\n{\"loss\": 2.3777, \"learning_rate\": 4.178163477977459e-05, \"epoch\": 7.85}\\n{\"loss\": 2.3794, \"learning_rate\": 4.162610728921394e-05, \"epoch\": 7.9}\\n{\"loss\": 2.3802, \"learning_rate\": 4.146941738911103e-05, \"epoch\": 7.96}\\n{\"loss\": 2.3493, \"learning_rate\": 4.131157603439545e-05, \"epoch\": 8.02}\\n{\"loss\": 2.2945, \"learning_rate\": 4.115259426050045e-05, \"epoch\": 8.08}\\n{\"loss\": 2.305, \"learning_rate\": 4.099248318259133e-05, \"epoch\": 8.14}\\n{\"loss\": 2.3094, \"learning_rate\": 4.083125399478838e-05, \"epoch\": 8.2}\\n{\"loss\": 2.3104, \"learning_rate\": 4.06689179693842e-05, \"epoch\": 8.26}\\n{\"loss\": 2.313, \"learning_rate\": 4.050548645605565e-05, \"epoch\": 8.31}\\n{\"loss\": 2.3091, \"learning_rate\": 4.0340970881070306e-05, \"epoch\": 8.37}\\n{\"loss\": 2.3175, \"learning_rate\": 4.017538274648759e-05, \"epoch\": 8.43}\\n{\"loss\": 2.3197, \"learning_rate\": 4.000873362935465e-05, \"epoch\": 8.49}\\n{\"loss\": 2.318, \"learning_rate\": 3.984103518089689e-05, \"epoch\": 8.55}\\n{\"loss\": 2.3211, \"learning_rate\": 3.967229912570346e-05, \"epoch\": 8.61}\\n{\"loss\": 2.3255, \"learning_rate\": 3.950253726090746e-05, \"epoch\": 8.67}\\n{\"loss\": 2.3281, \"learning_rate\": 3.933176145536115e-05, \"epoch\": 8.72}\\n{\"loss\": 2.3287, \"learning_rate\": 3.915998364880623e-05, \"epoch\": 8.78}\\n{\"loss\": 2.3233, \"learning_rate\": 3.898721585103897e-05, \"epoch\": 8.84}\\n{\"loss\": 2.3227, \"learning_rate\": 3.88134701410706e-05, \"epoch\": 8.9}\\n{\"loss\": 2.3271, \"learning_rate\": 3.8638758666282817e-05, \"epoch\": 8.96}\\n{\"loss\": 2.2956, \"learning_rate\": 3.8463093641578454e-05, \"epoch\": 9.02}\\n{\"loss\": 2.2508, \"learning_rate\": 3.828648734852756e-05, \"epoch\": 9.08}\\n{\"loss\": 2.2525, \"learning_rate\": 3.810895213450867e-05, \"epoch\": 9.13}\\n{\"loss\": 2.2645, \"learning_rate\": 3.793050041184555e-05, \"epoch\": 9.19}\\n{\"loss\": 2.2577, \"learning_rate\": 3.775114465693944e-05, \"epoch\": 9.25}\\n{\"loss\": 2.2656, \"learning_rate\": 3.7570897409396693e-05, \"epoch\": 9.31}\\n{\"loss\": 2.2699, \"learning_rate\": 3.738977127115215e-05, \"epoch\": 9.37}\\n{\"loss\": 2.2674, \"learning_rate\": 3.720777890558803e-05, \"epoch\": 9.43}\\n{\"loss\": 2.2674, \"learning_rate\": 3.70249330366486e-05, \"epoch\": 9.49}\\n{\"loss\": 2.2692, \"learning_rate\": 3.6841246447950536e-05, \"epoch\": 9.54}\\n{\"loss\": 2.2687, \"learning_rate\": 3.665673198188924e-05, \"epoch\": 9.6}\\n{\"loss\": 2.2675, \"learning_rate\": 3.6471402538740896e-05, \"epoch\": 9.66}\\n{\"loss\": 2.2728, \"learning_rate\": 3.6285271075760596e-05, \"epoch\": 9.72}\\n{\"loss\": 2.2826, \"learning_rate\": 3.609835060627641e-05, \"epoch\": 9.78}\\n{\"loss\": 2.2783, \"learning_rate\": 3.591065419877957e-05, \"epoch\": 9.84}\\n{\"loss\": 2.2757, \"learning_rate\": 3.5722194976010775e-05, \"epoch\": 9.9}\\n{\"loss\": 2.2782, \"learning_rate\": 3.553298611404279e-05, \"epoch\": 9.95}'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/data_analysis\\\\e4e16.txt', 'source': 'git'}, page_content='{\"loss\": 6.0893, \"learning_rate\": 1.464043101428906e-06, \"epoch\": 0.06}\\n{\"loss\": 3.8404, \"learning_rate\": 2.928086202857812e-06, \"epoch\": 0.12}\\n{\"loss\": 3.2746, \"learning_rate\": 4.392129304286718e-06, \"epoch\": 0.18}\\n{\"loss\": 3.0851, \"learning_rate\": 5.856172405715624e-06, \"epoch\": 0.23}\\n{\"loss\": 3.0191, \"learning_rate\": 7.32021550714453e-06, \"epoch\": 0.29}\\n{\"loss\": 2.9739, \"learning_rate\": 8.784258608573437e-06, \"epoch\": 0.35}\\n{\"loss\": 2.9451, \"learning_rate\": 1.0248301710002343e-05, \"epoch\": 0.41}\\n{\"loss\": 2.9188, \"learning_rate\": 1.1712344811431248e-05, \"epoch\": 0.47}\\n{\"loss\": 2.9068, \"learning_rate\": 1.3176387912860156e-05, \"epoch\": 0.53}\\n{\"loss\": 2.8928, \"learning_rate\": 1.464043101428906e-05, \"epoch\": 0.59}\\n{\"loss\": 2.8886, \"learning_rate\": 1.610447411571797e-05, \"epoch\": 0.64}\\n{\"loss\": 2.8636, \"learning_rate\": 1.7568517217146874e-05, \"epoch\": 0.7}\\n{\"loss\": 2.8627, \"learning_rate\": 1.9032560318575778e-05, \"epoch\": 0.76}\\n{\"loss\": 2.8549, \"learning_rate\": 2.0496603420004686e-05, \"epoch\": 0.82}\\n{\"loss\": 2.8454, \"learning_rate\": 2.196064652143359e-05, \"epoch\": 0.88}\\n{\"loss\": 2.8361, \"learning_rate\": 2.3424689622862496e-05, \"epoch\": 0.94}\\n{\"loss\": 2.8254, \"learning_rate\": 2.4888732724291404e-05, \"epoch\": 1.0}\\n{\"loss\": 2.7885, \"learning_rate\": 2.6352775825720312e-05, \"epoch\": 1.05}\\n{\"loss\": 2.778, \"learning_rate\": 2.781681892714922e-05, \"epoch\": 1.11}\\n{\"loss\": 2.7713, \"learning_rate\": 2.928086202857812e-05, \"epoch\": 1.17}\\n{\"loss\": 2.7492, \"learning_rate\": 3.0744905130007026e-05, \"epoch\": 1.23}\\n{\"loss\": 2.7463, \"learning_rate\": 3.220894823143594e-05, \"epoch\": 1.29}\\n{\"loss\": 2.7431, \"learning_rate\": 3.367299133286484e-05, \"epoch\": 1.35}\\n{\"loss\": 2.7383, \"learning_rate\": 3.513703443429375e-05, \"epoch\": 1.41}\\n{\"loss\": 2.7342, \"learning_rate\": 3.660107753572266e-05, \"epoch\": 1.46}\\n{\"loss\": 2.7312, \"learning_rate\": 3.8065120637151556e-05, \"epoch\": 1.52}\\n{\"loss\": 2.7225, \"learning_rate\": 3.952916373858047e-05, \"epoch\": 1.58}\\n{\"loss\": 2.7303, \"learning_rate\": 4.099320684000937e-05, \"epoch\": 1.64}\\n{\"loss\": 2.7197, \"learning_rate\": 4.245724994143828e-05, \"epoch\": 1.7}\\n{\"loss\": 2.7119, \"learning_rate\": 4.392129304286718e-05, \"epoch\": 1.76}\\n{\"loss\": 2.7138, \"learning_rate\": 4.5385336144296094e-05, \"epoch\": 1.82}\\n{\"loss\": 2.7046, \"learning_rate\": 4.684937924572499e-05, \"epoch\": 1.87}\\n{\"loss\": 2.7052, \"learning_rate\": 4.83134223471539e-05, \"epoch\": 1.93}\\n{\"loss\": 2.7034, \"learning_rate\": 4.977746544858281e-05, \"epoch\": 1.99}\\n{\"loss\": 2.65, \"learning_rate\": 4.999844814068063e-05, \"epoch\": 2.05}\\n{\"loss\": 2.642, \"learning_rate\": 4.9992630335241676e-05, \"epoch\": 2.11}\\n{\"loss\": 2.6387, \"learning_rate\": 4.99824977280984e-05, \"epoch\": 2.17}\\n{\"loss\": 2.6433, \"learning_rate\": 4.996805206857409e-05, \"epoch\": 2.23}\\n{\"loss\": 2.6392, \"learning_rate\": 4.9949295850610184e-05, \"epoch\": 2.28}\\n{\"loss\": 2.6425, \"learning_rate\": 4.99262323123357e-05, \"epoch\": 2.34}\\n{\"loss\": 2.6383, \"learning_rate\": 4.989886543550816e-05, \"epoch\": 2.4}\\n{\"loss\": 2.6352, \"learning_rate\": 4.9867199944826214e-05, \"epoch\": 2.46}\\n{\"loss\": 2.6332, \"learning_rate\": 4.983124130711395e-05, \"epoch\": 2.52}\\n{\"loss\": 2.632, \"learning_rate\": 4.9790995730377054e-05, \"epoch\": 2.58}\\n{\"loss\": 2.6247, \"learning_rate\": 4.974647016273105e-05, \"epoch\": 2.63}\\n{\"loss\": 2.6356, \"learning_rate\": 4.969767229120178e-05, \"epoch\": 2.69}\\n{\"loss\": 2.618, \"learning_rate\": 4.9644610540398275e-05, \"epoch\": 2.75}\\n{\"loss\": 2.627, \"learning_rate\": 4.958729407105832e-05, \"epoch\": 2.81}\\n{\"loss\": 2.6128, \"learning_rate\": 4.9525732778466884e-05, \"epoch\": 2.87}\\n{\"loss\": 2.6247, \"learning_rate\": 4.9459937290747805e-05, \"epoch\": 2.93}\\n{\"loss\": 2.6138, \"learning_rate\": 4.93899189670289e-05, \"epoch\": 2.99}\\n{\"loss\": 2.5517, \"learning_rate\": 4.9315689895480924e-05, \"epoch\": 3.04}\\n{\"loss\": 2.5331, \"learning_rate\": 4.923726289123055e-05, \"epoch\": 3.1}\\n{\"loss\": 2.5461, \"learning_rate\": 4.915465149414803e-05, \"epoch\": 3.16}\\n{\"loss\": 2.5477, \"learning_rate\": 4.906786996650956e-05, \"epoch\": 3.22}\\n{\"loss\": 2.5531, \"learning_rate\": 4.897693329053505e-05, \"epoch\": 3.28}\\n{\"loss\": 2.5494, \"learning_rate\": 4.8881857165801495e-05, \"epoch\": 3.34}\\n{\"loss\": 2.5485, \"learning_rate\": 4.878265800653258e-05, \"epoch\": 3.4}\\n{\"loss\": 2.5473, \"learning_rate\": 4.867935293876489e-05, \"epoch\": 3.45}\\n{\"loss\": 2.5425, \"learning_rate\": 4.857195979739123e-05, \"epoch\": 3.51}\\n{\"loss\": 2.547, \"learning_rate\": 4.8460497123081503e-05, \"epoch\": 3.57}\\n{\"loss\": 2.5455, \"learning_rate\": 4.834498415908185e-05, \"epoch\": 3.63}\\n{\"loss\": 2.5393, \"learning_rate\": 4.82254408478924e-05, \"epoch\": 3.69}\\n{\"loss\": 2.5422, \"learning_rate\": 4.810188782782438e-05, \"epoch\": 3.75}\\n{\"loss\": 2.5418, \"learning_rate\": 4.7974346429437e-05, \"epoch\": 3.81}\\n{\"loss\": 2.5411, \"learning_rate\": 4.784283867185494e-05, \"epoch\": 3.86}\\n{\"loss\": 2.5382, \"learning_rate\": 4.7707387258966854e-05, \"epoch\": 3.92}\\n{\"loss\": 2.5401, \"learning_rate\": 4.7568015575505745e-05, \"epoch\": 3.98}\\n{\"loss\": 2.4758, \"learning_rate\": 4.7424747683011714e-05, \"epoch\": 4.04}\\n{\"loss\": 2.4596, \"learning_rate\": 4.727760831567795e-05, \"epoch\": 4.1}\\n{\"loss\": 2.4661, \"learning_rate\": 4.71266228760805e-05, \"epoch\": 4.16}\\n{\"loss\": 2.468, \"learning_rate\": 4.697181743079274e-05, \"epoch\": 4.22}\\n{\"loss\": 2.4691, \"learning_rate\": 4.68132187058851e-05, \"epoch\": 4.27}\\n{\"loss\": 2.4784, \"learning_rate\": 4.665085408231108e-05, \"epoch\": 4.33}\\n{\"loss\": 2.4707, \"learning_rate\": 4.6484751591180044e-05, \"epoch\": 4.39}\\n{\"loss\": 2.474, \"learning_rate\": 4.631493990891789e-05, \"epoch\": 4.45}\\n{\"loss\": 2.4767, \"learning_rate\": 4.614144835231627e-05, \"epoch\": 4.51}\\n{\"loss\": 2.4727, \"learning_rate\": 4.596430687347121e-05, \"epoch\": 4.57}\\n{\"loss\": 2.4749, \"learning_rate\": 4.578354605461214e-05, \"epoch\": 4.63}\\n{\"loss\": 2.4682, \"learning_rate\": 4.559919710282207e-05, \"epoch\": 4.68}\\n{\"loss\": 2.4715, \"learning_rate\": 4.541129184464987e-05, \"epoch\": 4.74}\\n{\"loss\": 2.4725, \"learning_rate\": 4.521986272061573e-05, \"epoch\": 4.8}\\n{\"loss\": 2.4716, \"learning_rate\": 4.5024942779610424e-05, \"epoch\": 4.86}\\n{\"loss\": 2.4748, \"learning_rate\": 4.4826565673189734e-05, \"epoch\": 4.92}\\n{\"loss\": 2.4706, \"learning_rate\": 4.46247656497647e-05, \"epoch\": 4.98}\\n{\"loss\": 2.4176, \"learning_rate\": 4.441957754868891e-05, \"epoch\": 5.04}\\n{\"loss\": 2.3973, \"learning_rate\": 4.4211036794243685e-05, \"epoch\": 5.09}\\n{\"loss\": 2.3959, \"learning_rate\": 4.3999179389522365e-05, \"epoch\": 5.15}\\n{\"loss\": 2.4011, \"learning_rate\": 4.378404191021461e-05, \"epoch\": 5.21}\\n{\"loss\": 2.3968, \"learning_rate\": 4.356566149829188e-05, \"epoch\": 5.27}\\n{\"loss\": 2.4105, \"learning_rate\": 4.3344075855595104e-05, \"epoch\": 5.33}\\n{\"loss\": 2.4099, \"learning_rate\": 4.311932323732575e-05, \"epoch\": 5.39}\\n{\"loss\": 2.4033, \"learning_rate\": 4.289144244544134e-05, \"epoch\": 5.45}\\n{\"loss\": 2.4087, \"learning_rate\": 4.266047282195654e-05, \"epoch\": 5.5}\\n{\"loss\": 2.4065, \"learning_rate\": 4.2426454242151045e-05, \"epoch\": 5.56}\\n{\"loss\": 2.4052, \"learning_rate\": 4.21894271076854e-05, \"epoch\": 5.62}\\n{\"loss\": 2.4039, \"learning_rate\": 4.194943233962594e-05, \"epoch\": 5.68}\\n{\"loss\": 2.414, \"learning_rate\": 4.170651137138004e-05, \"epoch\": 5.74}\\n{\"loss\": 2.4105, \"learning_rate\": 4.146070614154297e-05, \"epoch\": 5.8}\\n{\"loss\": 2.411, \"learning_rate\": 4.121205908665742e-05, \"epoch\": 5.86}\\n{\"loss\": 2.4118, \"learning_rate\": 4.0960613133887196e-05, \"epoch\": 5.91}\\n{\"loss\": 2.405, \"learning_rate\": 4.070641169360612e-05, \"epoch\": 5.97}\\n{\"loss\": 2.3629, \"learning_rate\": 4.044949865190352e-05, \"epoch\": 6.03}\\n{\"loss\": 2.3323, \"learning_rate\": 4.018991836300765e-05, \"epoch\": 6.09}\\n{\"loss\": 2.339, \"learning_rate\": 3.992771564162819e-05, \"epoch\": 6.15}\\n{\"loss\": 2.3429, \"learning_rate\": 3.966293575521935e-05, \"epoch\": 6.21}\\n{\"loss\": 2.3424, \"learning_rate\": 3.939562441616475e-05, \"epoch\": 6.27}\\n{\"loss\": 2.3488, \"learning_rate\": 3.912582777388545e-05, \"epoch\": 6.32}\\n{\"loss\": 2.3461, \"learning_rate\": 3.8853592406872644e-05, \"epoch\": 6.38}\\n{\"loss\": 2.3479, \"learning_rate\": 3.8578965314646186e-05, \"epoch\": 6.44}\\n{\"loss\": 2.3537, \"learning_rate\": 3.8301993909640454e-05, \"epoch\": 6.5}\\n{\"loss\": 2.3516, \"learning_rate\": 3.8022726009018924e-05, \"epoch\": 6.56}\\n{\"loss\": 2.3557, \"learning_rate\": 3.7741209826418864e-05, \"epoch\": 6.62}\\n{\"loss\": 2.354, \"learning_rate\": 3.745749396362761e-05, \"epoch\": 6.68}\\n{\"loss\": 2.3501, \"learning_rate\": 3.71716274021918e-05, \"epoch\": 6.73}\\n{\"loss\": 2.3539, \"learning_rate\": 3.6883659494961055e-05, \"epoch\": 6.79}\\n{\"loss\": 2.3566, \"learning_rate\": 3.659363995756754e-05, \"epoch\": 6.85}\\n{\"loss\": 2.3525, \"learning_rate\": 3.630161885984296e-05, \"epoch\": 6.91}\\n{\"loss\": 2.3472, \"learning_rate\": 3.6007646617174284e-05, \"epoch\": 6.97}'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/PoetGen\\\\corpus_capsulated_datasets.py', 'source': 'git'}, page_content='import os\\nimport json\\nimport numpy as np\\nimport torch\\nimport random\\n\\nfrom utils.poet_utils import (\\n    StropheParams,  \\n    TextAnalysis, \\n    TextManipulation, \\n    SyllableMaker, \\n    VersologicalMaker, \\n    Tokens, \\n    DEFAULT_STROPHE\\n)\\nfrom torch.utils.data import Dataset\\nfrom transformers import PreTrainedTokenizerBase, PreTrainedModel\\nclass CorpusDatasetPytorch:\\n    \"\"\"Dataset class responsible for data loading.\\n    \"\"\"\\n    \\n    class RawDataset:\\n        \"\"\"Dataset distributing raw sting data with no preprocessing\\n        \"\"\"\\n        def __init__(self, data_file_paths, lower_case:bool = True):\\n            \"\"\"Construct the frame around Raw data generation\\n\\n            Args:\\n                data_file_paths (_type_): list of paths to data files\\n                lower_case (bool, optional): if resulting data should be in lowercase. Defaults to True.\\n            \"\"\"\\n            self._data_file_paths = data_file_paths\\n            self.lower_case = lower_case\\n        \\n        def gen_files(self):\\n            \"\"\"Get individual opened files\\n\\n            Yields:\\n                _type_: open file object\\n            \"\"\"\\n            for filename in self._data_file_paths:\\n                 yield open(filename, \\'r\\') \\n                 \\n        def get_verses(self):\\n            \"\"\"Get lines of text of poetry\\n\\n            Yields:\\n                str: individual verse line\\n            \"\"\"\\n            for step,file in enumerate(self.gen_files()):\\n                if step % 100 == 0:\\n                    print(f\"Processing file {step}\")\\n                datum = json.load(file)\\n                for data_line in datum:\\n                    for part_line in data_line[\\'body\\']:\\n                        for text_line in part_line:\\n                            yield text_line[\\'text\\'].lower() if self.lower_case else text_line[\\'text\\']\\n                            \\n        def get_strophes(self):\\n            \"\"\"Get strophe of poetry\\n\\n            Yields:\\n                str: 1 strophe of poetry\\n            \"\"\"\\n            for step,file in enumerate(self.gen_files()):\\n                if step % 100 == 0:\\n                    print(f\"Processing file {step}\")\\n                datum = json.load(file)\\n                for data_line in datum:\\n                    for part_line in data_line[\\'body\\']:\\n                        part = []\\n                        for text_line in part_line:\\n                            part.append(text_line[\\'text\\'])\\n                        yield \"\\\\n\".join(part).lower() if self.lower_case else \"\\\\n\".join(part)\\n        \\n        def get_poems(self):\\n            \"\"\"Get whole poem\\n\\n            Yields:\\n                str: 1 whole poem\\n            \"\"\"\\n            for step,file in enumerate(self.gen_files()):\\n                if step % 100 == 0:\\n                    print(f\"Processing file {step}\")\\n                datum = json.load(file)\\n                for data_line in datum:\\n                    body = [ f\"{data_line[\\'biblio\\'][\\'p_title\\']}:\" ]\\n                    \\n                    for part_line in data_line[\\'body\\']:\\n                        \\n                        for text_line in part_line:\\n                            body.append(text_line[\\'text\\'])\\n                        body.append(\"\\\\n\")\\n                    yield \"\\\\n\".join(body).lower() if self.lower_case else \"\\\\n\".join(body)\\n    \\n    class StrophesDataset(Dataset):\\n        \"\"\"Dataset of preprocessed strophe\\n\\n        Args:\\n            Dataset (_type_): Dataset is child of torch class for better integration with torch and huggingface\\n        \"\"\"\\n        def __init__(self, data_file_paths, dataset_parameters, segment_type, dataset_part):\\n            \"\"\"Construct the class our given data files path and store variables\\n\\n            Args:\\n                data_file_paths (_type_): list of paths to data files\\n\\n            \"\"\"\\n            self._data_file_paths = data_file_paths\\n            self.params = dataset_parameters\\n\\n            self.seg_type = segment_type\\n\\n            self.dataset_part = dataset_part\\n            self.relevant_indexes = []\\n            if dataset_part == \\'train\\':\\n                self.relevant_indexes = self.params[\\'train_indexes\\']\\n            elif dataset_part == \\'val\\':\\n                self.relevant_indexes = self.params[\\'val_indexes\\']\\n            elif dataset_part == \\'test\\':\\n                self.relevant_indexes = self.params[\\'test_indexes\\']\\n\\n            self.data = []\\n\\n            self.line_constructor = self._construct_line\\n            if segment_type == \\'BASE\\':\\n                self.line_constructor = self._construct_line\\n            elif segment_type == \\'SYLLABLE\\':\\n                self.line_constructor = self._construct_syllable_line\\n            elif segment_type == \\'VERSEMARK\\':\\n                self.line_constructor = self._construct_verse_marks_line\\n        \\n            self.custom_size = 1\\n        \\n        def gen_files(self):\\n            \"\"\"Get individual opened files\\n\\n            Yields:\\n                _type_: open file object\\n            \"\"\"\\n            for filename in self._data_file_paths:\\n                 yield open(filename, \\'r\\')\\n                 \\n                     \\n        \\n        def _construct_line(self, raw_text):\\n            \"\"\"Construct individual content line\\n\\n            Args:\\n                raw_text (str): raw verse line\\n\\n            Returns:\\n                str: Processed verse line with line parameters\\n            \"\"\"\\n            syllables = SyllableMaker.syllabify(\" \".join(raw_text.split()[-4:]))\\n            verse_marks = VersologicalMaker.verse_segmnent(\\'\\'.join([\\'\\'.join(word) for word in syllables[-2:]]))\\n            \\n            if len(syllables[-1]) == 1 and TextAnalysis.is_consonant(verse_marks[-1][-1][-1]) and (len(syllables) == 1 or len(syllables[-2]) > 1):\\n                verse_end = f\"{ TextManipulation._shortify(\\'\\'.join(verse_marks[-1][-1:])) } # \"\\n            else:\\n                verse_end = f\"{ TextManipulation._shortify(\\'\\'.join(verse_marks[-1][-2:])) } # \"\\n            return  verse_end  + raw_text\\n        \\n        def _construct_syllable_line(self, raw_text):\\n            \"\"\"Construct individual content line as sequence of syllables\\n\\n            Args:\\n                raw_text (str): raw verse line\\n\\n            Returns:\\n                str: Processed verse line as sequence of syllables with line parameters\\n            \"\"\"\\n            ending = raw_text[-1] if raw_text[-1] in [\\',\\',\\'.\\',\\'!\\',\\'?\\'] else \\'\\'\\n            syllables = SyllableMaker.syllabify(raw_text)\\n            \\n            verse_end = f\"{ TextManipulation._shortify(\\'\\'.join(syllables[-1][-2:])) } # \"\\n            return  verse_end + \"  \".join([\" \".join(syl) for syl in syllables]) + ending\\n        \\n        def _construct_verse_marks_line(self, raw_text):\\n            \"\"\"Construct individual content line as sequence of verse marks\\n\\n            Args:\\n                raw_text (str): raw verse line\\n\\n            Returns:\\n                str: Processed verse line as sequence of verse marks with line parameters\\n            \"\"\"\\n            ending = raw_text[-1] if raw_text[-1] in [\\',\\',\\'.\\',\\'!\\',\\'?\\'] else \\'\\'\\n            #syllables = SyllableMaker.syllabify(raw_text)\\n            verse_marks = VersologicalMaker.verse_segmnent(raw_text)\\n            verse_end = f\"{ TextManipulation._shortify(\\'\\'.join(verse_marks[-1][-2:])) } # \" \\n            return  verse_end + \"  \".join([\" \".join(syl) for syl in verse_marks]) + ending\\n        \\n        def _create_header(self, author, title, year):\\n            \"\"\"Create header for strophe\\n\\n            Args:\\n                author (str): author of the strophe\\n                title (str): title of the strophe\\n                year (str): year of the strophe\\n\\n            Returns:\\n                str: header of the strophe\\n            \"\"\"\\n            return f\"{Tokens.AUTHOR} {author}\\\\n{Tokens.TITLE} {title}\\\\n{Tokens.YEAR} {year}\\\\n\"\\n        \\n        def _create_summary(self, summary):\\n            \"\"\"Create summary for strophe\\n\\n            Args:\\n                summary (str): summary of the strophe\\n\\n            Returns:\\n                str: summary of the strophe\\n            \"\"\"\\n            return f\"{Tokens.SUMMARY} {summary}\\\\n\"\\n        \\n        def _create_category(self, category):\\n            \"\"\"Create category for strophe\\n\\n            Args:\\n                category (str): category of the strophe\\n\\n            Returns:\\n                str: category of the strophe\\n            \"\"\"\\n            return f\"{Tokens.CATEGORY} {category}\\\\n\"\\n        \\n        def _format_strophe(self, meter:str, rhyme: str, verses: list):\\n            return f\"{Tokens.STROPHE_START}\\\\n{Tokens.METER} {meter}\\\\n{Tokens.RHYME} {rhyme}\\\\n\" + \"\\\\n\".join(verses) + f\"\\\\n{Tokens.STROPHE_END}\\\\n\"\\n                                                           \\n        def data_body_gen(self):\\n            \"\"\"Preprocess and process data for usage\\n            \"\"\"\\n            i=0\\n            for step,file in enumerate(self.gen_files()):\\n                if step % 100 == 0:\\n                    print(f\"Processing file {step}\")\\n                    \\n                datum = json.load(file)\\n                # Check if file in proper indexes\\n                if i in self.relevant_indexes:\\n                    for data_line in datum:\\n\\n                        publish_year_text = TextManipulation._year_bucketor(data_line[\"biblio\"][\"year\"])\\n                        publish_year_true = data_line[\"biblio\"][\"year\"] if TextAnalysis._is_year(data_line[\"biblio\"][\"year\"]) else \\'NaN\\'\\n                        author = data_line[\"p_author\"][\"name\"] if \"p_author\" in data_line.keys() else (data_line[\"b_author\"][\"name\"] if \"b_author\" in data_line.keys() else \"Unknown\")\\n\\n                        poem_header = self._create_header(author, data_line[\"biblio\"][\"p_title\"], publish_year_text)\\n                        \\n                        category = str(data_line[\"categories\"]) if \"categories\" in data_line.keys() else \"\"\\n                        category_header = self._create_category(category.strip())\\n                        \\n                        summary = data_line[\"summarization\"] if \"summarization\" in data_line.keys() else data_line.get(\"sumarization\", \"\")\\n                        summary_header = self._create_summary(summary.strip())\\n                        \\n                        \\n                        previous_strophe = \"\"\\n\\n                        for part_line in data_line[\\'body\\']:                                                     \\n                            body = []\\n                            rhyme= []\\n                            metres = []\\n\\n                            for text_line in part_line:\\n                                # In rare cases multiple, but from searching only 1 metre per line\\n                                try:\\n                                    metres.append(StropheParams.METER_TRANSLATE.get(text_line[\"metre\"][0][\"type\"], \"J\"))\\n                                except:\\n                                    metres.append(StropheParams.METER_TRANSLATE.get(list(text_line[\"metre\"][0].keys())[0], \"J\"))\\n                                \\n                                rhyme.append(text_line[\"rhyme\"])  \\n                                scanned_text = TextManipulation._remove_most_nonchar(text_line[\"text\"], self.params[\\'lower_case\\'])\\n                                body.append(self.line_constructor(scanned_text))\\n\\n                            rhyme_str = TextManipulation._rhyme_string(rhyme)\\n                            meter = max(set(metres), key=metres.count)\\n                            current_strophe = self._format_strophe(meter, rhyme_str, body)\\n                            constructed_strophe = poem_header + category_header + summary_header + previous_strophe + current_strophe  \\n                            self.data.append({\\n                                    \"input_ids\" : constructed_strophe,\\n                                    \"context_ids\" : \"None\",\\n                                    \"year\": publish_year_true,\\n                                    \"rhyme\":  rhyme_str,\\n                                    \"metre_ids\" : metres.copy()\\n                                    })     \\n\\n                            previous_strophe = current_strophe\\n                i+=1            \\n        \\n        def __len__(self):\\n            \"\"\"Return length of training data\\n\\n            Returns:\\n                int: length of training data\\n            \"\"\"\\n            return int(len(self.data) * self.custom_size)\\n        \\n        def __getitem__(self, index):\\n            \"\"\"return indexed item\\n\\n            Args:\\n                index (int): index from where to return\\n\\n            Returns:\\n                dict: dict with indexed data\\n            \"\"\"\\n            return self.data[index]\\n        \\n        def change_custom_size(self,float_size:float = 1):\\n            if float_size > 1:\\n                print(\"Improper size, revert to full size\")\\n                self.custom_size = 1\\n            elif float_size <= 0:\\n                print(\"Size must be positive, revert to full size\")\\n                self.custom_size = 1\\n            else:\\n                self.custom_size = float_size\\n            \\n            np.random.shuffle(self.data)\\n        \\n    class DPODataset(StrophesDataset):\\n        \\n        def data_body_gen(self):\\n            \"\"\"Preprocess and process data for usage\\n            \"\"\"\\n            i=0\\n            previous_poem_strophes = [DEFAULT_STROPHE]\\n            for step,file in enumerate(self.gen_files()):\\n                if step % 100 == 0:\\n                    print(f\"Processing file {step}\")\\n                    \\n                datum = json.load(file)\\n                # Check if file in proper indexes\\n                \\n                if i in self.relevant_indexes:\\n                    \\n                    for data_line in datum:\\n\\n                        publish_year_text = TextManipulation._year_bucketor(data_line[\"biblio\"][\"year\"])\\n                        author = data_line[\"p_author\"][\"name\"] if \"p_author\" in data_line.keys() else (data_line[\"b_author\"][\"name\"] if \"b_author\" in data_line.keys() else \"Unknown\")\\n\\n                        poem_header = self._create_header(author, data_line[\"biblio\"][\"p_title\"], publish_year_text)\\n                        \\n                        category = str(data_line[\"categories\"]) if \"categories\" in data_line.keys() else \"\"\\n                        category_header = self._create_category(category.strip())\\n                        \\n                        summary = data_line[\"summarization\"] if \"summarization\" in data_line.keys() else data_line.get(\"sumarization\", \"\")\\n                        summary_header = self._create_summary(summary.strip())    \\n                        \\n                        previous_strophe = \"\"\\n                        \\n                        poem_strophes = []\\n\\n                        for part_line in data_line[\\'body\\']:                                                     \\n                            body = []\\n                            rhyme= []\\n                            metres = []\\n\\n                            for text_line in part_line:\\n                                # In rare cases multiple, but from searching only 1 metre per line\\n                                try:\\n                                    metres.append(StropheParams.METER_TRANSLATE.get(text_line[\"metre\"][0][\"type\"], \"J\"))\\n                                except:\\n                                    metres.append(StropheParams.METER_TRANSLATE.get(list(text_line[\"metre\"][0].keys())[0], \"J\"))\\n                                \\n                                rhyme.append(text_line[\"rhyme\"])  \\n                                scanned_text = TextManipulation._remove_most_nonchar(text_line[\"text\"], self.params[\\'lower_case\\'])\\n                                body.append(self.line_constructor(scanned_text))\\n\\n                            rhyme_str = TextManipulation._rhyme_string(rhyme)\\n                            meter = max(set(metres), key=metres.count)\\n                            current_strophe = self._format_strophe(meter, rhyme_str, body)\\n                            poem_strophes.append(current_strophe)\\n                            \\n                            random.shuffle(previous_poem_strophes)\\n                            self.data.append({\\n                                    \"prompt\" : poem_header + category_header + summary_header,\\n                                    \"chosen\" :  previous_strophe + current_strophe,\\n                                    \"rejected\" : \"\".join(previous_poem_strophes[:2]),\\n                                    })     \\n\\n                            previous_strophe = current_strophe\\n                            \\n                        previous_poem_strophes = poem_strophes\\n                i+=1            \\n       \\n    @staticmethod\\n    def collate(batch, tokenizer: PreTrainedTokenizerBase ,max_len = 1024, max_context = 1024, format: str = \\'METER_VERSE\\'):\\n        \"\"\"Process data for usage in LM\\n\\n        Args:\\n            batch (_type_): Batch with selected data points\\n            tokenizer (PreTrainedTokenizerBase): tokenizer to tokenize input text\\n            max_len (int, optional): Maximum length of tokenization. Defaults to 1024.\\n            max_context (int, optional): Maximum length of tokenization of context. Defaults to 1024.\\n            mask_rate (float, optional): Rate in with to mask data. Defaults to 0.0.\\n\\n        Returns:\\n            dict: tokenized and processed to tensors data\\n        \"\"\"\\n        \\n        tokenizer.model_max_length = max_len\\n        tokenized = tokenizer([text[\\'input_ids\\'] + tokenizer.eos_token for text in batch],return_tensors=\\'pt\\', truncation=True, padding=True)\\n        input_ids = tokenized[\\'input_ids\\']\\n        attention = tokenized[\"attention_mask\"]\\n    \\n        \\n        nums = None\\n        if \"nums\" in batch[0].keys():\\n            nums = torch.tensor(np.asarray([text[\\'nums\\'] for text in batch], dtype=np.int32), dtype=torch.float32)\\n            \\n        rhyme=None\\n        if \"rhyme\" in batch[0].keys():\\n            rhyme = torch.tensor(np.asarray([TextAnalysis._rhyme_vector(text[\"rhyme\"]) for text in batch], dtype=np.int32), dtype=torch.float32)\\n        \\n        verse_end = None\\n        if \"verse_end\" in batch[0].keys():       \\n            verse_end = torch.tensor(np.asarray([CorpusDatasetPytorch.VersesDataset._ending_vector(text[\"verse_end\"]) for text in batch], dtype=np.int32), dtype=torch.float32)\\n        \\n        year = None\\n        if \"year\" in batch[0].keys():      \\n            year = torch.tensor(np.asarray([TextAnalysis._publish_year_vector(text[\"year\"]) for text in batch], dtype=np.int32), dtype=torch.float32)\\n            \\n        metre = None\\n        if \"metre\" in batch[0].keys():       \\n            metre = torch.tensor(np.asarray([TextAnalysis._metre_vector(text[\"metre\"]) for text in batch], dtype=np.int32), dtype=torch.float32)\\n        \\n        context_ids = None\\n        context_attention_mask = None\\n        if \"context_ids\" in batch[0].keys():\\n            tokenizer.model_max_length = max_context\\n            tokenized_context = tokenizer([text[\\'context_ids\\'] + tokenizer.eos_token  for text in batch],return_tensors=\\'pt\\', truncation=True, padding=True)\\n            context_ids = tokenized_context[\\'input_ids\\']\\n            context_attention_mask = tokenized_context[\\'attention_mask\\'] \\n        \\n        return {\\n            \"input_ids\": input_ids,\\n            \"labels\": input_ids.type(torch.LongTensor),\\n            \"attention_mask\": attention,\\n            \"context_ids\" : context_ids,\\n            \"context_attention_mask\" : context_attention_mask,\\n            \"nums\" :  nums,\\n            \"rhyme\": rhyme,\\n            \"verse_end\" : verse_end,\\n            \"year\": year,\\n            \"metre\" : metre}\\n        \\n        \\n    @staticmethod\\n    def collate_distil(batch, tokenizer: PreTrainedTokenizerBase ,surrogate_model: PreTrainedModel = None,surrogate_model_device=None ,max_len = 1024):\\n        \"\"\"Process data for usage in distilled training\\n\\n        Args:\\n            batch (_type_): Batch with selected data points\\n            tokenizer (PreTrainedTokenizerBase):  tokenizer to tokenize input text\\n            surrogate_model (PreTrainedModel, optional): Model to base the hidden layers data. Defaults to None.\\n            surrogate_model_device (_type_, optional): Device to compute the hidden layer data on. Defaults to None.\\n            max_len (int, optional): Maximum length of tokenization. Defaults to 1024.\\n\\n        Returns:\\n            dict: tokenized and processed to tensors data\\n        \"\"\"\\n        tokenizer.model_max_length = max_len\\n        tokenized = tokenizer([text[\\'input_ids\\'] + tokenizer.eos_token for text in batch], return_tensors=\\'pt\\', truncation=True, padding=True)\\n        input_ids = tokenized[\\'input_ids\\']\\n        attention = tokenized[\"attention_mask\"]\\n        \\n        with torch.no_grad():\\n            # This is Tuple\\n            model_hidden_states = surrogate_model(input_ids=input_ids.to(surrogate_model_device), \\n                                                  attention_mask=attention.to(surrogate_model_device), \\n                                                  labels=input_ids.type(torch.LongTensor).to(surrogate_model_device))[\\'hidden_states\\']\\n        model_hidden_states = [hidden.cpu().detach() for hidden in model_hidden_states]\\n        \\n        return {\\n            \"input_ids\": input_ids,\\n            \"labels\": input_ids.type(torch.LongTensor),\\n            \"attention_mask\": attention,\\n            \"to_replicate_states\": model_hidden_states\\n         }\\n        \\n    @staticmethod\\n    def collate_validator(batch, tokenizer: PreTrainedTokenizerBase, make_syllables:bool = False, make_verse_marks:bool = False, max_len = 512):\\n        \"\"\"Collate for Validator Training\\n\\n        Args:\\n            batch (_type_): Batch with selected data points\\n            tokenizer (PreTrainedTokenizerBase): tokenizer to tokenize input text   \\n            make_syllables (bool): Make syllables to use in model (If data is not syllabic). Defaults to False\\n            make_verse_marks(bool): Make Verse Marks to use in model (If data is not verse marks). Defaults to False\\n            max_len (int, optional): Maximum length of tokenization. Defaults to 512.\\n\\n        Returns:\\n            dict: tokenized and processed to tensors data\\n        \"\"\"\\n        \\n        if make_syllables and make_verse_marks:\\n            raise RuntimeError(\"Can\\'t both make syllables and make verse marks. Unset one to False\")\\n        \\n        reform = None\\n        if make_syllables:\\n            reform = SyllableMaker.syllabify\\n        \\n        if make_verse_marks:\\n            reform = VersologicalMaker.verse_segmnent\\n        \\n        tokenizer.model_max_length = max_len\\n        data_ids = [\"\\\\n\".join(\\n            [\"  \".join(\\n                   [\" \".join(syl) for syl in reform(line.split(\\'#\\')[-1])]\\n                ) + (line[-1] if line[-1] in [\\',\\',\\'.\\',\\'!\\',\\'?\\'] else \\'\\') if ( (make_syllables or make_verse_marks) and line) else line.split(\\'#\\')[-1] for line in text[\\'input_ids\\'].splitlines()[1:]] \\n            ) for text in batch ]\\n        \\n        \\n        tokenized = tokenizer(data_ids, return_tensors=\\'pt\\', truncation=True, padding=True)\\n        input_ids = tokenized[\\'input_ids\\']\\n        attention = tokenized[\"attention_mask\"]\\n            \\n        rhyme=None\\n        if \"rhyme\" in batch[0].keys():\\n            rhyme = torch.tensor(np.asarray([TextAnalysis._rhyme_vector(text[\"rhyme\"]) for text in batch], dtype=np.int32), dtype=torch.float32)\\n            \\n        year_bucket = None\\n        year = None\\n        if \"year\" in batch[0].keys():      \\n            year_bucket = torch.tensor(np.asarray([TextAnalysis._publish_year_vector(text[\"year\"]) for text in batch], dtype=np.int32), dtype=torch.float32)\\n            year = torch.tensor(np.asarray([ [int(text[\\'year\\'])] if text[\\'year\\'] != \\'NaN\\' else [0] for text in batch], dtype=np.int32), dtype=torch.float32)\\n        \\n        return  {\\n            \"input_ids\": input_ids,\\n            \"labels\": input_ids.type(torch.LongTensor),\\n            \"attention_mask\": attention,\\n            \"rhyme\": rhyme,\\n            \"metre_ids\": None,\\n            \"year_bucket\": year_bucket,\\n            \\'year\\':year}\\n    \\n    @staticmethod\\n    def collate_meter(batch, tokenizer: PreTrainedTokenizerBase, make_syllables:bool = False, make_verse_marks:bool = False, max_len = 512):\\n        \"\"\"Collate for Isolated Meter Training\\n\\n        Args:\\n            batch (_type_): Batch with selected data points\\n            tokenizer (PreTrainedTokenizerBase): tokenizer to tokenize input text   \\n            make_syllables (bool): Make syllables to use in model (If data is not syllabic). Defaults to False\\n            make_verse_marks(bool): Make Verse Marks to use in model (If data is not verse marks). Defaults to False\\n            max_len (int, optional): Maximum length of tokenization. Defaults to 512.\\n\\n        Returns:\\n            dict: tokenized and processed to tensors data\\n        \"\"\"\\n        \\n        tokenizer.model_max_length = max_len\\n        data_ids = []\\n        metre = []\\n        \\n        if make_syllables and make_verse_marks:\\n            raise RuntimeError(\"Can\\'t both make syllables and make verse marks. Unset one to False\")\\n        \\n        reform = None\\n        if make_syllables:\\n            reform = SyllableMaker.syllabify\\n        \\n        if make_verse_marks:\\n            reform = VersologicalMaker.verse_segmnent\\n          \\n        for datum in batch:\\n            data_ids += [\\n                    \"  \".join(\\n                    [\" \".join(syl) for syl in reform(line.split(\\'#\\')[-1])]\\n                ) + (line[-1] if line[-1] in [\\',\\',\\'.\\',\\'!\\',\\'?\\'] else \\'\\') if ( (make_syllables or make_verse_marks) and line) else line.split(\\'#\\')[-1] for line in datum[\\'input_ids\\'].splitlines()[1:]\\n                ]\\n            if \"metre_ids\" in datum.keys():\\n                metre += [TextAnalysis._metre_vector(one_metre) for one_metre in datum[\\'metre_ids\\']]\\n                \\n        tokenized = tokenizer(data_ids, return_tensors=\\'pt\\', truncation=True, padding=True)\\n        input_ids = tokenized[\\'input_ids\\']\\n        attention = tokenized[\"attention_mask\"]\\n        \\n        metre_ids = None\\n        if len(metre) > 0:\\n            metre_ids = torch.tensor(np.asarray(metre, dtype=np.int32), dtype=torch.float32)\\n            \\n        return  {\\n            \"input_ids\": input_ids,\\n            \"labels\": input_ids.type(torch.LongTensor),\\n            \"attention_mask\": attention,\\n            \"rhyme\": None,\\n            \"metre_ids\": metre_ids,\\n            \"year_bucket\": None,\\n            \"year\": None}\\n        \\n    @staticmethod\\n    def collate_meter_context(batch, tokenizer: PreTrainedTokenizerBase, make_syllables:bool = False, make_verse_marks:bool = False, max_len = 512):\\n        \"\"\"Collate for Context Meter Training\\n\\n        Args:\\n            batch (_type_): Batch with selected data points\\n            tokenizer (PreTrainedTokenizerBase): tokenizer to tokenize input text   \\n            make_syllables (bool): Make syllables to use in model (If data is not syllabic). Defaults to False\\n            make_verse_marks(bool): Make Verse Marks to use in model (If data is not verse marks). Defaults to False\\n            max_len (int, optional): Maximum length of tokenization. Defaults to 512.\\n\\n        Returns:\\n            dict: tokenized and processed to tensors data\\n        \"\"\"\\n        \\n        tokenizer.model_max_length = max_len\\n        data_ids = []\\n        \\n        if make_syllables and make_verse_marks:\\n            raise RuntimeError(\"Can\\'t both make syllables and make verse marks. Unset one to False\")\\n        \\n        reform = None\\n        if make_syllables:\\n            reform = SyllableMaker.syllabify\\n        \\n        if make_verse_marks:\\n            reform = VersologicalMaker.verse_segmnent\\n        \\n        metre = []\\n        for datum in batch:\\n            base_datums = [\\n                    \"  \".join(\\n                    [\" \".join(syl) for syl in reform(line.split(\\'#\\')[-1])]\\n                ) + (line[-1] if line[-1] in [\\',\\',\\'.\\',\\'!\\',\\'?\\'] else \\'\\') if (( make_syllables or make_verse_marks ) and line) else line.split(\\'#\\')[-1] for line in datum[\\'input_ids\\'].splitlines()[1:]\\n                ]\\n            i = 0\\n            for i in range(len(base_datums)):\\n                data_ids.append(\\n                    \"\\\\n\".join(base_datums[:i] + [\\'# \\' + base_datums[i]] + base_datums[i+1:])\\n                    ) \\n            if \"metre_ids\" in datum.keys():\\n                metre += [TextAnalysis._metre_vector(one_metre) for one_metre in datum[\\'metre_ids\\']]\\n                \\n        tokenized = tokenizer(data_ids, return_tensors=\\'pt\\', truncation=True, padding=True)\\n        input_ids = tokenized[\\'input_ids\\']\\n        attention = tokenized[\"attention_mask\"]\\n        \\n        metre_ids = None\\n        if len(metre) > 0:\\n            metre_ids = torch.tensor(np.asarray(metre, dtype=np.int32), dtype=torch.float32)\\n            \\n        return  {\\n            \"input_ids\": input_ids,\\n            \"labels\": input_ids.type(torch.LongTensor),\\n            \"attention_mask\": attention,\\n            \"rhyme\": None,\\n            \"metre_ids\": metre_ids,\\n            \"year_bucket\": None,\\n            \"year\": None}\\n    \\n\\n    def get_filenames(self):\\n        \"\"\"Get paths of data files\\n\\n        Returns:\\n            list: Paths of data files\\n        \"\"\"\\n        data_filenames = os.listdir(self.data_dir)\\n        data_by_files = []\\n        for filename in data_filenames:\\n            file_path = os.path.join(self.data_dir, filename)\\n            data_by_files.append(file_path)\\n        return data_by_files\\n        \\n    def load_raw_(self):\\n        \"\"\"Load Raw dataset with raw string data\\n        \"\"\"\\n        filenames = self.get_filenames()\\n            \\n        self.raw_dataset = CorpusDatasetPytorch.RawDataset(filenames, self.dataset_parameters[\\'lower_case\\'])\\n    \\n    def load_json_filenames(self, dataset_parameters, segmentation_type):\\n        \"\"\"Load and process datasets\\n\\n        Args:\\n            dataset_parameters (_type_): Dataset parameters\\n            segmentation_type (_type_): Segementation to be done\\n        \"\"\"\\n        \\n        filenames = self.get_filenames()\\n\\n        # Parse Train Data\\n        \\n        self.train_strophes = CorpusDatasetPytorch.StrophesDataset(filenames,dataset_parameters, segment_type= segmentation_type, dataset_part=\\'train\\')\\n        self.train_strophes.data_body_gen()\\n        \\n        self.dpo_train_strophes = CorpusDatasetPytorch.DPODataset(filenames,dataset_parameters, segment_type= segmentation_type, dataset_part=\\'train\\')\\n        self.dpo_train_strophes.data_body_gen()\\n         \\n\\n        # Parse Val Data\\n        \\n        self.val_strophes = CorpusDatasetPytorch.StrophesDataset(filenames,dataset_parameters, segment_type= segmentation_type, dataset_part=\\'val\\')\\n        self.val_strophes.data_body_gen()\\n        \\n        self.dpo_val_strophes = CorpusDatasetPytorch.DPODataset(filenames,dataset_parameters, segment_type= segmentation_type, dataset_part=\\'val\\')\\n        self.dpo_val_strophes.data_body_gen()\\n\\n        \\n        # Parse Test Data\\n        \\n        self.test_strophes = CorpusDatasetPytorch.StrophesDataset(filenames,dataset_parameters, segment_type= segmentation_type, dataset_part=\\'test\\')\\n        self.test_strophes.data_body_gen()\\n\\n        \\n        sub_dir = os.path.join(dataset_parameters[\\'cache_dir\\'], segmentation_type)\\n\\n        json.dump(self.train_strophes.data, open( os.path.join(sub_dir, \"TRAIN_STOPHES.json\"), \\'w+\\'), indent = 6)\\n        json.dump(self.dpo_train_strophes.data, open(os.path.join(sub_dir, \"DPO_TRAIN_STOPHES.json\"), \\'w+\\'), indent = 6)\\n\\n        json.dump(self.val_strophes.data, open( os.path.join(sub_dir, \"VAL_STOPHES.json\"), \\'w+\\'), indent = 6)\\n        json.dump(self.dpo_val_strophes.data, open(os.path.join(sub_dir, \"DPO_VAL_STOPHES.json\"), \\'w+\\'), indent = 6)\\n\\n        json.dump(self.test_strophes.data, open( os.path.join(sub_dir, \"TEST_STOPHES.json\"), \\'w+\\'), indent = 6)\\n              \\n        \\n    def create_config(self, cache_directory, lower_case,\\n                      validation_data_rate,test_data_rate):\\n        \\n        dataset_params = {\\'lower_case\\': lower_case,\\n                          \\'train_indexes\\':[],\\n                          \\'val_indexes\\': [],\\n                          \\'test_indexes\\': [],\\n                          \\'cache_dir\\' : cache_directory}\\n        os.makedirs(cache_directory, exist_ok=True )\\n        os.makedirs(os.path.join(cache_directory, \\'BASE\\'), exist_ok=True )\\n        os.makedirs(os.path.join(cache_directory, \\'SYLLABLE\\'), exist_ok=True )\\n        os.makedirs(os.path.join(cache_directory, \\'VERSEMARK\\'), exist_ok=True )\\n\\n        i = 0\\n        for filename in self.get_filenames():\\n            file = open(filename, \\'r\\')\\n            datum = json.load(file)\\n            rand_split = np.random.rand()\\n            if rand_split > validation_data_rate + test_data_rate:\\n                dataset_params[\\'train_indexes\\'].append(i)\\n            elif rand_split < validation_data_rate:\\n                dataset_params[\\'val_indexes\\'].append(i)\\n            else:\\n                dataset_params[\\'test_indexes\\'].append(i)\\n            i+=1\\n\\n        json.dump(dataset_params, open(os.path.join(os.path.dirname(__file__), \\'config.json\\'), \\'w+\\'), indent=6)\\n\\n        return dataset_params\\n    \\n    def check_file_existence(self, dataset_parameters, segmentation_type):\\n        sub_dir = os.path.join(dataset_parameters[\\'cache_dir\\'], segmentation_type)\\n        return  os.path.exists(os.path.join(sub_dir, \\'TRAIN_STOPHES.json\\')) and  \\\\\\n                os.path.exists(os.path.join(sub_dir, \\'DPO_TRAIN_STOPHES.json\\')) and  \\\\\\n                os.path.exists(os.path.join(sub_dir, \\'VAL_STOPHES.json\\')) and  \\\\\\n                os.path.exists(os.path.join(sub_dir, \\'DPO_VAL_STOPHES.json\\')) and  \\\\\\n                os.path.exists(os.path.join(sub_dir, \\'TEST_STOPHES.json\\'))\\n    \\n    def load_cached(self, dataset_parameters, segmentation_type):\\n        self.train_strophes = CorpusDatasetPytorch.StrophesDataset([], dataset_parameters =dataset_parameters, segment_type =segmentation_type, dataset_part=\\'train\\' )\\n        self.dpo_train_strophes = CorpusDatasetPytorch.DPODataset([], dataset_parameters =dataset_parameters, segment_type =segmentation_type, dataset_part=\\'train\\'  )\\n\\n        self.val_strophes = CorpusDatasetPytorch.StrophesDataset([], dataset_parameters =dataset_parameters, segment_type =segmentation_type, dataset_part=\\'val\\'  )\\n        self.dpo_val_strophes = CorpusDatasetPytorch.DPODataset([], dataset_parameters =dataset_parameters, segment_type =segmentation_type, dataset_part=\\'val\\'  )\\n\\n        self.test_strophes = CorpusDatasetPytorch.StrophesDataset([], dataset_parameters =dataset_parameters, segment_type =segmentation_type, dataset_part=\\'test\\'  )\\n\\n        sub_dir = os.path.join(dataset_parameters[\\'cache_dir\\'], segmentation_type)\\n\\n        self.train_strophes.data =json.load( open( os.path.join(sub_dir, \"TRAIN_STOPHES.json\"), \\'r\\'))\\n        self.dpo_train_strophes.data =json.load( open( os.path.join(sub_dir, \"DPO_TRAIN_STOPHES.json\"), \\'r\\'))\\n\\n        self.val_strophes.data =json.load( open( os.path.join(sub_dir, \"VAL_STOPHES.json\"), \\'r\\'))\\n        self.dpo_val_strophes.data =json.load( open( os.path.join(sub_dir, \"DPO_VAL_STOPHES.json\"), \\'r\\'))\\n\\n        self.test_strophes.data =json.load( open( os.path.join(sub_dir, \"TEST_STOPHES.json\"), \\'r\\'))\\n\\n        \\n    def __init__(self, SEGMENT_TYPE, data_dir = \"PoetGen\\\\corpusCzechVerse-master\\\\ccv\", cache_dir=os.path.join(os.path.dirname(__file__), \\'ProcessedData\\'), \\n                  lower_case=True, val_data_rate=0.05, test_data_rate=0.05):\\n        \"\"\"Create Dataset with specified segementatiom\\n\\n        Args:\\n            SEGMENT_TYPE (_type_): Segmentation type. Choose from BASE, SYLLABLE, VERSEMARK\\n            data_dir (str, optional): path to uprocessed data. Defaults to \"PoetGen\\\\corpusCzechVerse-master\\\\ccv\".\\n            cache_dir (_type_, optional): where to store processed data. Defaults to os.path.join(os.path.dirname(__file__), \\'ProcessedData\\').\\n            lower_case (bool, optional): if to use lowercase. Defaults to True.\\n            val_data_rate (float, optional): size of validation data. Defaults to 0.05.\\n            test_data_rate (float, optional): size of test data. Defaults to 0.05.\\n        \"\"\"\\n        \\n        self.data_dir = data_dir\\n        self.SEGMENT_TYPE =SEGMENT_TYPE\\n        if os.path.isfile( os.path.join(os.path.dirname(__file__), \\'config.json\\')):\\n            self.dataset_parameters =  json.load(open(os.path.join(os.path.dirname(__file__), \\'config.json\\'), \\'r\\'))\\n        else:\\n            self.dataset_parameters = self.create_config(cache_directory = cache_dir, \\n                               lower_case = lower_case,\\n                               validation_data_rate = val_data_rate,\\n                               test_data_rate = test_data_rate)\\n\\n        if  self.check_file_existence(self.dataset_parameters, self.SEGMENT_TYPE):\\n            self.load_cached(self.dataset_parameters, self.SEGMENT_TYPE)\\n            \\n        else:\\n            self.load_json_filenames(self.dataset_parameters, self.SEGMENT_TYPE)\\n            \\n        self.load_raw_()\\n        \\n        \\n        \\n#if __name__ == \"__main__\":\\n# Line Count\\n#    print(len(list(CorpusDatasetPytorch(os.path.abspath(os.path.join(os.path.dirname(__file__), \"corpusCzechVerse\", \"ccv\")) ).raw_dataset.get_text())))\\n# Strophe Count\\n#    print(len(list(CorpusDatasetPytorch(os.path.abspath(os.path.join(os.path.dirname(__file__), \"corpusCzechVerse\", \"ccv\")) ).raw_dataset.get_part())))\\n# Poem Count\\n#    print(len(list(CorpusDatasetPytorch(os.path.abspath(os.path.join(os.path.dirname(__file__), \"corpusCzechVerse\", \"ccv\")) ).raw_dataset.get_body())))'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/PoetGen\\\\generate_poems.py', 'source': 'git'}, page_content='import argparse\\nimport os\\nimport torch\\nimport numpy as np\\n\\nfrom transformers import AutoTokenizer, PreTrainedTokenizerBase, PreTrainedTokenizerFast\\nfrom utils.poet_utils import StropheParams, Tokens, TextManipulation, TextAnalysis\\nfrom utils.base_poet_models import PoetModelBase\\nfrom utils.validators import ValidatorInterface\\n\\nfrom corpus_capsulated_datasets import CorpusDatasetPytorch\\n\\nparser = argparse.ArgumentParser()\\n\\n#parser.add_argument(\"--model_path_full\", default=\\'jinymusim/gpt-czech-poet\\',  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--model_path_full\", default=os.path.abspath(os.path.join(os.path.dirname(__file__), \\'backup_LMS\\', \\'CZ-New-Syllable-BPE-NormalText-gpt-cz-poetry-all-e4e16_LM\\')),  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--result_file\", default= os.path.abspath(os.path.join(os.path.dirname(__file__),\\'results_new\\', \"syllable_generated_poems.txt\")), type=str, help=\"Where to store the decoding efforts\")\\nparser.add_argument(\"--sample\", default=True, type=bool, help=\"If to sample during generation\")\\n\\nparser.add_argument(\"--rhyme_model_path_full\", default=os.path.abspath(os.path.join(os.path.dirname(__file__),\\'utils\\', \\'validators\\', \\'rhyme\\', \\'distilroberta-base_BPE_validator_1706752010848\\')),  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--metre_model_path_full\", default=os.path.abspath(os.path.join(os.path.dirname(__file__),\\'utils\\' ,\"validators\", \\'meter\\', \\'Context_distilroberta-base_BPE_validator_1706752010848\\')),  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--year_model_path_full\", default=os.path.abspath(os.path.join(os.path.dirname(__file__),\\'utils\\' ,\"validators\", \\'year\\', \\'ufal-robeczech-base_BPE_validator_1706753939607\\')),  type=str, help=\"Path to Model\")\\n\\nparser.add_argument(\"--validator_tokenizer_model_rhyme\", default=\\'distilroberta-base\\', type=str, help=\"Validator tokenizer\")\\nparser.add_argument(\"--validator_tokenizer_model_meter\", default=\\'distilroberta-base\\', type=str, help=\"Validator tokenizer\")\\nparser.add_argument(\"--validator_tokenizer_model_year\", default=\\'ufal/robeczech-base\\', type=str, help=\"Validator tokenizer\")\\nparser.add_argument(\"--val_syllables_rhyme\", default=False, type=bool, help=\"Does validator use syllables\")\\nparser.add_argument(\"--val_syllables_meter\", default=False, type=bool, help=\"Does validator use syllables\")\\nparser.add_argument(\"--val_syllables_year\", default=False, type=bool, help=\"Does validator use syllables\")\\n\\nparser.add_argument(\"--meter_with_context\", default=True, type=bool, help=\"Does Meter uses context\")\\n\\nparser.add_argument(\"--runs_per_setting\", default=1, type=int, help=\"Number of runs per setting\")\\n\\nif __name__ == \"__main__\":\\n    args = parser.parse_args([] if \"__file__\" not in globals() else None)\\n    \\n_ ,model_rel_name =  os.path.split(args.model_path_full)\\n\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n\\nmodel = PoetModelBase(args.model_path_full).to(device)\\nmodel.eval()\\n\\nrhyme_model, meter_model, year_model = None, None, None\\nrhyme_model_name, meter_model_name, year_model_name = \"\", \"\", \"\"\\nif args.rhyme_model_path_full:\\n    rhyme_model: ValidatorInterface = (torch.load(args.rhyme_model_path_full, map_location=torch.device(\\'cpu\\'))).to(device)\\n    rhyme_model.eval()\\n    _,  rhyme_model_name = os.path.split(args.rhyme_model_path_full)\\n\\nif args.metre_model_path_full:\\n    meter_model: ValidatorInterface = (torch.load(args.metre_model_path_full, map_location=torch.device(\\'cpu\\'))).to(device)\\n    meter_model.eval()\\n    _, meter_model_name = os.path.split(args.metre_model_path_full)\\n    \\nif args.year_model_path_full:\\n    year_model: ValidatorInterface = (torch.load(args.year_model_path_full, map_location=torch.device(\\'cpu\\'))).to(device)\\n    year_model.eval()\\n    _,  year_model_name = os.path.split(args.year_model_path_full)\\n# Load Rhyme tokenizer\\nvalidator_tokenizer_rhyme: PreTrainedTokenizerBase = None\\nif args.validator_tokenizer_model_rhyme:\\n    try:\\n        validator_tokenizer_rhyme = AutoTokenizer.from_pretrained(args.validator_tokenizer_model_rhyme)\\n    except:\\n        validator_tokenizer_rhyme: PreTrainedTokenizerBase = PreTrainedTokenizerFast(tokenizer_file=args.validator_tokenizer_model_rhyme)\\n        validator_tokenizer_rhyme.eos_token = Tokens.EOS\\n        validator_tokenizer_rhyme.eos_token_id = Tokens.EOS_ID\\n        validator_tokenizer_rhyme.pad_token = Tokens.PAD\\n        validator_tokenizer_rhyme.pad_token_id = Tokens.PAD_ID\\n        validator_tokenizer_rhyme.unk_token = Tokens.UNK\\n        validator_tokenizer_rhyme.unk_token_id = Tokens.UNK_ID\\n        validator_tokenizer_rhyme.cls_token = Tokens.CLS\\n        validator_tokenizer_rhyme.cls_token_id = Tokens.CLS_ID\\n        validator_tokenizer_rhyme.sep_token = Tokens.SEP\\n        validator_tokenizer_rhyme.sep_token_id = Tokens.SEP_ID\\n        \\n# Load Meter tokenizer\\nvalidator_tokenizer_meter: PreTrainedTokenizerBase = None\\nif args.validator_tokenizer_model_meter:\\n    try:\\n        validator_tokenizer_meter = AutoTokenizer.from_pretrained(args.validator_tokenizer_model_meter)\\n    except:\\n        validator_tokenizer_meter: PreTrainedTokenizerBase = PreTrainedTokenizerFast(tokenizer_file=args.validator_tokenizer_model_meter)\\n        validator_tokenizer_meter.eos_token = Tokens.EOS\\n        validator_tokenizer_meter.eos_token_id = Tokens.EOS_ID\\n        validator_tokenizer_meter.pad_token = Tokens.PAD\\n        validator_tokenizer_meter.pad_token_id = Tokens.PAD_ID\\n        validator_tokenizer_meter.unk_token = Tokens.UNK\\n        validator_tokenizer_meter.unk_token_id = Tokens.UNK_ID\\n        validator_tokenizer_meter.cls_token = Tokens.CLS\\n        validator_tokenizer_meter.cls_token_id = Tokens.CLS_ID\\n        validator_tokenizer_meter.sep_token = Tokens.SEP\\n        validator_tokenizer_meter.sep_token_id = Tokens.SEP_ID\\n        \\n# Load Year tokenizer\\nvalidator_tokenizer_year: PreTrainedTokenizerBase = None\\nif args.validator_tokenizer_model_year:\\n    try:\\n        validator_tokenizer_year = AutoTokenizer.from_pretrained(args.validator_tokenizer_model_year)\\n    except:\\n        validator_tokenizer_year: PreTrainedTokenizerBase = PreTrainedTokenizerFast(tokenizer_file=args.validator_tokenizer_model_year)\\n        validator_tokenizer_year.eos_token = Tokens.EOS\\n        validator_tokenizer_year.eos_token_id = Tokens.EOS_ID\\n        validator_tokenizer_year.pad_token = Tokens.PAD\\n        validator_tokenizer_year.pad_token_id = Tokens.PAD_ID\\n        validator_tokenizer_year.unk_token = Tokens.UNK\\n        validator_tokenizer_year.unk_token_id = Tokens.UNK_ID\\n        validator_tokenizer_year.cls_token = Tokens.CLS\\n        validator_tokenizer_year.cls_token_id = Tokens.CLS_ID\\n        validator_tokenizer_year.sep_token = Tokens.SEP\\n        validator_tokenizer_year.sep_token_id = Tokens.SEP_ID\\n \\n# Load LM tokenizers       \\ntokenizer: PreTrainedTokenizerBase =  AutoTokenizer.from_pretrained(args.model_path_full)\\n\\ndef decoder_helper(type, rhyme, year, meter):\\n    if type == \"BASIC\":\\n        start = f\"# {rhyme} # {year}\\\\n{meter}\"\\n        tokenized = tokenizer.encode(start, return_tensors=\\'pt\\', truncation=True)\\n        out = model.model.generate(tokenized.to(device), \\n                                        max_length=512,\\n                                        do_sample=True,\\n                                        top_k=50,\\n                                        eos_token_id = tokenizer.eos_token_id,\\n                                        early_stopping=True,\\n                                        pad_token_id= tokenizer.pad_token_id)\\n        return tokenizer.decode(out.cpu()[0], skip_special_tokens=True)\\n    if type==\"FORCED\":\\n        start_forced = f\"# {rhyme} # {year}\\\\n{meter} #\"\\n        return model.generate_forced(start_forced, tokenizer, verse_len=len(rhyme), sample=True, device=device)\\n    \\nfor rhyme in StropheParams.RHYME[:8]:\\n    for year in [1900, 1860]:\\n        for meter in [\\'J\\', \\'T\\', \\'D\\']:\\n            for type in [\\'BASIC\\', \\'FORCED\\']:\\n                for _ in range(args.runs_per_setting):\\n                    generated_poem:str = decoder_helper(type, rhyme, year, meter)\\n                    meters = []\\n                    rhyme_pred = \\'\\'\\n                    year_pred = 0\\n                    for line in generated_poem.splitlines():\\n                        # Skip Empty lines\\n                        if not line.strip(): \\n                            break\\n                        if not (TextManipulation._remove_most_nonchar(line)).strip():\\n                            break\\n                        # Validate for Strophe Parameters\\n                        if TextAnalysis._is_param_line(line):\\n\\n                            data = CorpusDatasetPytorch.collate_validator([{\"input_ids\" :generated_poem}],tokenizer=validator_tokenizer_rhyme,\\n                                                                               make_syllables=args.val_syllables_rhyme,\\n                                                                               max_len=rhyme_model.model.config.max_position_embeddings - 2)\\n                            rhyme_pred =StropheParams.RHYME[np.argmax(rhyme_model.predict_state(input_ids=data[\\'input_ids\\'].to(device)).detach().flatten().cpu().numpy())]\\n\\n                            data = CorpusDatasetPytorch.collate_validator([{\"input_ids\" :generated_poem}],tokenizer=validator_tokenizer_year,\\n                                                                               make_syllables=args.val_syllables_year,\\n                                                                               max_len=year_model.model.config.max_position_embeddings - 2)\\n                            year_pred = round(year_model.predict_state(input_ids=data[\\'input_ids\\'].to(device)).detach().flatten().cpu().numpy()[0])\\n                            continue\\n                        \\n                    if args.meter_with_context:\\n                        data = CorpusDatasetPytorch.collate_meter_context([{\"input_ids\" :generated_poem}],tokenizer=validator_tokenizer_meter,\\n                                                    make_syllables=args.val_syllables_meter,\\n                                                    max_len=meter_model.model.config.max_position_embeddings - 2)\\n                    else:\\n                        data = CorpusDatasetPytorch.collate_meter([{\"input_ids\" :generated_poem}],tokenizer=validator_tokenizer_meter,\\n                                                    make_syllables=args.val_syllables_meter,\\n                                                    max_len=meter_model.model.config.max_position_embeddings - 2)\\n                    for j in range(data[\\'input_ids\\'].shape[0]):\\n                        meters.append(\\n                            StropheParams.METER[np.argmax(meter_model.predict_state(input_ids=data[\\'input_ids\\'][j,:].reshape(1,-1).to(device)).detach().flatten().cpu().numpy())]\\n                        )\\n\\n                    with open(args.result_file, \\'a\\', encoding=\"utf-8\") as file:\\n                        print(f\"REQUESTED: {rhyme}, {year}, {meter}, GENERATED USING: {type}\\\\n\", file=file)\\n                        print(generated_poem, file=file)\\n                        print(f\"PREDICTED: {rhyme_pred}, {year_pred}, {meters}\\\\n\\\\n\", file=file)\\n                    print(f\"REQUESTED: {rhyme}, {year}, {meter}, GENERATED USING: {type}\\\\n\")\\n                    print(generated_poem)\\n                    print(f\"PREDICTED: {rhyme_pred}, {year_pred}, {meters}\\\\n\\\\n\")'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/PoetGen\\\\lm_finetune_torch_trainer_api.py', 'source': 'git'}, page_content='# Outide Packages\\nimport torch\\nimport os\\nimport argparse\\nfrom datasets import Dataset\\n\\nfrom trl import (\\n    DPOConfig, DPOTrainer\\n)\\nfrom transformers import  (\\n    AutoTokenizer, \\n    AutoModelForCausalLM, \\n    PreTrainedTokenizerFast, \\n    PreTrainedTokenizerBase, \\n    TrainingArguments,\\n    Trainer,\\n    \\n    EarlyStoppingCallback,\\n    IntervalStrategy\\n)\\nfrom functools import partial\\n\\n# Project Packages\\nfrom utils.base_poet_models import (\\n    PoetModelBase,\\n    PoetModelSecondaryTasks,\\n    PoetModelHalfBase,\\n    PoetModelVerseEnd,\\n    PoetModelContextInput,\\n    PoetModelContextYear,\\n    PoetModelAllTasks,\\n    DistilModel,\\n    PoetModelSmall\\n)\\n\\nfrom peft import (\\n    LoraConfig, \\n    get_peft_model\\n)\\n\\n\\nfrom corpus_capsulated_datasets import CorpusDatasetPytorch\\nfrom utils.poet_model_utils import ModelManipulation, PoetModelInterface\\n\\nfrom utils.poet_utils import Tokens, parse_boolean\\n\\n\\nparser = argparse.ArgumentParser()\\n\\nparser.add_argument(\"--batch_size_poet\", default=8, type=int, help=\"Batch size.\")\\nparser.add_argument(\"--epochs_poet\", default=1, type=int, help=\"Number of epochs for poet gen\")\\nparser.add_argument(\"--learning_rate\", default=1e-5, type=float, help=\"Learning Rate for Finetuning\")\\nparser.add_argument(\"--train_masked\", default=False, type=bool, help=\"Train for consistency secondary training\")\\nparser.add_argument(\"--input_mask_rate\", default=0.0, type=float, help=\"Rate of input masking\")\\n\\nparser.add_argument(\"--data_path\",  default=os.path.abspath(os.path.join(os.path.dirname(__file__), \"corpusCzechVerse\", \"ccv-new-summary-one-sentence\")), type=str, help=\"Path to Data\")\\n\\n#TODO: Join syllabification by better symbol (maybe extra space arround) DONE\\n#TODO: Make meter validator with context\\n#   NO_MARK verse       \\n#   MARK    verse     <- Predicting this METER\\n#   NO_MARK verse\\n#   NO_MARK verse\\n# DONE\\n\\n# huggyllama/llama-7b 4096\\n# bigscience/bloom-560m 2048\\n# TheBloke/Llama-2-7B-fp16 4096\\n\\n# lchaloupsky/czech-gpt2-oscar 1024 Czech Model\\n# spital/gpt2-small-czech-cs 1024 Alt Czech Model\\n# distilgpt2 1024 Alt En Model\\n# gpt2 1024 EN Model\\n# stabilityai/StableBeluga-7B 4096 Large\\n# RWKV/rwkv-4-169m-pile 1024 RNN\\n# unsloth/Llama-3.2-1B-Instruct 4096\\n\\n# Introduce Layered Model, Best done by modifiing \\n# self.h = nn.ModuleList([GPT2Block(config) for _ in range(config.num_hidden_layers)])\\n\\n# This gives Model only 5 blocks\\n# model.base_model.h = torch.nn.ModuleList([transformers.models.gpt2.modeling_gpt2.GPT2Block(model.base_model.config) for _ in range(5)])\\n\\n# Adding Custom Modules to model is possible\\n# model.base_model.h = torch.nn.ModuleList(\\n    # [transformers.models.gpt2.modeling_gpt2.GPT2Block(model.base_model.config) for _ in range(5)] + \\\\\\n    # [torch.nn.Linear(model.base_model.config.hidden_size, model.base_model.config.hidden_size) for _ in range(2)]\\n    # )\\n\\n# Extending Appending and Inserting Modules Also Possible\\n# model.base_model.h.extend([torch.nn.Linear(768,1)])\\n# model.base_model.h.append(torch.nn.Linear(1,768))\\n# model.base_model.h.insert(7,torch.nn.Linear(768,768))\\n\\nparser.add_argument(\"--default_hf_model\", default=\\'unsloth/Llama-3.2-1B\\', type=str, help=\"Default Model from HF to use\")\\nparser.add_argument(\"--use_default_model\",  default=True, type=bool, help=\"Use Default Model\")\\n#parser.add_argument(\"--tokenizer\", default=os.path.abspath(os.path.join(os.path.dirname(__file__), \"utils\", \"tokenizers\", \"Unicode\", \"unicode_tokenizer.json\")), type=str, help=\"Tokenizer to use\")\\nparser.add_argument(\"--tokenizer\", default=\\'unsloth/Llama-3.2-1B\\', type=str, help=\"Tokenizer to use\")\\n#parser.add_argument(\"--tokenizer\", default=os.path.join(os.path.dirname(__file__), \\'backup_LMS\\',\\'CZ-Unicode-Tokenizer-NormalText-gpt-cz-poetry-base-e4e16_LM\\' ), type=str, help=\"Tokenizer to use\")\\nparser.add_argument(\"--model_type\",  default=\"base\", type=str, choices=[\"base\", \"secondary_tasks\", \"half\", \"verse\", \"context\", \"year\", \"all\", \\'distil\\', \\'small\\'], help=\"What type of Model is to be constructed\")\\nparser.add_argument(\"--model_path\", default=os.path.abspath(os.path.join(os.path.dirname(__file__), \"Test-Model\")),  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--max_len\", default=8192, type=int, help=\"Max length for tokenizer\")\\nparser.add_argument(\"--context_max_len\", default=1, type=int, help=\"Max length of context for tokenizer\")\\n\\nparser.add_argument(\"--syllables\", default=False, type=bool, help=\"If inputs should be parsed by syllables\")\\nparser.add_argument(\"--lower_case\", default=True, type=bool, help=\"If to lower case data\")\\n\\nparser.add_argument(\"--mirror_imbed\", default=True, type=bool, help=\"If to mirror input embedding to output ones\")\\n\\nparser.add_argument(\"--val_data_rate\", default=0.05, type=float, help=\"Rate of validation data\")\\nparser.add_argument(\"--test_data_rate\", default=0.05, type=float, help=\"Rate of test data\")\\n\\nparser.add_argument(\"--size_test\", default=False, type=parse_boolean, help=\\'If to conduct size test on data\\')\\nparser.add_argument(\"--sizes_to_test\", default=1, type=float, help=\\'Size to test on\\')\\n\\nparser.add_argument(\"--dpo\", default=False, type=bool, help=\"If to use DPO training\")\\nparser.add_argument(\"--dpo_epochs\", default=2, type=int, help=\"Number of epochs for DPO training\")\\n\\nparser.add_argument(\"--lora\", default=False, type=bool, help=\"If to use LORA training\")\\n\\n\\ndef train_model(model: PoetModelInterface, tokenizer: PreTrainedTokenizerBase ,dataset: CorpusDatasetPytorch, collate_fnc, args: argparse.Namespace):\\n    # Verse Training\\n    if args.epochs_poet !=0:\\n        \\n        if args.lora:\\n            lora_training_args = LoraConfig(\\n                task_type=\"CAUSAL_LM\"\\n            )\\n            model.model = get_peft_model(model.model, lora_training_args, mixed=True)\\n        \\n        \\n        training_args = TrainingArguments(\\n                output_dir=args.model_path + \"TEMP\",\\n                overwrite_output_dir= True,\\n                save_strategy  = IntervalStrategy.EPOCH,\\n                learning_rate=args.learning_rate,\\n                save_safetensors=False,\\n                save_total_limit=1,\\n                auto_find_batch_size = True if torch.cuda.is_available() else False,\\n                logging_steps = 500,\\n                num_train_epochs = args.epochs_poet,\\n                bf16 = True if torch.cuda.is_available() else False,\\n                logging_dir = \\'./logs\\',\\n            )\\n    \\n        trainer = Trainer(\\n                model = model,\\n                args = training_args,\\n                train_dataset= dataset.train_strophes,\\n                data_collator=collate_fnc\\n            )\\n            \\n        trainer.train()\\n        \\n    if args.dpo and args.dpo_epochs !=0:\\n\\n        dpo_training_args = DPOConfig(\\n                output_dir=args.model_path + \"TEMP\",\\n                overwrite_output_dir= True,\\n                save_strategy= IntervalStrategy.EPOCH,\\n                save_safetensors=False,\\n                save_total_limit=1,\\n                auto_find_batch_size = True if torch.cuda.is_available() else False,\\n                logging_steps = 500,\\n                max_length = args.max_len,\\n                max_prompt_length = args.max_len,\\n                remove_unused_columns=False,\\n                num_train_epochs = args.dpo_epochs,\\n                use_liger_kernel = True if torch.cuda.is_available() else False,\\n                bf16 = True if torch.cuda.is_available() else False,\\n                logging_dir = \\'./logs\\',\\n            )\\n        \\n        # Move model to CPU, done to avoid CUDA memory error\\n        model = model.cpu()\\n        train_dataset = Dataset.from_list(dataset.dpo_train_strophes.data)\\n    \\n        dpo_trainer = DPOTrainer(\\n                model = model.model,\\n                args = dpo_training_args,\\n                train_dataset=train_dataset,\\n                processing_class=tokenizer\\n            )\\n        dpo_trainer.train()\\n\\n\\ndef create_model_and_tokenizer(args: argparse.Namespace):\\n    \"\"\"Create Model and Tokenizer, put model on best device\\n\\n    Args:\\n        args (argparse.Namespace): Arguments of the model and tokenizer\\n\\n    Raises:\\n        TypeError: Model type not recognized\\n\\n    Returns:\\n        tuple: tuple of model and tokenizer\\n    \"\"\"\\n    if args.use_default_model:\\n        tokenizer = None\\n        if args.model_type == \"base\":  \\n                \\n            model = PoetModelBase(args.default_hf_model)\\n        elif args.model_type == \"secondary_tasks\":\\n            model = PoetModelSecondaryTasks(args.default_hf_model)\\n        elif args.model_type == \"half\":\\n            model = PoetModelHalfBase(args.default_hf_model)\\n        elif args.model_type == \"verse\":\\n            model =  PoetModelVerseEnd(args.default_hf_model)\\n        elif args.model_type == \"context\":\\n            model = PoetModelContextInput(args.default_hf_model, args.context_max_len)\\n        elif args.model_type == \"year\":\\n            model = PoetModelContextYear(args.default_hf_model, args.context_max_len)\\n        elif args.model_type == \"all\":\\n            model = PoetModelAllTasks(args.default_hf_model)\\n        elif args.model_type == \\'distil\\':\\n            model = DistilModel(args.default_hf_model)\\n        elif args.model_type == \\'small\\':\\n            model = PoetModelSmall()\\n        else:\\n            raise TypeError(\"Given model type doesn\\'t exists\")\\n        \\n        try:   \\n             \\n            tokenizer: PreTrainedTokenizerBase =  AutoTokenizer.from_pretrained(args.tokenizer)\\n            if tokenizer.pad_token == None:\\n                tokenizer.pad_token = tokenizer.eos_token\\n                tokenizer.pad_token_id = tokenizer.eos_token_id\\n            if args.model_type == \\'small\\':\\n                ModelManipulation.exchange_embedding(model, tokenizer, AutoTokenizer.from_pretrained(args.default_hf_model), args.mirror_imbed)\\n                \\n        except: #TODO: Need model to update embedding matrix\\n            tokenizer: PreTrainedTokenizerBase = PreTrainedTokenizerFast(tokenizer_file=args.tokenizer)\\n            tokenizer.eos_token = Tokens.EOS\\n            tokenizer.eos_token_id = Tokens.EOS_ID\\n            tokenizer.pad_token = Tokens.PAD\\n            tokenizer.pad_token_id = Tokens.PAD_ID\\n            tokenizer.unk_token = Tokens.UNK\\n            tokenizer.unk_token_id = Tokens.UNK_ID\\n            \\n            ModelManipulation.exchange_embedding(model, tokenizer, AutoTokenizer.from_pretrained(args.default_hf_model), args.mirror_imbed)\\n    else:\\n        tokenizer = AutoTokenizer.from_pretrained(args.model_path)\\n        model = torch.load(args.model_path, map_location=torch.device(\\'cpu\\'))\\n\\n    \\n    return model, tokenizer\\n\\ndef main(args: argparse.Namespace):\\n    \\n    model, tokenizer = create_model_and_tokenizer(args)\\n    \\n    \\n    # Partial Function to use as data collection with input masking\\n    if args.model_type == \\'distil\\':\\n        device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n        collate = partial(CorpusDatasetPytorch.collate_distil, tokenizer=tokenizer,\\n                          surrogate_model=AutoModelForCausalLM.from_pretrained(args.default_hf_model,output_hidden_states=True).to(device), surrogate_model_device=device, max_len=args.max_len)\\n    else:\\n        collate = partial(CorpusDatasetPytorch.collate, tokenizer=tokenizer,max_len=args.max_len, \\n                      max_context=args.context_max_len)\\n    \\n\\n    train_data = CorpusDatasetPytorch(SEGMENT_TYPE=\\'BASE\\', data_dir=args.data_path, \\n                                    lower_case=args.lower_case,\\n                                    val_data_rate=args.val_data_rate, test_data_rate=args.test_data_rate)\\n    \\n    \\n    if not args.size_test:\\n        train_model(model, tokenizer, train_data, collate, args)\\n        \\n        torch.save(model.cpu(), args.model_path + \".model\")\\n        model.save_LM(f\"{args.model_path}_LM\")\\n        tokenizer.save_pretrained(f\"{args.model_path}_LM\")\\n        \\n    else:\\n        train_data.train_strophes.change_custom_size(args.sizes_to_test)\\n        \\n        # Size compensation\\n        args.epochs_poet =  int(args.epochs_poet/args.sizes_to_test)\\n        train_model(model, tokenizer, train_data, collate, args)\\n        \\n        torch.save(model.cpu(), args.model_path + f\"_data_size={args.sizes_to_test}.model\")\\n        model.save_LM(f\"{args.model_path}_data_size={args.sizes_to_test}_LM\")\\n        tokenizer.save_pretrained(f\"{args.model_path}_data_size={args.sizes_to_test}_LM\")\\n      \\n\\n\\nif __name__ == \"__main__\":\\n    args = parser.parse_args([] if \"__file__\" not in globals() else None)\\n    print(\"Cuda is available: \", torch.cuda.is_available())\\n    main(args)\\n'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/PoetGen\\\\model_validator.py', 'source': 'git'}, page_content='import torch\\nimport os\\nimport random\\nimport re\\nimport argparse  \\nimport numpy as np\\n\\nfrom tqdm import tqdm\\nfrom transformers import  AutoTokenizer, PreTrainedTokenizerFast, PreTrainedTokenizerBase\\n\\nfrom utils.poet_utils import TextAnalysis, TextManipulation, Tokens, SyllableMaker\\nfrom utils.validators import ValidatorInterface\\nfrom utils.base_poet_models import PoetModelBase, PoetModelFunctionalInterface\\nfrom corpus_capsulated_datasets import CorpusDatasetPytorch\\n\\n\\nclass ModelValidator:\\n    \"\"\"Class to Validate LMs using Validators and Analysis\\n    \"\"\"\\n    def __init__(self, args: argparse.Namespace,\\n                 result_dir: str = os.path.abspath(os.path.join(os.path.dirname(__file__),\"results_new\"))) -> None:\\n        \"\"\"Construct Validators using given arguments. Save the requested number of repeats\\n\\n        Args:\\n            args (argparse.Namespace): Arguments of Validation\\n            result_dir (str, optional): Directory to store results. Defaults to os.path.abspath(os.path.join(os.path.dirname(__file__),\"results\")).\\n        \"\"\"\\n\\n        self.device = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n        \\n        self.args = args\\n        \\n        self.model_name = args.model_path_full\\n        # Split Path to find only the LM name itself\\n        _ ,self.model_rel_name =  os.path.split(self.model_name)\\n        # Load Model as Pickle file or as stored LM \\n        if \"_LM\" in self.model_rel_name:\\n            self.model_rel_name = re.sub(\"_LM\", \"\", self.model_rel_name)\\n            self.model: PoetModelFunctionalInterface = PoetModelBase(self.model_name).to(self.device)\\n        else:\\n            self.model: PoetModelFunctionalInterface= (torch.load(self.model_name, map_location=torch.device(\\'cpu\\'))).to(self.device)\\n        self.model.eval()\\n        \\n        # Load validators \\n        self.rhyme_model, self.meter_model, self.year_model = None, None, None\\n        self.rhyme_model_name, self.meter_model_name, self.year_model_name = \"\", \"\", \"\"\\n        if args.rhyme_model_path_full:\\n            self.rhyme_model: ValidatorInterface = (torch.load(args.rhyme_model_path_full, map_location=torch.device(\\'cpu\\'))).to(self.device)\\n            self.rhyme_model.eval()\\n            _,  self.rhyme_model_name = os.path.split(args.rhyme_model_path_full)\\n        \\n        if args.metre_model_path_full:\\n            self.meter_model: ValidatorInterface = (torch.load(args.metre_model_path_full, map_location=torch.device(\\'cpu\\'))).to(self.device)\\n            self.meter_model.eval()\\n            _, self.meter_model_name = os.path.split(args.metre_model_path_full)\\n            \\n        if args.year_model_path_full:\\n            self.year_model: ValidatorInterface = (torch.load(args.year_model_path_full, map_location=torch.device(\\'cpu\\'))).to(self.device)\\n            self.year_model.eval()\\n            _,  self.year_model_name = os.path.split(args.year_model_path_full)\\n            \\n\\n            \\n        # Load Rhyme tokenizer\\n        self.validator_tokenizer_rhyme: PreTrainedTokenizerBase = None\\n        if args.validator_tokenizer_model_rhyme:\\n            try:\\n                    self.validator_tokenizer_rhyme = AutoTokenizer.from_pretrained(args.validator_tokenizer_model_rhyme)\\n            except:\\n                self.validator_tokenizer_rhyme: PreTrainedTokenizerBase = PreTrainedTokenizerFast(tokenizer_file=args.validator_tokenizer_model_rhyme)\\n                self.validator_tokenizer_rhyme.eos_token = Tokens.EOS\\n                self.validator_tokenizer_rhyme.eos_token_id = Tokens.EOS_ID\\n                self.validator_tokenizer_rhyme.pad_token = Tokens.PAD\\n                self.validator_tokenizer_rhyme.pad_token_id = Tokens.PAD_ID\\n                self.validator_tokenizer_rhyme.unk_token = Tokens.UNK\\n                self.validator_tokenizer_rhyme.unk_token_id = Tokens.UNK_ID\\n                self.validator_tokenizer_rhyme.cls_token = Tokens.CLS\\n                self.validator_tokenizer_rhyme.cls_token_id = Tokens.CLS_ID\\n                self.validator_tokenizer_rhyme.sep_token = Tokens.SEP\\n                self.validator_tokenizer_rhyme.sep_token_id = Tokens.SEP_ID\\n                \\n        # Load Meter tokenizer\\n        self.validator_tokenizer_meter: PreTrainedTokenizerBase = None\\n        if args.validator_tokenizer_model_meter:\\n            try:\\n                    self.validator_tokenizer_meter = AutoTokenizer.from_pretrained(args.validator_tokenizer_model_meter)\\n            except:\\n                self.validator_tokenizer_meter: PreTrainedTokenizerBase = PreTrainedTokenizerFast(tokenizer_file=args.validator_tokenizer_model_meter)\\n                self.validator_tokenizer_meter.eos_token = Tokens.EOS\\n                self.validator_tokenizer_meter.eos_token_id = Tokens.EOS_ID\\n                self.validator_tokenizer_meter.pad_token = Tokens.PAD\\n                self.validator_tokenizer_meter.pad_token_id = Tokens.PAD_ID\\n                self.validator_tokenizer_meter.unk_token = Tokens.UNK\\n                self.validator_tokenizer_meter.unk_token_id = Tokens.UNK_ID\\n                self.validator_tokenizer_meter.cls_token = Tokens.CLS\\n                self.validator_tokenizer_meter.cls_token_id = Tokens.CLS_ID\\n                self.validator_tokenizer_meter.sep_token = Tokens.SEP\\n                self.validator_tokenizer_meter.sep_token_id = Tokens.SEP_ID\\n                \\n        # Load Year tokenizer\\n        self.validator_tokenizer_year: PreTrainedTokenizerBase = None\\n        if args.validator_tokenizer_model_year:\\n            try:\\n                    self.validator_tokenizer_year = AutoTokenizer.from_pretrained(args.validator_tokenizer_model_year)\\n            except:\\n                self.validator_tokenizer_year: PreTrainedTokenizerBase = PreTrainedTokenizerFast(tokenizer_file=args.validator_tokenizer_model_year)\\n                self.validator_tokenizer_year.eos_token = Tokens.EOS\\n                self.validator_tokenizer_year.eos_token_id = Tokens.EOS_ID\\n                self.validator_tokenizer_year.pad_token = Tokens.PAD\\n                self.validator_tokenizer_year.pad_token_id = Tokens.PAD_ID\\n                self.validator_tokenizer_year.unk_token = Tokens.UNK\\n                self.validator_tokenizer_year.unk_token_id = Tokens.UNK_ID\\n                self.validator_tokenizer_year.cls_token = Tokens.CLS\\n                self.validator_tokenizer_year.cls_token_id = Tokens.CLS_ID\\n                self.validator_tokenizer_year.sep_token = Tokens.SEP\\n                self.validator_tokenizer_year.sep_token_id = Tokens.SEP_ID\\n         \\n        # Load LM tokenizers       \\n        try:    \\n            self.tokenizer: PreTrainedTokenizerBase =  AutoTokenizer.from_pretrained(self.model_name)\\n        except:\\n            self.tokenizer: PreTrainedTokenizerBase = PreTrainedTokenizerFast(tokenizer_file=args.backup_tokenizer_model)\\n            self.tokenizer.eos_token = Tokens.EOS\\n            self.tokenizer.eos_token_id = Tokens.EOS_ID\\n            self.tokenizer.pad_token = Tokens.PAD\\n            self.tokenizer.pad_token_id = Tokens.PAD_ID\\n            self.tokenizer.unk_token = Tokens.UNK\\n            self.tokenizer.unk_token_id = Tokens.UNK_ID\\n            \\n        self.dataset = CorpusDatasetPytorch(\\'BASE\\',data_dir=args.data_path_poet)\\n        self.validation_data = self.dataset.test_strophes.data\\n        \\n        # Store the Validation arguments  \\n        self.epochs = args.num_runs\\n        self.runs_per_epoch = args.num_samples\\n        self.result_dir = result_dir\\n        if not os.path.exists(self.result_dir):\\n            os.makedirs(self.result_dir)\\n            \\n            \\n    def decode_helper(self, type:str, index:int):\\n        \"\"\"Wrapper around LM generation\\n\\n        Args:\\n            type (str): Which type of generation to use (\\'BASIC\\', \\'FORCED\\')\\n\\n        Returns:\\n            str: Generated Strophe\\n        \"\"\"\\n        \\n        if type  == \"BASIC\":\\n            # Up to first meter\\n            FORMAT = \"METER_VERSE\"\\n            if self.model_rel_name.startswith(\\'CZ\\') or self.model_rel_name.startswith(\\'ALT\\') or self.model_rel_name.startswith(\\'EN\\') or self.model_rel_name.startswith(\\'ENALT\\'):\\n                start = f\"# {self.validation_data[index][\\'rhyme\\']} # {TextManipulation._year_bucketor(self.validation_data[index][\\'year\\'])}\\\\n{self.validation_data[index][\\'metre_ids\\'][0]}\"\\n                FORMAT = \"METER_VERSE\"\\n            elif self.model_rel_name.startswith(\\'gpt\\'):\\n                start = f\"# {self.validation_data[index][\\'rhyme\\']} # {TextManipulation._year_bucketor(self.validation_data[index][\\'year\\'])} # {self.validation_data[index][\\'metre_ids\\'][0]}\"\\n                FORMAT = \"BASIC\"\\n            elif self.model_rel_name.startswith(\\'New\\'):\\n                start = f\"{self.validation_data[index][\\'rhyme\\']} # {TextManipulation._year_bucketor(self.validation_data[index][\\'year\\'])} # {self.validation_data[index][\\'metre_ids\\'][0]}\"\\n                FORMAT = \"OLD\"\\n            else:\\n                start = f\"# {self.validation_data[index][\\'rhyme\\']} # {TextManipulation._year_bucketor(self.validation_data[index][\\'year\\'])} # {self.validation_data[index][\\'metre_ids\\'][0]}\"\\n                FORMAT = \"VERSE_PAR\"\\n            tokenized_poet_start = self.tokenizer.encode(start, return_tensors=\\'pt\\', truncation=True)\\n            if self.args.sample:\\n                out = self.model.model.generate(tokenized_poet_start.to(self.device), \\n                                        max_length=256,\\n                                        do_sample=True,\\n                                        top_k=50,\\n                                        eos_token_id = self.tokenizer.eos_token_id,\\n                                        early_stopping=True,\\n                                        pad_token_id=self.tokenizer.pad_token_id)\\n                \\n            else:\\n                out = self.model.model.generate(tokenized_poet_start.to(self.device), \\n                                        max_length=256,\\n                                        num_beams=8,\\n                                        no_repeat_ngram_size=2,\\n                                        eos_token_id = self.tokenizer.eos_token_id,\\n                                        early_stopping=True,\\n                                        pad_token_id=self.tokenizer.pad_token_id)\\n                \\n            return self.tokenizer.decode(out.cpu()[0], skip_special_tokens=True)\\n        if type == \"FORCED\":\\n            \\n            if self.model_rel_name.startswith(\\'CZ\\') or self.model_rel_name.startswith(\\'ALT\\') or self.model_rel_name.startswith(\\'EN\\') or self.model_rel_name.startswith(\\'ENALT\\'):\\n                FORMAT = \"METER_VERSE\"\\n            elif self.model_rel_name.startswith(\\'gpt\\'):\\n                FORMAT = \"BASIC\"\\n            elif self.model_rel_name.startswith(\\'New\\'):\\n                FORMAT = \"OLD\"\\n            else:\\n                FORMAT = \"VERSE_PAR\"\\n                \\n            if FORMAT == \"METER_VERSE\":\\n                start_forced = f\"# {self.validation_data[index][\\'rhyme\\']} # {TextManipulation._year_bucketor(self.validation_data[index][\\'year\\'])}\"\\n                for id in self.validation_data[index][\\'metre_ids\\']:\\n                    start_forced = start_forced + f\"\\\\n{id} #\"\\n            elif FORMAT == \\'OLD\\':\\n                start_forced = f\"{self.validation_data[index][\\'rhyme\\']} # {TextManipulation._year_bucketor(self.validation_data[index][\\'year\\'])} # {self.validation_data[index][\\'metre_ids\\'][0]}\"\\n            else:\\n                start_forced =  f\"# {self.validation_data[index][\\'rhyme\\']} # {TextManipulation._year_bucketor(self.validation_data[index][\\'year\\'])} # {self.validation_data[index][\\'metre_ids\\'][0]}\"\\n            \\n            return self.model.generate_forced(start_forced, self.tokenizer, sample=self.args.sample, format=FORMAT, device=self.device)\\n            \\n            \\n            \\n    def validate_decoding(self, type:str):\\n        \"\"\"Validate LM given generation type. Measure metrics (Rhyme acc, Metrum acc, End acc, Syllable count acc)\\n\\n        Args:\\n            type (str): Type of generation to use\\n        \"\"\"\\n        # Store of individual runs of evaluation\\n        end_accuracy, sylab_accuracy = [], []\\n        rhyme_accuracy, rhyme_top_k, rhyme_label_acc, levenshtein_dist = [], [], [], []\\n        metre_accuracy, metre_top_k, metre_label_acc  = [], [], []\\n        year_accuracy, year_top_k, year_label_acc, year_distance_dist  = [], [], [], []\\n        \\n        syllable_running_ration = []\\n        # Run the requested amount of evaluations\\n        for _ in tqdm(range(self.epochs), desc=f\"Validation {type}\"):\\n            # Store results of current evaluation\\n            end_all, sylab_all  = 0,0\\n            rhyme_all, rhyme_top_k_all, rhyme_label_all, lev_distance_all = 0,0,0,0\\n            metre_all, metre_top_k_all, metre_label_all = 0,0,0\\n            year_all, year_top_k_all, year_label_all, year_distance_all = 0,0,0,0\\n            \\n            \\n            end_pos, sylab_pos = 0, 0\\n            rhyme_pos, rhyme_top_k_pos, rhyme_label_pos, lev_distance = 0,0,0,0\\n            metre_pos, metre_top_k_pos, metre_label_pos  = 0,0,0\\n            year_pos, year_top_k_pos, year_label_pos, year_distance = 0,0,0,0\\n            \\n            \\n            samples = random.choices(list(range(len(self.validation_data))), k=self.runs_per_epoch)\\n            # Run the requested steps in evaluation\\n            for i in tqdm(range(self.runs_per_epoch), leave=False):\\n                # Get generated Strophe\\n                decoded_cont:str = self.decode_helper(type,samples[i])\\n                # Validate line by line\\n                STROPHE_METER = \\'J\\'\\n                PRESENT_METERS = []\\n                for line in decoded_cont.splitlines():\\n                    # Skip Empty lines\\n                    if not line.strip(): \\n                        break\\n                    if not (TextManipulation._remove_most_nonchar(line)).strip():\\n                        break\\n                    # Validate for Strophe Parameters\\n                    if TextAnalysis._is_param_line(line):\\n                        values = TextAnalysis._first_line_analysis(line)\\n                        \\n                        rhyme_all +=1\\n                        rhyme_top_k_all +=1\\n                        rhyme_label_all +=1\\n                        \\n                        year_all +=1\\n                        year_top_k_all +=1\\n                        year_label_all +=1\\n                        year_distance_all +=1\\n                        \\n                        # Validate for Rhyme schema\\n                        if self.rhyme_model != None and \"RHYME\" in values.keys():\\n                            data = CorpusDatasetPytorch.collate_validator([{\"input_ids\" :decoded_cont, \\'rhyme\\' : values[\"RHYME\"]}],tokenizer=self.validator_tokenizer_rhyme,\\n                                                                           make_syllables=self.args.val_syllables_rhyme,\\n                                                                           max_len=self.rhyme_model.model.config.max_position_embeddings - 2)\\n                            res = self.rhyme_model.validate_model(input_ids=data[\\'input_ids\\'].to(self.device),\\n                                                                   rhyme=data[\\'rhyme\\'], k=self.args.top_k)\\n                            rhyme_pos += res[\\'acc\\']\\n                            rhyme_top_k_pos += res[\\'top_k\\']\\n                            rhyme_label_pos += res[\\'predicted_label\\']\\n                            if res[\\'acc\\'] < 0.5:\\n                                lev_distance_all += 1\\n                                lev_distance +=res[\\'lev_distance\\']\\n                            \\n                        \\n                        #Validate for Year\\n                        if self.year_model != None and \"YEAR\" in values.keys():\\n                            data = CorpusDatasetPytorch.collate_validator([{\"input_ids\" :decoded_cont, \"year\": values[\"YEAR\"]}],tokenizer=self.validator_tokenizer_year,\\n                                                                           make_syllables=self.args.val_syllables_year,\\n                                                                           max_len=self.year_model.model.config.max_position_embeddings - 2)\\n                            res = self.year_model.validate_model(input_ids=data[\\'input_ids\\'].to(self.device),\\n                                                                   year_bucket=data[\\'year_bucket\\'],k=self.args.top_k)\\n                            \\n                            year_pos += res[\\'acc\\']\\n                            year_top_k_pos += res[\\'top_k\\']\\n                            year_label_pos += res[\\'predicted_label\\']\\n                            if res[\\'acc\\'] < 0.5:\\n                                year_distance += res[\\'distance\\']\\n                            \\n                        if \\'STROPHE_METER\\' in values.keys():\\n                            STROPHE_METER = values[\\'STROPHE_METER\\']\\n                                \\n                        \\n                        # Measure Syllable uniqueness\\n                        all_sylabs = []\\n                        for line in decoded_cont.splitlines()[1:]:\\n                            all_sylabs += [syl for syl_word in SyllableMaker.syllabify(line.split(\"#\")[-1]) for syl in syl_word]\\n                        if len(all_sylabs)  != 0:\\n                            syllable_running_ration.append(len(set(all_sylabs))/len(all_sylabs))\\n                        \\n                        continue\\n                            \\n                    # Else validate for individual verse\\n                    line_analysis = TextAnalysis._continuos_line_analysis(line)\\n                    # Was Still empty in terms of any text\\n                    if len(line_analysis.keys()) == 0:\\n                        continue\\n                    \\n                    \\n                    if self.meter_model != None and \"METER\" in line_analysis.keys():\\n                        PRESENT_METERS.append(line_analysis[\"METER\"])\\n                    elif self.meter_model != None:\\n                        PRESENT_METERS.append(STROPHE_METER)\\n                        \\n                        \\n                        \\n                    \\n                    end_all += 1\\n                    if \"END\" in line_analysis.keys() and  TextAnalysis._end_matches(line, line_analysis[\\'END\\']):\\n                        end_pos +=1\\n                    \\n                    sylab_all +=1\\n                    if \"LENGTH\" in line_analysis.keys() and \"TRUE_LENGTH\" in line_analysis.keys() and line_analysis[\"LENGTH\"] == line_analysis[\"TRUE_LENGTH\"]:\\n                        sylab_pos +=1\\n                        \\n                # Validate for Metrum\\n                if self.meter_model != None:\\n                    if self.args.train_with_context:\\n                        data = CorpusDatasetPytorch.collate_meter_context([{\"input_ids\" :decoded_cont, \"metre_ids\": PRESENT_METERS}],tokenizer=self.validator_tokenizer_meter,\\n                                                                       make_syllables=self.args.val_syllables_meter,\\n                                                                       max_len=self.meter_model.model.config.max_position_embeddings - 2)\\n                    else:\\n                        data = CorpusDatasetPytorch.collate_meter([{\"input_ids\" :decoded_cont, \"metre_ids\": PRESENT_METERS}],tokenizer=self.validator_tokenizer_meter,\\n                                                                       make_syllables=self.args.val_syllables_meter,\\n                                                                       max_len=self.meter_model.model.config.max_position_embeddings - 2)\\n                    if data[\\'input_ids\\'] != None and  data[\\'metre_ids\\'] != None:\\n                        for j in range(min(data[\\'input_ids\\'].shape[0], data[\\'metre_ids\\'].shape[0])):\\n                            res = self.meter_model.validate_model(input_ids=data[\"input_ids\"][j,:].reshape(1,-1).to(self.device),\\n                                        attention_mask=data[\\'attention_mask\\'][j,:].reshape(1,-1).to(self.device),\\n                                        rhyme=None, \\n                                        metre_ids=data[\"metre_ids\"][j,:].reshape(1,-1),\\n                                        year_bucket=None)\\n                            \\n                            metre_pos += res[\\'acc\\']\\n                            metre_top_k_pos += res[\\'top_k\\']\\n                            metre_label_pos += res[\\'predicted_label\\']\\n                        \\n                        metre_all += len(PRESENT_METERS)\\n                        metre_top_k_all += len(PRESENT_METERS)\\n                        metre_label_all += len(PRESENT_METERS)\\n                        \\n                \\n                    \\n                    \\n            # Store Results        \\n            end_accuracy.append(0 if end_all == 0 else end_pos/end_all)\\n            sylab_accuracy.append(0 if sylab_all==0 else sylab_pos/sylab_all)\\n            \\n            rhyme_accuracy.append(0 if rhyme_all==0 else rhyme_pos/rhyme_all)\\n            rhyme_top_k.append(0 if rhyme_top_k_all == 0 else rhyme_top_k_pos/rhyme_top_k_all)\\n            rhyme_label_acc.append(0 if rhyme_label_all==0 else rhyme_label_pos/rhyme_label_all)\\n            levenshtein_dist.append(0 if lev_distance_all == 0 else lev_distance/lev_distance_all)\\n            \\n            metre_accuracy.append(0 if metre_all==0 else metre_pos/metre_all)\\n            metre_top_k.append(0 if metre_top_k_all==0 else  metre_top_k_pos/metre_top_k_all)\\n            metre_label_acc.append(0 if metre_label_all==0 else metre_label_pos/metre_label_all)\\n            \\n            year_accuracy.append(0 if year_all==0 else year_pos/year_all)\\n            year_top_k.append(0 if year_top_k_all==0 else year_top_k_pos/year_top_k_all)\\n            year_label_acc.append(0 if year_label_all==0 else year_label_pos/year_label_all)\\n            year_distance_dist.append(0 if year_distance_all == 0 else year_distance/year_distance_all)\\n            \\n            \\n        # Log all results and configuration\\n        with open(os.path.abspath(os.path.join(self.result_dir, self.model_rel_name + \".txt\")), \\'a\\') as file:\\n            print(f\" ===== {type} Decoding Validation: Epochs: {self.epochs}, Runs per epoch: {self.runs_per_epoch}, SAMPLING: {str(self.args.sample)} =====\", file=file)\\n            # Line Metrics\\n            print(f\"Num Sylabs Accuracy: {np.mean(sylab_accuracy):.4f} +- {np.std(sylab_accuracy, ddof=1):.4f}\", file=file)\\n            print(f\"Endings Accuracy: {np.mean(end_accuracy):.4f} +- {np.std(end_accuracy, ddof=1):.4f}\", file=file)\\n            print(f\"Unique Syllable Ratio: {np.mean(syllable_running_ration):.4f} +- {np.std(syllable_running_ration, ddof=1):.4f}\\\\n\", file=file)\\n            # Rhyme Related Metrics\\n            print(f\"Rhyme model: {self.rhyme_model_name}, Syllable {str(self.args.val_syllables_rhyme)}\", file=file)\\n            print(f\"Rhyme Accuracy: {np.mean(rhyme_accuracy):.4f} +- {np.std(rhyme_accuracy, ddof=1):.4f}\", file=file)\\n            print(f\"Rhyme top {self.args.top_k} presence: {np.mean(rhyme_top_k):.4f} +- {np.std(rhyme_top_k, ddof=1):.4f}\", file=file)\\n            print(f\"Rhyme label presence: {np.mean(rhyme_label_acc):.4f} +- {np.std(rhyme_label_acc, ddof=1):.4f}\", file=file)\\n            # Measures Levenshtein distance only on wrong examples!\\n            print(f\"Rhyme Levenshtein distance: {np.mean(levenshtein_dist):.4f} +- {np.std(levenshtein_dist, ddof=1):.4f}\\\\n\", file=file)\\n            # Metre related metrics\\n            print(f\"Metre model: {self.meter_model_name}, Syllable {str(self.args.val_syllables_meter)}\", file=file)\\n            print(f\"Metre Accuracy: {np.mean(metre_accuracy):.4f} +- {np.std(metre_accuracy, ddof=1):.4f}\", file=file)\\n            print(f\"Metre top {self.args.top_k} presence: {np.mean(metre_top_k):.4f} +- {np.std(metre_top_k, ddof=1):.4f}\", file=file)\\n            print(f\"Metre label presence: {np.mean(metre_label_acc):.4f} +- {np.std(metre_label_acc, ddof=1):.4f}\\\\n\", file=file)\\n            # Year related metrics\\n            print(f\"Year model: {self.year_model_name}, Syllable {str(self.args.val_syllables_year)}\", file=file)\\n            print(f\"Year Accuracy: {np.mean(year_accuracy):.4f} +- {np.std(year_accuracy, ddof=1):.4f}\", file=file)\\n            print(f\"Year top {self.args.top_k} presence: {np.mean(year_top_k):.4f} +- {np.std(year_top_k, ddof=1):.4f}\", file=file)\\n            print(f\"Year label presence: {np.mean(year_label_acc):.4f} +- {np.std(year_label_acc, ddof=1):.4f}\", file=file)\\n            # Measure average distance on wrong examples\\n            print(f\"Year predict distance: {np.mean(year_distance_dist):.4f} +- {np.std(year_distance_dist, ddof=1):.4f}\\\\n\", file=file)\\n                    \\n            \\n    def full_validate(self):\\n        \"\"\"Validate both generation types\\n        \"\"\"\\n\\n        self.validate_decoding(\"BASIC\")\\n        self.validate_decoding(\"FORCED\")\\n        \\n      \\n      \\n\\n        \\nparser = argparse.ArgumentParser()\\n\\nparser.add_argument(\"--backup_tokenizer_model\", default=os.path.abspath(os.path.join(os.path.dirname(__file__), \"utils\", \"tokenizers\", \"BPE\", \"new_syllabs_processed_tokenizer.json\")), type=str, help=\"Default Model from HF to use\")\\nparser.add_argument(\"--data_path_poet\",  default=os.path.abspath(os.path.join(os.path.dirname(__file__), \"corpusCzechVerse\", \"ccv\")), type=str, help=\"Path to Data\")\\n\\nparser.add_argument(\"--num_samples\", default=10, type=int, help=\"Number of samples to test the tokenizer on\")\\nparser.add_argument(\"--num_runs\", default=2, type=int, help=\"Number of runs on datasets\")\\n\\nparser.add_argument(\"--model_path_full\", default=os.path.abspath(os.path.join(os.path.dirname(__file__),\\'backup_LMS\\', \"CZ-New-Processed-BPE-NormalText-gpt-cz-poetry-all-e8e32_LM\")),  type=str, help=\"Path to Model\")\\n\\nparser.add_argument(\"--rhyme_model_path_full\", default=os.path.abspath(os.path.join(os.path.dirname(__file__),\\'utils\\', \\'validators\\', \\'rhyme\\', \\'distilroberta-base_BPE_validator_1706752010848\\')),  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--validator_tokenizer_model_rhyme\", default=\\'distilroberta-base\\', type=str, help=\"Validator tokenizer\")\\nparser.add_argument(\"--val_syllables_rhyme\", default=False, type=bool, help=\"Does validator use syllables\")\\n\\nparser.add_argument(\"--metre_model_path_full\", default=os.path.abspath(os.path.join(os.path.dirname(__file__),\\'utils\\' ,\"validators\", \\'meter\\', \\'Context_distilroberta-base_BPE_validator_1706752010848\\')),  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--validator_tokenizer_model_meter\", default=\\'distilroberta-base\\', type=str, help=\"Validator tokenizer\")\\nparser.add_argument(\"--val_syllables_meter\", default=False, type=bool, help=\"Does validator use syllables\")\\nparser.add_argument(\"--train_with_context\", default=True, type=bool, help=\"If meter validator was trained with context in mind.\")\\n\\nparser.add_argument(\"--year_model_path_full\", default=os.path.abspath(os.path.join(os.path.dirname(__file__),\\'utils\\' ,\"validators\", \\'year\\', \\'ufal-robeczech-base_BPE_validator_1706753939607\\')),  type=str, help=\"Path to Model\")\\nparser.add_argument(\"--validator_tokenizer_model_year\", default=\\'ufal/robeczech-base\\', type=str, help=\"Validator tokenizer\")\\nparser.add_argument(\"--val_syllables_year\", default=False, type=bool, help=\"Does validator use syllables\")\\n\\nparser.add_argument(\"--top_k\", default=2, type=int, help=\"Top k number\")\\nparser.add_argument(\"--sample\", default=True, type=bool, help=\"If to Sample\")\\n\\n\\n\\ndef main(args):\\n    val = ModelValidator(args)\\n    val.full_validate()\\n\\nif __name__ == \"__main__\":\\n    args = parser.parse_args([] if \"__file__\" not in globals() else None)\\n    main(args)\\n    \\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache = LoadedRepositoriesAndFiles()\n",
    "document_loader = GitDocumentLoader(repo_path=\"https://github.com/jinymusim/GPT-Czech-Poet.git\", branch=\"main\", loaded_repositories_and_files=cache)\n",
    "documents =  document_loader.load()\n",
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'ItemId': 'c:\\\\users\\\\qwert\\\\git\\\\docu-bot\\\\data\\\\document_cache\\\\requirements\\\\requirements.txt', 'source': 'user'}, page_content=\"#\\n# This file is autogenerated by pip-compile with Python 3.11\\n# by the following command:\\n#\\n#    pip-compile '.\\\\requirements.in'\\n#\\naiofiles==23.2.1\\n    # via gradio\\naiohappyeyeballs==2.4.3\\n    # via aiohttp\\naiohttp==3.10.10\\n    # via\\n    #   langchain\\n    #   langchain-community\\naiosignal==1.3.1\\n    # via aiohttp\\nannotated-types==0.7.0\\n    # via pydantic\\nanyio==4.6.2.post1\\n    # via\\n    #   gradio\\n    #   httpx\\n    #   openai\\n    #   starlette\\n    #   watchfiles\\nasgiref==3.8.1\\n    # via opentelemetry-instrumentation-asgi\\nattrs==24.2.0\\n    # via aiohttp\\nbackoff==2.2.1\\n    # via posthog\\nbcrypt==4.2.0\\n    # via chromadb\\nbuild==1.2.2.post1\\n    # via chromadb\\ncachetools==5.5.0\\n    # via google-auth\\ncertifi==2024.8.30\\n    # via\\n    #   httpcore\\n    #   httpx\\n    #   kubernetes\\n    #   requests\\nchardet==5.2.0\\n    # via -r .\\\\requirements.in\\ncharset-normalizer==3.4.0\\n    # via requests\\nchroma-hnswlib==0.7.6\\n    # via chromadb\\nchromadb==0.5.18\\n    # via\\n    #   -r .\\\\requirements.in\\n    #   langchain-chroma\\nclick==8.1.7\\n    # via\\n    #   typer\\n    #   uvicorn\\ncolorama==0.4.6\\n    # via\\n    #   build\\n    #   click\\n    #   tqdm\\n    #   uvicorn\\ncoloredlogs==15.0.1\\n    # via onnxruntime\\ndataclasses-json==0.6.7\\n    # via langchain-community\\ndeprecated==1.2.14\\n    # via\\n    #   opentelemetry-api\\n    #   opentelemetry-exporter-otlp-proto-grpc\\n    #   opentelemetry-semantic-conventions\\ndistro==1.9.0\\n    # via openai\\ndurationpy==0.9\\n    # via kubernetes\\nfastapi==0.115.5\\n    # via\\n    #   chromadb\\n    #   gradio\\n    #   langchain-chroma\\nffmpy==0.4.0\\n    # via gradio\\nfilelock==3.16.1\\n    # via\\n    #   huggingface-hub\\n    #   torch\\nflatbuffers==24.3.25\\n    # via onnxruntime\\nfrozenlist==1.5.0\\n    # via\\n    #   aiohttp\\n    #   aiosignal\\nfsspec==2024.10.0\\n    # via\\n    #   gradio-client\\n    #   huggingface-hub\\n    #   torch\\nfuzzywuzzy==0.18.0\\n    # via -r .\\\\requirements.in\\ngitdb==4.0.11\\n    # via gitpython\\ngitpython==3.1.43\\n    # via -r .\\\\requirements.in\\ngoogle-auth==2.36.0\\n    # via kubernetes\\ngoogleapis-common-protos==1.66.0\\n    # via opentelemetry-exporter-otlp-proto-grpc\\ngradio==5.5.0\\n    # via -r .\\\\requirements.in\\ngradio-client==1.4.2\\n    # via gradio\\ngreenlet==3.1.1\\n    # via sqlalchemy\\ngrpcio==1.67.1\\n    # via\\n    #   chromadb\\n    #   opentelemetry-exporter-otlp-proto-grpc\\nh11==0.14.0\\n    # via\\n    #   httpcore\\n    #   uvicorn\\nhttpcore==1.0.6\\n    # via httpx\\nhttptools==0.6.4\\n    # via uvicorn\\nhttpx==0.27.2\\n    # via\\n    #   chromadb\\n    #   gradio\\n    #   gradio-client\\n    #   langsmith\\n    #   openai\\n    #   safehttpx\\nhttpx-sse==0.4.0\\n    # via langchain-community\\nhuggingface-hub==0.26.2\\n    # via\\n    #   gradio\\n    #   gradio-client\\n    #   tokenizers\\nhumanfriendly==10.0\\n    # via coloredlogs\\nidna==3.10\\n    # via\\n    #   anyio\\n    #   httpx\\n    #   requests\\n    #   yarl\\nimportlib-metadata==8.5.0\\n    # via opentelemetry-api\\nimportlib-resources==6.4.5\\n    # via chromadb\\njinja2==3.1.4\\n    # via\\n    #   gradio\\n    #   torch\\njiter==0.7.1\\n    # via openai\\njsonpatch==1.33\\n    # via langchain-core\\njsonpointer==3.0.0\\n    # via jsonpatch\\nkubernetes==31.0.0\\n    # via chromadb\\nlangchain==0.3.7\\n    # via\\n    #   -r .\\\\requirements.in\\n    #   langchain-community\\nlangchain-chroma==0.1.4\\n    # via -r .\\\\requirements.in\\nlangchain-community==0.3.6\\n    # via -r .\\\\requirements.in\\nlangchain-core==0.3.16\\n    # via\\n    #   langchain\\n    #   langchain-chroma\\n    #   langchain-community\\n    #   langchain-openai\\n    #   langchain-text-splitters\\nlangchain-openai==0.2.7\\n    # via -r .\\\\requirements.in\\nlangchain-text-splitters==0.3.2\\n    # via langchain\\nlangsmith==0.1.142\\n    # via\\n    #   langchain\\n    #   langchain-community\\n    #   langchain-core\\nmarkdown-it-py==3.0.0\\n    # via rich\\nmarkupsafe==2.1.5\\n    # via\\n    #   gradio\\n    #   jinja2\\nmarshmallow==3.23.1\\n    # via dataclasses-json\\nmdurl==0.1.2\\n    # via markdown-it-py\\nmmh3==5.0.1\\n    # via chromadb\\nmonotonic==1.6\\n    # via posthog\\nmpmath==1.3.0\\n    # via sympy\\nmultidict==6.1.0\\n    # via\\n    #   aiohttp\\n    #   yarl\\nmypy-extensions==1.0.0\\n    # via typing-inspect\\nnetworkx==3.4.2\\n    # via torch\\nninja==1.11.1.1\\n    # via -r .\\\\requirements.in\\nnumpy==1.26.4\\n    # via\\n    #   chroma-hnswlib\\n    #   chromadb\\n    #   gradio\\n    #   langchain\\n    #   langchain-chroma\\n    #   langchain-community\\n    #   onnxruntime\\n    #   pandas\\noauthlib==3.2.2\\n    # via\\n    #   kubernetes\\n    #   requests-oauthlib\\nonnxruntime==1.20.0\\n    # via chromadb\\nopenai==1.54.4\\n    # via\\n    #   -r .\\\\requirements.in\\n    #   langchain-openai\\nopentelemetry-api==1.28.1\\n    # via\\n    #   chromadb\\n    #   opentelemetry-exporter-otlp-proto-grpc\\n    #   opentelemetry-instrumentation\\n    #   opentelemetry-instrumentation-asgi\\n    #   opentelemetry-instrumentation-fastapi\\n    #   opentelemetry-sdk\\n    #   opentelemetry-semantic-conventions\\nopentelemetry-exporter-otlp-proto-common==1.28.1\\n    # via opentelemetry-exporter-otlp-proto-grpc\\nopentelemetry-exporter-otlp-proto-grpc==1.28.1\\n    # via chromadb\\nopentelemetry-instrumentation==0.49b1\\n    # via\\n    #   opentelemetry-instrumentation-asgi\\n    #   opentelemetry-instrumentation-fastapi\\nopentelemetry-instrumentation-asgi==0.49b1\\n    # via opentelemetry-instrumentation-fastapi\\nopentelemetry-instrumentation-fastapi==0.49b1\\n    # via chromadb\\nopentelemetry-proto==1.28.1\\n    # via\\n    #   opentelemetry-exporter-otlp-proto-common\\n    #   opentelemetry-exporter-otlp-proto-grpc\\nopentelemetry-sdk==1.28.1\\n    # via\\n    #   chromadb\\n    #   opentelemetry-exporter-otlp-proto-grpc\\nopentelemetry-semantic-conventions==0.49b1\\n    # via\\n    #   opentelemetry-instrumentation\\n    #   opentelemetry-instrumentation-asgi\\n    #   opentelemetry-instrumentation-fastapi\\n    #   opentelemetry-sdk\\nopentelemetry-util-http==0.49b1\\n    # via\\n    #   opentelemetry-instrumentation-asgi\\n    #   opentelemetry-instrumentation-fastapi\\norjson==3.10.11\\n    # via\\n    #   chromadb\\n    #   gradio\\n    #   langsmith\\noverrides==7.7.0\\n    # via chromadb\\npackaging==24.2\\n    # via\\n    #   -r .\\\\requirements.in\\n    #   build\\n    #   gradio\\n    #   gradio-client\\n    #   huggingface-hub\\n    #   langchain-core\\n    #   marshmallow\\n    #   onnxruntime\\n    #   opentelemetry-instrumentation\\npandas==2.2.3\\n    # via gradio\\npillow==11.0.0\\n    # via gradio\\nposthog==3.7.0\\n    # via chromadb\\npropcache==0.2.0\\n    # via yarl\\nprotobuf==5.28.3\\n    # via\\n    #   googleapis-common-protos\\n    #   onnxruntime\\n    #   opentelemetry-proto\\npyasn1==0.6.1\\n    # via\\n    #   pyasn1-modules\\n    #   rsa\\npyasn1-modules==0.4.1\\n    # via google-auth\\npydantic==2.9.2\\n    # via\\n    #   chromadb\\n    #   fastapi\\n    #   gradio\\n    #   langchain\\n    #   langchain-core\\n    #   langsmith\\n    #   openai\\n    #   pydantic-settings\\npydantic-core==2.23.4\\n    # via pydantic\\npydantic-settings==2.6.1\\n    # via langchain-community\\npydub==0.25.1\\n    # via gradio\\npygments==2.18.0\\n    # via rich\\npypdf==5.1.0\\n    # via -r .\\\\requirements.in\\npypika==0.48.9\\n    # via chromadb\\npyproject-hooks==1.2.0\\n    # via build\\npyreadline3==3.5.4\\n    # via humanfriendly\\npython-dateutil==2.9.0.post0\\n    # via\\n    #   kubernetes\\n    #   pandas\\n    #   posthog\\npython-dotenv==1.0.1\\n    # via\\n    #   pydantic-settings\\n    #   uvicorn\\npython-multipart==0.0.12\\n    # via gradio\\npytz==2024.2\\n    # via pandas\\npyyaml==6.0.2\\n    # via\\n    #   chromadb\\n    #   gradio\\n    #   huggingface-hub\\n    #   kubernetes\\n    #   langchain\\n    #   langchain-community\\n    #   langchain-core\\n    #   uvicorn\\nregex==2024.11.6\\n    # via tiktoken\\nrequests==2.32.3\\n    # via\\n    #   huggingface-hub\\n    #   kubernetes\\n    #   langchain\\n    #   langchain-community\\n    #   langsmith\\n    #   posthog\\n    #   requests-oauthlib\\n    #   requests-toolbelt\\n    #   tiktoken\\nrequests-oauthlib==2.0.0\\n    # via kubernetes\\nrequests-toolbelt==1.0.0\\n    # via langsmith\\nrich==13.9.4\\n    # via\\n    #   chromadb\\n    #   typer\\nrsa==4.9\\n    # via google-auth\\nruff==0.7.3\\n    # via gradio\\nsafehttpx==0.1.1\\n    # via gradio\\nsemantic-version==2.10.0\\n    # via gradio\\nshellingham==1.5.4\\n    # via typer\\nsix==1.16.0\\n    # via\\n    #   kubernetes\\n    #   posthog\\n    #   python-dateutil\\nsmmap==5.0.1\\n    # via gitdb\\nsniffio==1.3.1\\n    # via\\n    #   anyio\\n    #   httpx\\n    #   openai\\nsqlalchemy==2.0.35\\n    # via\\n    #   langchain\\n    #   langchain-community\\nstarlette==0.41.2\\n    # via\\n    #   fastapi\\n    #   gradio\\nsympy==1.13.1\\n    # via\\n    #   onnxruntime\\n    #   torch\\ntenacity==9.0.0\\n    # via\\n    #   chromadb\\n    #   langchain\\n    #   langchain-community\\n    #   langchain-core\\ntiktoken==0.8.0\\n    # via langchain-openai\\ntokenizers==0.20.3\\n    # via chromadb\\ntomlkit==0.12.0\\n    # via gradio\\ntorch==2.5.1\\n    # via -r .\\\\requirements.in\\ntqdm==4.67.0\\n    # via\\n    #   chromadb\\n    #   huggingface-hub\\n    #   openai\\ntyper==0.13.0\\n    # via\\n    #   chromadb\\n    #   gradio\\ntyping-extensions==4.12.2\\n    # via\\n    #   chromadb\\n    #   fastapi\\n    #   gradio\\n    #   gradio-client\\n    #   huggingface-hub\\n    #   langchain-core\\n    #   openai\\n    #   opentelemetry-sdk\\n    #   pydantic\\n    #   pydantic-core\\n    #   sqlalchemy\\n    #   torch\\n    #   typer\\n    #   typing-inspect\\ntyping-inspect==0.9.0\\n    # via dataclasses-json\\ntzdata==2024.2\\n    # via pandas\\nurllib3==2.2.3\\n    # via\\n    #   kubernetes\\n    #   requests\\nuvicorn[standard]==0.32.0\\n    # via\\n    #   chromadb\\n    #   gradio\\nwatchfiles==0.24.0\\n    # via uvicorn\\nwebsocket-client==1.8.0\\n    # via kubernetes\\nwebsockets==12.0\\n    # via\\n    #   gradio-client\\n    #   uvicorn\\nwheel==0.45.0\\n    # via -r .\\\\requirements.in\\nwrapt==1.16.0\\n    # via\\n    #   deprecated\\n    #   opentelemetry-instrumentation\\nyarl==1.17.1\\n    # via aiohttp\\nzipp==3.21.0\\n    # via importlib-metadata\\n\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_document_loader = ZipDocumentLoader(temp_file=r\"C:\\Users\\qwert\\Git\\Docu-Bot\\requirements.txt\", loaded_repositories_and_files=cache)\n",
    "zip_document = zip_document_loader.load()\n",
    "zip_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test document vectorization and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstores = LoadedVectorStores()\n",
    "docstore = DocumentStore()\n",
    "cache = LoadedRepositoriesAndFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_loader = GitDocumentLoader(repo_path=\"https://github.com/jinymusim/GPT-Czech-Poet.git\", branch=\"main\", loaded_repositories_and_files=cache)\n",
    "chroma_vector = create_vector_store_from_document_loader(document_loader=document_loader, vectorstores=vectorstores, docstore=docstore)\n",
    "multi_vector_store = MultiVectorStore(chroma_vectors=list(vectorstores._vectorstores.values()))\n",
    "document_retrieval = DocumentRetrieval(vectorstore=multi_vector_store, docstore=docstore, search_kwargs={\"min_score\": 0.1, \"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/README.md', 'source': 'git', 'doc_id': '5bc8f45a-8190-4905-9452-e15d5587f135', 'sub_docs': [Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/README.md', 'doc_id': '5bc8f45a-8190-4905-9452-e15d5587f135', 'source': 'git', 'score': 0.26036688131935415}, page_content='parser.add_argument(\"--prompt_ending\", default=True, type=bool, help=\"Ending of Verse is prompted into training data\")')]}, page_content='parser.add_argument(\"--prompt_ending\", default=True, type=bool, help=\"Ending of Verse is prompted into training data\")'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/PoetGen\\\\corpusCzechVerse\\\\README.md', 'source': 'git', 'doc_id': '0d125fb5-515c-42c3-abe2-e1d317292d57', 'sub_docs': [Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/PoetGen\\\\corpusCzechVerse\\\\README.md', 'doc_id': '0d125fb5-515c-42c3-abe2-e1d317292d57', 'source': 'git', 'score': 0.46283561742141166}, page_content='*NOTE: There are several re-editions in the corpus. One poem thus may appear multiple times.')]}, page_content='*NOTE: There are several re-editions in the corpus. One poem thus may appear multiple times.'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/PoetGen\\\\corpusCzechVerse\\\\README.md', 'source': 'git', 'doc_id': '3d604363-6b62-4050-acf7-d8100f4fcb67', 'sub_docs': [Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/PoetGen\\\\corpusCzechVerse\\\\README.md', 'doc_id': '3d604363-6b62-4050-acf7-d8100f4fcb67', 'source': 'git', 'score': 0.4892683092919652}, page_content=\"Furthermore we'd like to ask you to quote following articles:\\n\\n> What is CCV:\")]}, page_content=\"Furthermore we'd like to ask you to quote following articles:\\n\\n> What is CCV:\"),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/README.md', 'source': 'git', 'doc_id': '927f17d6-95be-4529-a83a-b292c9d9e6ec', 'sub_docs': [Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/README.md', 'doc_id': '927f17d6-95be-4529-a83a-b292c9d9e6ec', 'source': 'git', 'score': 0.5575734798192447}, page_content='# GPT Czech Poet\\nLearning models for Czech Poet Generation')]}, page_content='# GPT Czech Poet\\nLearning models for Czech Poet Generation'),\n",
       " Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/README.md', 'source': 'git', 'doc_id': '80a1cdec-5f9f-49ef-8f2e-81bb1d638c1d', 'sub_docs': [Document(metadata={'ItemId': 'https://github.com/jinymusim/GPT-Czech-Poet/blob/main/README.md', 'doc_id': '80a1cdec-5f9f-49ef-8f2e-81bb1d638c1d', 'source': 'git', 'score': 0.6347471770697912}, page_content='For use, all modeling scripts automatically converts data to needed format.')]}, page_content='For use, all modeling scripts automatically converts data to needed format.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_retrieval._get_relevant_documents(\"What is Czech Poet?\", run_manager=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstores = LoadedVectorStores()\n",
    "docstore = DocumentStore()\n",
    "cache = LoadedRepositoriesAndFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is Czech Poet?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'According to the text, \"Czech Poet\" is a learning model that generates text based on input data.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver = prepare_retriever(\n",
    "    long_branches=cache.get_cached_repo_branches(cache.get_cached_repositories()[0]),\n",
    "    zip_files= [],\n",
    "    docstore=docstore,\n",
    "    loaded_vectorstores=vectorstores,\n",
    "    model_type=\"Llama-3.2-1B\",\n",
    "    api_key=API_KEY,\n",
    "    search_kwargs={\"min_score\": 0.5, \"k\": 10}\n",
    ")\n",
    "documents = get_documents(\"What is Czech Poet?\", retriver)\n",
    "rag([{\"role\": \"user\", \"content\": \"What is Czech Poet?\"}], documents, \"Llama-3.2-1B\", API_KEY, prompt=PROMPTS.INPUT_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is Czech Poet?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'I cannot provide a direct quote from the provided text as it is not a source of information. However, I can provide a definition of \"Czech Poet\" based on general knowledge.\\n\\n\"Czech Poet\" typically refers to a person who writes poetry.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver = prepare_retriever(\n",
    "    long_branches=cache.get_cached_repo_branches(cache.get_cached_repositories()[0]),\n",
    "    zip_files= [],\n",
    "    docstore=docstore,\n",
    "    loaded_vectorstores=vectorstores,\n",
    "    model_type=\"Llama-3.2-1B\",\n",
    "    api_key=API_KEY,\n",
    "    search_kwargs={\"min_score\": 0.1, \"k\": 10},\n",
    "    retrieval_type=RETRIEVAL_TYPES.RERANK,\n",
    ")\n",
    "documents = get_documents(\"What is Czech Poet?\", retriver)\n",
    "rag([{\"role\": \"user\", \"content\": \"What is Czech Poet?\"}], documents, \"Llama-3.2-1B\", API_KEY, prompt=PROMPTS.INPUT_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is Czech Poet?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Czech Poet is a project that aims to generate poetry based on a large corpus of Czech texts. The project uses machine learning models to analyze the structure and grammar of the text, and then generates poetry based on that analysis.\\n\\nThe project consists of several components:\\n\\n1. **Corpus of Czech Verse**: A large corpus of Czech texts, which includes books, articles, and other written works.\\n2. **Tokenizer**: A custom tokenizer that breaks down the text into smaller units, such as st'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver = prepare_retriever(\n",
    "    long_branches=cache.get_cached_repo_branches(cache.get_cached_repositories()[0]),\n",
    "    zip_files= [],\n",
    "    docstore=docstore,\n",
    "    loaded_vectorstores=vectorstores,\n",
    "    model_type=\"Llama-3.2-1B\",\n",
    "    api_key=API_KEY,\n",
    "    search_kwargs={\"min_score\": 0.5, \"k\": 10},\n",
    "    retrieval_type=RETRIEVAL_TYPES.QUERY_ALTERATION,\n",
    ")\n",
    "documents = get_documents(\"What is Czech Poet?\", retriver)\n",
    "rag([{\"role\": \"user\", \"content\": \"What is Czech Poet?\"}], documents, \"Llama-3.2-1B\", API_KEY, prompt=PROMPTS.INPUT_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is Czech Poet?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'According to the document, Czech Poet refers to a learning model that is used for generating text in Czech (the official language of the Czech Republic).'},\n",
       " {'role': 'user', 'content': 'How do I use it?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Unfortunately, I don't have any specific instructions or guidelines on how to use a specific Czech Poet learning model. However, I can provide some general guidance on how to use a machine learning model for text generation, which is likely the Czech Poet model in this case.\\n\\nHere are some possible ways to use the Czech Poet learning model:\\n\\n1. **Text input**: You can input a text prompt or a portion of text to the Czech Poet model to generate a response. You can use\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver = prepare_retriever(\n",
    "    long_branches=cache.get_cached_repo_branches(cache.get_cached_repositories()[0]),\n",
    "    zip_files= [],\n",
    "    docstore=docstore,\n",
    "    loaded_vectorstores=vectorstores,\n",
    "    model_type=\"Llama-3.2-1B\",\n",
    "    api_key=API_KEY,\n",
    "    search_kwargs={\"min_score\": 0.5, \"k\": 10}\n",
    ")\n",
    "documents = get_documents(\"What is Czech Poet?\", retriver)\n",
    "messages = rag([{\"role\": \"user\", \"content\": \"What is Czech Poet?\"}], documents, \"Llama-3.2-1B\", API_KEY, prompt=PROMPTS.INPUT_PROMPT)\n",
    "rag(messages + [{\"role\": \"user\", \"content\": \"How do I use it?\"}], documents, \"Llama-3.2-1B\", API_KEY, prompt=PROMPTS.INPUT_PROMPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

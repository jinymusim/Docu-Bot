{"user_input": "How do I build a docker image for dCache if I am new to the codebase?", "reference_contexts": ["Building dCache\n===============\n\nRequirements\n------------\n\nTo build dCache, you need Maven 3.5.0 or newer. Centos/Redhat 7\ncomes with version 3.0.5. See https://maven.apache.org/install.html\non how to install a newer version.\n\ndCache requires Java-11 for building. Running the resulting \nbinaries on newer JDKs should be possible.\n\nBuilding\n--------\n\ndCache uses Maven as a build system and as a repository of Maven\nartifacts. A top level aggregator project allows all Maven modules to\nbe built in one operation:\n\n    mvn package\n\nGenerated artifacts are to be found in the target directories inside\neach component.\n\nSpecific modules can be build using the -am and -pl options of Maven,\ne.g.:\n\n    mvn package -am -pl packages/tar\n\nwill build the _packages/tar_ module and all modules required by\n_packages/tar_.\n\nIt is not possible to cd into a subdirectory and execute Maven on a\nsingle module only. Such an attempt would fail because internal\ndependencies cannot be found in any Maven repository. Instead Maven\nmust be invoked on the top-level aggregator project with the\nappropriate options to compile specific modules.\n\nAll generated files can be removed through the clean phase:\n\n    mvn clean\n\nThis phase can also be combined with other phases, e.g.:\n\n    mvn clean package\n\nPackaging dCache\n----------------\n\nRPM and DEB packages can be build by compiling the _packages/fhs_\nmodule with the _rpm_ and _deb_ profiles, respectively, i.e.:\n\n    mvn clean package -am -pl packages/fhs -P rpm\n\nor\n\n    mvn clean package -am -pl packages/fhs -P deb\n\nThe two profiles must not be invoked in the same run. \n\nNote that the platform native RPM and DEB build tools must be installed. \nFor building RPMs on both CentOS and Debian platforms, installing \nthe rpm and rpmbuild packages suffices. \nFor building DEBs on Debian or Ubuntu platforms, the dpkg-dev, \ndebhelper, bash-completion, sh-make, quilt, and fakeroot packages\nmust be installed.\n\nThe package revision number can be customized by defining the\nbuild.number property, e.g.:\n\n    mvn clean package -am -pl packages/fhs -P deb -Dbuild.number=2\n\nRPM packages of the srmclient can be created by running the package\nphase with the _rpm_ profile on _modules/srm-client_ module, i.e.:\n\n    mvn clean package -am -pl modules/srm-client -P rpm\n\nDEB packages have not been defined for srmclient.\n\nThe generated packages can be found in the target directory of the\nrespectively module. RPMs are in the RPMS subdirectory.\n\n\nThe dCache tarball package is build by packaging the _packages/tar_\nmodule:\n\n    mvn clean package -am -pl modules/tar\n\n\nBuilding a docker image\n-----------------------\n\nBuilding a container image is disabled by default.  This is because it\nrequires a running docker deployment, which not all developers have\ninstalled.\n\nYou may enable building of docker images by activating the `container`\nprofile for the `tar` packaging:\n\n    mvn clean package -am -pl packages/tar -P container\n\n\nThe system-test module\n----------------------\n\nThe _packages/system-test_ module generates a ready-to-run single domain dCache\ninstance suitable for testing. It is configured to use an embedded database and\nthus to run without postgresql. To build it simply run:\n\n    mvn clean package -am -pl packages/system-test\n\nYou will need the `patch` utility for it to succeed.\nThis entails a completely self-contained dCache instance in\n`packages/system-test/target/dcache`. It can be started using:\n\n    packages/system-test/target/bin/ctlcluster start\n\nTo use GSI and TLS protocols, you may have to copy the test CA certificates\ninto your `/etc/grid-security/certificates/` directory. Follow the instructions\nshown at the end of the build.\n\nA test script is provided in `packages/system-test/target/bin/test` to execute\nvarious test transfers. This script is not intended for automatic testing. It\nrequires that various grid-related tools are available on the host.\n\n\nAn interactive database console for the embedded database can be started using\n\n    packages/system-test/target/bin/hsqldb DATABASE\n\nwhere DATABASE is the database to inspect, usually a service name, eg. space\nmanager or pin manager. The embedded database is single process, and thus the\nconsole can only be started if dCache is not running.\n\n\nMultiple versions of dCache can be tested for compatibility by installing\nolder versions using\n\n    packages/system-test/target/bin/ctlcluster install VERSION\n\nThis will download a tarball release of that version and install it in\n`packages/system-test/target`. The `ctlcluster` utility can be used to control\nall versions at once. Using the switch subcommand, particular services can\nbe moved between versions, eg.\n\n    packages/system-test/target/bin/ctlcluster switch pool.name=pool_write 2.7.5\n\nwill move the pool_write pool to version 2.7.5. The utility can however not\ncompensate for changes in configuration properties or database schemas between\nversions. Such incompatibilities have to be resolved manually.\n\n\nUnit tests\n----------\n\nBy default Maven executes all unit tests while building. This can be\ntime consuming and will fail if no internet connection is\navailable. The unit tests can be disabled by appending the `-DskipTests`\noption to any mvn command.\n"], "reference": "Building a container image is disabled by default because it requires a running docker deployment, which not all developers have installed. To enable building of docker images, you need to activate the `container` profile for the `tar` packaging by running the command: mvn clean package -am -pl packages/tar -P container.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is FTP in the context of dCache?", "reference_contexts": ["dCache\n======\n\n<img src=\"dCache.png\" height=\"165\" width=\"200\">\n\n__dCache__ is a system for storing and retrieving huge amounts of data,\ndistributed among a large number of heterogeneous server nodes, under\na single virtual filesystem tree with a variety of standard access\nmethods. Depending on the Persistency Model, dCache provides methods\nfor exchanging data with backend (tertiary) Storage Systems as well\nas space management, pool attraction, dataset replication, hot spot\ndetermination and recovery from disk or node failures. Connected to\na tertiary storage system, the cache simulates unlimited direct\naccess storage space. Data exchanges to and from the underlying HSM\nare performed automatically and invisibly to the user. Beside HEP\nspecific protocols, data in dCache can be accessed via __NFSv4.1\n(pNFS)__, __FTP__ as well as through __WebDav__.\n\n[![DOI](https://zenodo.org/badge/9113580.svg)](https://zenodo.org/badge/latestdoi/9113580)\n\nDocumentation\n=============\n\n[The dCache book](docs/TheBook/src/main/markdown/index.md)\n\n[User Guide](docs/UserGuide/src/main/markdown/index.md)\n\nGetting Started\n===============\n\nThe file [BUILDING.md](BUILDING.md) describes how to compile dCache\ncode and build various packages.\n\nThe file also describes how to create the __system-test__ deployment,\nwhich provides a quick and easy way to get a working dCache.  Running\nsystem-test requires no special privileges and all the generated files\nreside within the code-base.\n\nThere are also packages of stable releases at https://www.dcache.org/downloads/.\n\nLicense\n=======\n\nThe project is licensed under __AGPL v3__. Some parts licensed under __BSD__ and __LGPL__. See the source code for details.\n\nFor more info, check the official [dCache.ORG](http://www.dcache.org) web page.\n\nContributors\n============\ndCache is a joint effort between\n[Deutsches Elektronen-Synchrotron DESY](http://www.desy.de),\n[Fermi National Accelerator Laboratory](http://www.fnal.gov)\nand [Nordic DataGrid Facility](http://www.ndgf.org).\n\nHow to contribute\n=================\n\n**dCache** uses the linux kernel model where git is not only source repository,\nbut also the way to track contributions and copyrights.\n\nEach submitted patch must have a \"Signed-off-by\" line.  Patches without\nthis line will not be accepted.\n\nThe sign-off is a simple line at the end of the explanation for the\npatch, which certifies that you wrote it or otherwise have the right to\npass it on as an open-source patch.  The rules are pretty simple: if you\ncan certify the below:\n```\n\n    Developer's Certificate of Origin 1.1\n\n    By making a contribution to this project, I certify that:\n\n    (a) The contribution was created in whole or in part by me and I\n         have the right to submit it under the open source license\n         indicated in the file; or\n\n    (b) The contribution is based upon previous work that, to the best\n        of my knowledge, is covered under an appropriate open source\n        license and I have the right under that license to submit that\n        work with modifications, whether created in whole or in part\n        by me, under the same open source license (unless I am\n        permitted to submit under a different license), as indicated\n        in the file; or\n\n    (c) The contribution was provided directly to me by some other\n        person who certified (a), (b) or (c) and I have not modified\n        it.\n\n    (d) I understand and agree that this project and the contribution\n        are public and that a record of the contribution (including all\n        personal information I submit with it, including my sign-off) is\n        maintained indefinitely and may be redistributed consistent with\n        this project or the open source license(s) involved.\n\n```\nthen you just add a line saying ( git commit -s )\n\n    Signed-off-by: Random J Developer <random@developer.example.org>\n\nusing your real name (sorry, no pseudonyms or anonymous contributions.)\n\nWe use an adapted version of the [`Google style guide for Java`](https://github.com/google/styleguide) that can be found in the root of this project for IntelliJ.\nThe used reformatting involves optimization of imports (reordering), application of all syntactical sugar settings, but does not include code rearrangement (fields, methods, classes) or code cleanup for existing code. Reformatting should be applied to the changed code before submitting a patch.\n"], "reference": "FTP is one of the methods through which data in dCache can be accessed, alongside HEP specific protocols, NFSv4.1 (pNFS), and WebDav.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Can you explain how to compile and use the Nearline Storage Plugin with dCache, including the steps for verification and activation?", "reference_contexts": ["Nearline Storage Plugin for dCache\n==================================\n\nThis is nearline storage plugin for dCache.\n\nTo compile the plugin, run:\n\n    mvn package\n\nThis produces a tarball in the `target` directory containing the plugin.\n\nUsing the plugin with dCache\n----------------------------\n\nTo use this plugin with dCache, place the directory containing this\nfile in /usr/local/share/dcache/plugins/ on a dCache pool. Restart\nthe pool to load the plugin.\n\nTo verify that the plugin is loaded, navigate to the pool in the dCache admin\nshell and issue the command:\n\n    hsm show providers\n\nThe plugin should be listed.\n\nTo activate the plugin, create an HSM instance using:\n\n    hsm create osm name ${name} [-key=value]...\n\n"], "reference": "To compile the Nearline Storage Plugin for dCache, you need to run the command 'mvn package', which will produce a tarball in the 'target' directory containing the plugin. To use this plugin with dCache, place the directory containing the plugin file in '/usr/local/share/dcache/plugins/' on a dCache pool and restart the pool to load the plugin. To verify that the plugin is loaded, navigate to the pool in the dCache admin shell and issue the command 'hsm show providers'; the plugin should be listed. To activate the plugin, create an HSM instance using the command 'hsm create osm name ${name} [-key=value]...'.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How do I use the dCache plugin?", "reference_contexts": ["Nearline Storage Plugin for dCache\n==================================\n\nThis is nearline storage plugin for dCache.\n\nUsing the plugin with dCache\n----------------------------\n\nTo use this plugin with dCache, place the directory containing this\nfile in /usr/local/share/dcache/plugins/ on a dCache pool. Restart\nthe pool to load the plugin.\n\nTo verify that the plugin is loaded, navigate to the pool in the dCache admin\nshell and issue the command:\n\n    hsm show providers\n\nThe plugin should be listed.\n\nTo activate the plugin, create an HSM instance using:\n\n    hsm create osm name ${name} [-key=value]...\n\n"], "reference": "To use the dCache plugin, place the directory containing the plugin file in /usr/local/share/dcache/plugins/ on a dCache pool and restart the pool to load the plugin.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is NFS version 4 Protocol in relation to ACLs?", "reference_contexts": ["ACLs in dCache\n==============\n\ndCache includes support for Access Control Lists (ACLs). This support is conforming to the [NFS version 4 Protocol specification](https://tools.ietf.org/rfc/rfc7530.txt).\n\nThis chapter provides some background information and details on configuring dCache to use ACLs and how to administer the resulting system.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Introduction\n\ndCache allows control over namespace operations (e.g., creating new files and directories, deleting items, renaming items) and data operations (reading data, writing data) using the standard Unix permission model. In this model, files and directories have both owner and group-owner attributes and a set of permissions that apply to the owner, permissions for users that are members of the group-owner group and permissions for other users.\n\nAlthough Unix permission model is flexible enough for many deployment scenarios there are configurations that either cannot configured easily or are impossible. To satisfy these more complex permission handling dCache has support for ACL-based permission handling.\n\nAn Access Control List (ACL) is a set of rules for determining whether an end-user is allowed to undertake some specific operation. Each ACL is tied to a specific namespace entry: a file or directory. When an end-user wishes to undertake some operation then the ACL for that namespace entry is checked to see if that user is authorised. If the operation is to create a new file or directory then the ACL of the parent directory is checked.\n\n> **ACLs FOR FILES AND DIRECTORIES**\n>\n> Each ACL is associated with a specific file or directory in dCache. Although the general form is the same whether the ACL is associated with a file or directory, some aspects of an ACL may change. Because of this, we introduce the terms file-ACL and directory-ACL when taking about ACLs associated with a file or a directory respectively. If the term ACL is used then it refers to both file-ACLs and directory-ACLs.\n\nEach ACL contains a list of one or more Access Control Entries (ACEs). The ACEs describe how dCache determines whether an end-user is authorised. Each ACE contains information about which group of end users it applies to and describes whether this group is authorised for some subset of possible operations.\n\nThe order of the ACEs within an ACL is significant. When checking whether an end-user is authorised each ACE is checked in turn to see if it applies to the end-user and the requested operation. If it does then that ACE determines whether that end-user is authorised. If not then the next ACE is checked. Thus an ACL can have several ACEs and the first matched ACE �wins�.\n\nOne of the problems with traditional Unix-based permission model is its inflexible handling of newly created files and directories. With transitional filesystems, the permissions that are set are under the control of the user-process creating the file. The sysadmin has no direct control over the permissions that newly files or directories will have. The ACL permission model solves this problem by allowing explicit configuration using inheritance.\n\nACL inheritance is when a new file or directory is created with an ACL containing a set of ACEs from the parent directory's ACL. The inherited ACEs are specially marked so that only those that are intended will be inherited.\n\nInheritance only happens when a new file or directory is created. After creation, the ACL of the new file or directory is completely decoupled from the parent directory's ACL: the ACL of the parent directory may be altered without affecting the ACL of the new file or directory and visa versa.\n\nInheritance is optional. Within a directory's ACL some ACEs may be inherited whilst others are not. New files or directories will receive only those ACEs that are configured; the remaining ACEs will not be copied.\n\n## Configuring ACL support\n\nThe `dcache.conf` and layout files contain a number of settings that\nmay be adjusted to configure dCache's permission settings. These\nsettings are are described in this section.\n\nTo enable ACL support set `pnfsmanager.enable.acl`=`true` in the layout file.\n\n\n```ini\n[<domainName>/pnfsmanager]\npnfsmanager.enable.acl=true\n```\n\nAs NFSv4.0 protocol specification has a native ACL support, dCache's NFS door doesn't use this option and relays on `acl` option in export configurations. See [Exporting filesystem](config-nfs.md#exporting-filesystem) for details.\n\n### Setting and getting ACLs\n\nAltering dCache ACL behaviour is achieved by connecting to the `PnfsManager` [well-known cell](rf-glossary.md#well-known-cell) using the administrator interface. For further details about how to use the administrator interface, see [the section called �The Admin Interface�](intouch.md#the-admin-interface).\n\nThe `setfacl` command is used to set a new ACL. This command accepts\narguments with the following form `setfacl <ID> <ACE> [<ACE>...]`\n\nThe *ID* argument is either a pnfs-ID or the absolute path of some file or directory in dCache. The `setfacl` command requires ordered list of one or more *ACE* separated by spaces. Any existing ACEs for that *ID* will be replaced. The format and description of these ACE values are described below.\n\nThe `getfacl` is used to obtain the current ACL for objects in dCache namespace. It takes the following arguments.\n\n    getfacl [pnfsId] | [globalPath]\n\nThe `getfacl` command fetches the ACL information of a namespace item (a file or directory). The item may be specified by its PNFS-ID or its absolute path.\n\n#### Description of the ACE structure\n\nThe *ACE* arguments to the `setfacl` command have a specific format. This format is described below in Extended Backus-Naur Form (EBNF).\n\n    [1]\tACE\t::=\tSubject ':' Access |\n                            Subject ':' Access ':' Inheritance\n    [2]\tSubject\t::=\t'USER:' UserID | 'GROUP:' GroupID |\n        'OWNER@' |\n        'GROUP@' |\n        'EVERYONE@' |\n        'ANONYMOUS@' |\n        'AUTHENTICATED@'\n    [3]\tAccess\t::=\t'+' Mask |\n        '-' Mask\n    [4]\tMask\t::=\tMask MaskItem |\n        MaskItem\n    [5]\tMaskItem\t::=\t'r' | 'l' | 'w' | 'f' | 's' | 'a' | 'n' | 'N' | 'x' | 'd' | 'D' | 't' | 'T' | 'c' | 'C' | 'o'\n    [6]\tInheritance\t::=\tInheritance Flag | Flag\n    [7]\tFlag\t::=\t'f' | 'd' | 'o'\n    [8]\tUserID\t::=\tINTEGER\n    [9]\tGroupID\t::=\tINTEGER\nThe various options are described below.\n\n##### The Subject\n\nThe [Subject](#the-subject) defines to which user or group of users the ACE will apply. It acts as a filter so that only those users that match the Subject will have their access rights affected.\n\nAs indicated by the EBNF above, the Subject of an ACE can take one of several forms. These are described below:\n\n**USER:id**\nThe `USER:` prefix indicates that the ACE applies only to the specific end-user: the dCache user with ID *id*. For example, `USER:0:+w` is an ACE that allows user 0 to write over a file's existing data.\n\n**GROUP:id**\nThe `GROUP:` prefix indicates that the ACE applies only to those end-users who are a member of the specific group: the dCache group with ID *id*. For example, `GROUP:20:+a` is an ACE that allows any user who is a member of group 20 to append data to the end of a file.\n\n**OWNER@**\nThe `OWNER@` subject indicates that the ACE applies only to whichever end-user owns the file or directory. For example, `OWNER@:+d` is an ACE that allows the file's or directory's owner to delete it.\n\n**GROUP@**\nThe `GROUP@` subject indicates that the ACE applies only to all users that are members of the group-owner of the file or directory. For example, `GROUP@:+l` is an ACE that allows any user that is in a directory's group-owner to list the directory's contents.\n\n**EVERYONE@**\nThe `EVERYONE@` subject indicates that the ACE applies to all users. For example, `EVERYONE@:+r` is an ACE that makes a file world-readable.\n\n> It is important to note that \"EVERYONE@\" is not equivalent to the UNIX \"other\" entity.  This is because, by definition, UNIX \"other\" does not include the owner or owning group of a file. \"EVERYONE@\" means literally everyone, including the owner or owning group.\n\n**ANONYMOUS@**\nThe `ANONYMOUS@` Subject indicates that the ACE applies to all users who have not authenticated themselves. For example, `ANONYMOUS@:-l` is an ACE that prevents unauthenticated users from listing the contents of a directory.\n\n`AUTHENTICATED@`\nThe `AUTHENTICATED@` Subject indicates that an ACE applies to all authenticated users. For example, `AUTHENTICATED@:+r` is an ACE that allows any authenticated user to read a file's contents.\n\n> **AUTHENTICATED OR ANONYMOUS**\n>\n> An end user of dCache is either authenticated or is unauthenticated, but never both. Because of this, an end user operation will either match ACEs with `ANONYMOUS@` Subjects or `AUTHENTICATED@` Subjects but the request will never match both at the same time.\n\n##### Access mask\n\n[Access](#description-of-the-ace-structure) (defined in the [ACE EBNF](#description-of-the-ace-structure) above) describes what kind of operations are being described by the ACE and whether the ACE is granting permission or denying it.\n\nAn individual ACE can either grant permissions or deny them, but never both. However, an ACL may be composed of any mixture of authorising- and denying- ACEs. The first character of [Access](##description-of-the-ace-structure) describes whether the ACE is authorising or denying.\n\nIf [Access](#description-of-the-ace-structure) begins with a plus symbol (`+`) then the ACE authorises the [Subject](#description-of-the-ace-structure) some operations. The ACE `EVERYONE@:+r` authorises all users to read a file since the ACE-ACCESS begins with a `+`.\n\nIf the [Access](#description-of-the-ace-structure) begins with a minus symbol (`-`) then the ACE denies the [Subject](#description-of-the-ace-structure) some operations. The ACE `EVERYONE@:-r` prevents any user from reading a file since the Access begins with a `-`.\n\nThe first character of [Access](#description-of-the-ace-structure) must be `+` or `-`, no other possibility is allowed. The initial `+` or `-` of [Access](#description-of-the-ace-structure) is followed by one or more operation letters. These letters form the ACE's *access mask* ([Mask](#description-of-the-ace-structure) in [ACE EBNF](#description-of-the-ace-structure) above).\n\nThe access mask describes which operations may be allowed or denied by the ACE. Each type of operation has a corresponding letter; for example, obtaining a directory listing has a corresponding letter `l`. If a user attempts an operation of a type corresponding to a letter present in the access mask then the ACE may affect whether the operation is authorised. If the corresponding letter is absent from the access mask then the ACE will be ignored for this operation.\n\nThe following table describes the access mask letters and the corresponding operations:\n\n> **FILE- AND DIRECTORY- SPECIFIC OPERATIONS**\n>\n> Some operations and, correspondingly, some access mask letters only make sense for ACLs attached to certain types of items. Some operations only apply to directories, some operations are only for files and some operations apply to both files and directories.\n>\n> When configuring an ACL, if an ACE has an operation letter in the access mask that is not applicable to whatever the ACL is associated with then the letter is converted to an equivalent. For example, if `l` (list directory) is in the access mask of an ACE that is part of a file-ACL then it is converted to `r`. These mappings are described in the following table.\n\n**r**\nreading data from a file. Specifying `r` in an ACE's access mask controls whether end-users are allowed to read a file's contents. If the ACE is part of a directory-ACL then the letter is converted to `l`.\n\n**l**\nlisting the contents of a directory. Specifying `l` in an ACE's access mask controls whether end-users are allowed to list a directory's contents. If the ACE is part of a file-ACL then the letter is converted to `r`.\n\n**w**\noverwriting a file's existing contents. Specifying `w` in an ACE's access mask controls whether end-users are allowed to write data anywhere within the file's current offset range. This includes the ability to write to any arbitrary offset and, as a result, to grow the file. If the ACE is part of a directory-ACL then the letter is converted to `f`.\n\n**f**\ncreating a new file within a directory. Specifying `f` in an ACE's access mask controls whether end-users are allowed to create a new file. If the ACE is part of an file-ACL then then the letter is converted to `w`.\n\n**s**\ncreating a subdirectory within a directory. Specifying `s` in an ACE's access mask controls whether end-users are allowed to create new subdirectories. If the ACE is part of a file-ACL then the letter is converted to `a`.\n\n**a**\nappending data to the end of a file. Specifying `a` in an ACE's access mask controls whether end-users are allowed to add data to the end of a file. If the ACE is part of a directory-ACL then the letter is converted to `s`.\n\n**n**\nreading attributes. Specifying `n` in an ACE's access mask controls whether end-users are allowed to read attributes. This letter may be specified in ACEs that are part of a file-ACL and those that are part of a directory-ACL.\n\n**N**\nwrite attributes. Specifying `N` in an ACE's access mask controls whether end-users are allowed to write attributes. This letter may be specified in ACEs that are part of a file-ACL and those that are part of a directory-ACL.\n\n**x**\nexecuting a file or entering a directory. `x` may be specified in an ACE that is part of a file-ACL or a directory-ACL; however, the operation that is authorised will be different.\n\nSpecifying **x** in an ACEs access mask that is part of a file-ACL will control whether end users matching the ACE Subject are allowed to execute that file.\n\nSpecifying **x** in an ACEs access mask that is part of a directory-ACL will control whether end users matching ACE Subject are allowed to search a directory for a named file or subdirectory. This operation is needed for end users to change their current working directory.\n\n**d**\ndeleting a namespace entry. Specifying **d** in an ACE's access mask controls whether end-users are allowed to delete the file or directory the ACL is attached. The end user must be also authorised for the parent directory (see `D`).\n\n**D**\ndeleting a child of a directory. Specifying **D** in the access mask of an ACE that is part of a directory-ACL controls whether end-users are allowed to delete items within that directory. The end user must be also authorised for the existing item (see **d**).\n\n**t**\nreading basic attributes. Specifying `t` in the access mask of an ACE controls whether end users are allowed to read basic (i.e., non-ACL) attributes of that item.\n\n**T**\naltering basic attributes. Specifying `T` in an ACE's access mask controls whether end users are allowed to alter timestamps of the item the ACE's ACL is attached.\n\n**c**\nreading ACL information. Specifying `c` in an ACE's access mask controls whether end users are allowed to read the ACL information of the item to which the ACE's ACL is attached.\n\n**C**\nwriting ACL information. Specifying `C` in an ACE's access mask controls whether end users are allowed to update ACL information of the item to which the ACE's ACL is attached.\n\n**o**\naltering owner and owner-group information. Specifying `o` controls whether end users are allowed to change ownership information of the item to which the ACE's ACL is attached.\n\n##### ACL inheritance\n\nTo enable ACL inheritance, the optional [inheritance flags](#description-of-the-ace-structure) must be defined. The flag is a list of letters. There are three possible letters that may be included and the order doesn't matter.\n\n**f**\nThis inheritance flag only affects those ACEs that form part of an directory-ACL. If the ACE is part of a file-ACL then specifying `f` has no effect.\n\nIf a file is created in a directory with an ACE with `f` in inheritance flags then the ACE is copied to the newly created file's ACL. This ACE copy will not have the `f` inheritance flag.\n\nSpecifying `f` in an ACE's inheritance flags does not affect whether this ACE is inherited by a newly created subdirectory. See `d` for more details.\n\n**d**\nThis inheritance flag only affect those ACEs that form part of an directory-ACL. If the ACE is part of a file-ACL then specifying `d` has no effect.\n\nSpecifying `d` in an ACE's inheritance flags does not affect whether this ACE is inherited by a newly created file. See `f` for more details.\n\nIf a subdirectory is created in a directory with an ACE with `d` in the ACE's inheritance flag then the ACE is copied to the newly created subdirectory's ACL. This ACE copy will have the `d` inheritance flag specified. If the `f` inheritance flag is specified then this, too, will be copied.\n\n**o or r**\nThe `o` flag may only be used when the ACE also has the `f`, `d` or both `f` and `d` inheritance flags. The `o` flag is the same as the `r` flag.\n\nSpecifying `o` in the inheritance flag will suppress the ACE. No user operations will be authorised or denied as a result of such an ACE.\n\nWhen a file or directory inherits from an ACE with `o` in the inheritance flags then the `o` is *not* present in the newly created file or directory's ACE. Since the newly created file or directory will not have the `o` in it's inheritance flags the ACE will take effect.\n\nAn `o` in the inheritance flag allows child files or directories to inherit authorisation behaviour that is different from the parent directory.\n\n**g**\nThe `g` inheritance flag indicates this ACE refers to a group.\n\n### Accessing ACL over NFS mount\n\nAs mentioned earlier, NFSv4.0 natively supports ACL. On linux system there are two commands to get and set ACLs - `nfs4_getfacl` and `nfs4_setfacl`. The both commands are are a part of `nfs4-acl-tools` package and available on most of the modern linux distributions.\n\n    $ nfs4_setfacl -�s 'A::user@domain:r' file.X\n    $ nfs4_getfacl file.X\n    A::user@domain:r\n    A::OWNER@:rwaDxtTcC\n    A::GROUP@:rwaDxtc\n    A::EVERYONE@:rxtc\n\nFor other operations systems, usually ACL manipulation integrated into `setfacl` and `getfacl` commands.\n\n#### Combining UNIX permissions with ACLs\n\nWhen file's permission is changed with `chmod` command, then files ACL is adjusted to reflect new permissions.\n\n    $ nfs4_setfacl �a 'A::EVERYONE@:rw' file.X\n    $ nfs4_getfacl file.X\n    A::EVERYONE@:rwtc\n    A::OWNER@:rwatTcC\n    A::GROUP@:rwatc\n    $ chmod 000 file.X    # <==� ACL are adjusted here\n    $ nfs4_getfacl file.X\n    A::OWNER@:tTcC\n    A::GROUP@:t\n    A::EVERYONE@:t\n    $\n\n## Examples\n\nThis section gives some specific examples of how to set ACLs to achieve some specific behaviour.\n\n### Example 1\n\nACL allowing specific user to delete files in a directory\n\nThis example demonstrates how to configure a directory-ACL so user\n3750 can delete any file within the directory\n`/pnfs/example.org/data/exampleDir`.\n\n    (PnfsManager) admin > setfacl /pnfs/example.org/data/exampleDir EVERYONE@:+l USER:3750:D\n        (...line continues...)   USER:3750:+d:of\n    (PnfsManager) admin > setfacl /pnfs/example.org/data/exampleDir/existingFile1\n        (...line continues...)   USER:3750:+d:f\n    (PnfsManager) admin > setfacl /pnfs/example.org/data/exampleDir/existingFile2\n        (...line continues...)   USER:3750:+d:f\n\nThe first command creates an ACL for the directory. This ACL has three ACEs. The first ACE allows anyone to list the contents of the directory. The second ACE allows user 3750 to delete content within the directory in general. The third ACE is inherited by all newly created files and specifies that user 3750 is authorised to delete the file independent of that file's ownership.\n\nThe second and third commands creates an ACL for files that already exists within the directory. Since ACL inheritance only applies to newly created files or directories, any existing files must have an ACL explicitly set.\n\n### Example 2\n\nACL to deny a group  #\n\nThe following example demonstrates authorising all end users to list a directory. Members of group 1000 can also create subdirectories. However, any member of group 2000 can do neither.\n\n    (PnfsManager) admin > setfacl /pnfs/example.org/data/exampleDir GROUP:2000:-sl\n        (...line continues...)    EVERYONE@:+l GROUP:1000:+s\n\nThe first ACE denies any member of group 2000 the ability to create subdirectories or list the directory contents. As this ACE is first, it takes precedence over other ACEs.\n\nThe second ACE allows everyone to list the directory's content. If an end user who is a member of group 2000 attempts to list a directory then their request will match the first ACE so will be denied. End users attempting to list a directory that are not a member of group 2000 will not match the first ACE but will match the second ACE and will be authorised.\n\nThe final ACE authorises members of group 1000 to create subdirectories. If an end user who is a member of group 1000 and group 2000 attempts to create a subdirectory then their request will match the first ACE and be denied.\n\n### Example 3\n\nACL to allow a user to delete all files and subdirectories\n\nThis example is an extension to [Example 18.1, �ACL allowing specific user to delete files in a directory�](#example-18.1.-acl-allowing-specific-user-to-delete-files-in-a-directory). The previous example allowed deletion of the contents of a directory] but not the contents of any subdirectories. This example allows user 3750 to delete all files and subdirectories within the directory.\n\n\n   (PnfsManager) admin > setfacl /pnfs/example.org/data/exampleDir USER:3750:+D:d\n        (...line continues...)    USER:3750:+d:odf\n\nThe first ACE is `USER:3750:+D:d`. This authorises user 3750 to delete\nany contents of directory `/pnfs/example.org/data/exampleDir` that has\nan ACL authorising them with `d` operation.\n\nThe first ACE also contains the inheritance flag `d` so newly created subdirectories will inherit this ACE. Since the inherited ACE will also contain the `d` inheritance flag, this ACE will be copied to all subdirectories when they are created.\n\nThe second ACE is `USER:3750:+d:odf`. The ACE authorises user 3750 to\ndelete whichever item the ACL containing this ACE is associated\nwith. However, since the ACE contains the `o` in the inheritance\nflags, user 3750 is *not* authorised to delete the directory\n`/pnfs/example.org/data/exampleDir`\n\nSince the second ACE has both the `d` and `f` inheritance flags, it\nwill be inherited by all files and subdirectories of\n`/pnfs/example.org/data/exampleDir`, but without the `o` flag. This\nauthorises user 3750 to delete these items.\n\nSubdirectories (and files) will inherit the second ACE with both `d`\nand `f` inheritance flags. This implies that all files and\nsub-subdirecties within a subdirectory of\n`/pnfs/example.org/data/exampleDir` will also inherit this ACE, so\nwill also be deletable by user 3750.\n\n### Example 4\n\nObtain ACL information by absolute path\n\n       (PnfsManager) admin > getfacl /pnfs/example.org/data/exampleDir\n        ACL: rsId = 00004EEFE7E59A3441198E7EB744B0D8BA54, rsType = DIR\n        order = 0, type = A, accessMsk = lfsD, who = USER, whoID = 12457\n        order = 1, type = A, flags = f, accessMsk = lfd, who = USER, whoID = 87552\n        In extra format:\n        USER:12457:+lfsD\n        USER:87552:+lfd:f\n\nThe information is provided twice. The first part gives detailed information about the ACL. The second part, after the `In extra format:` heading, provides a list of ACEs that may be used when updating the ACL using the `setfacl` command.\n\n"], "reference": "NFS version 4 Protocol specification includes native support for Access Control Lists (ACLs), which allows for more complex permission handling compared to traditional Unix permission models.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the significance of the poolDomain in the dCache system?", "reference_contexts": ["THE ADMIN SERVICE\n=================\n\n> **JUST USE COMMANDS THAT ARE DOCUMENTED HERE**\n>\n> Only commands described in this documentation should be used for the administration of a dCache system.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## First Steps\n\ndCache has a powerful administration interface.  Administration protocol is implemented as `admin` cell that\nembeds `ssh` server. Once logged to admin interface an administrator can connect or send commands to other cells\nin the system.\n\nIt is useful to run the admin service in its own separate domain.\nIn the example of [the section called �Installing a dCache instance�](install.md)\nthis domain is called  adminDoorDomain:\n\n```ini\n[adminDoorDomain]\n[adminDoorDomain/admin]\n```\n\n> **Note**\n>\n> All configurable values of the ssh admin interface can be found in\n> the `/usr/share/dcache/defaults/admin.properties` file. Please do\n> NOT change any value in this file. Instead enter the key value\n> combination in the `/etc/dcache/dcache.conf`.\n\n\n## Access with SSH\n\nThe `admin` service embeds `ssh` server listening on port 22224 (configurable) and supports the following authentication mechanisms :\n\n- kerberos\n- password\n- public key authentication.\n\nThe mechanisms can be enabled by setting the following variable:\n\n```ini\nadmin.ssh.authn.enabled = password,publickey,kerberos\n```\n\n(that is comma separated mechanism names). By default `publickey` and `password` are enabled.\nTo enable `kerberos` it needs to be added to the list.\nTo complete `kerberos` setup the following variable needs to be defined:\n\n```ini\ndcache.authn.kerberos.realm=EXAMPLE.ORG\n```\n\nand `admin.ssh.authn.kerberos.keytab-file` should point existing keytab file. Default is `/etc/krb5.keytab`.\n\nThere are two ways of authorizing administrators to access the dCache `ssh` admin interface - public key based\nauthorization and `gPlazma` based authorization.  The configuration of both authorization mechanisms is\ndescribed below.\n\n\n### Public Key Authorization\n\nTo authorize administrators by their public key insert the key into\nthe file `authorized_keys2` which should be placed in the directory\n`/etc/dcache/admin` as specified in the file\n`/usr/share/dcache/defaults/admin.properties` under\n`admin.paths.authorized-keys`. Each key has to be one line (no line\nbreaks) and should have a standard format, such as:\n\n\n    ssh-rsa AAAAB3....GWvM= admin@localhost\n\n\n> **IMPORTANT**\n>\n> Please make sure that the copied key is still in one line. Any line-break will prevent the key from being read.\n>\n> DSA keys are deprecated and will not work.\n>\n> The key's comment (the part behind the equal sign) should match the admin user's name, e.g., admin@localhost\n\n\nNow you can login to the admin interface by\n\n```console-user\nssh -p 22224 -l admin headnode.example.org\n|dCache (##VERSION##)\n|Type \"\\?\" for help.\n```\n\nPublic key based authorization is default with a fallback to `gPlazma` `kpwd` plugin.\n\n\n### Access via gPlazma and the `dcache.kpwd` file\n\nTo use `gPlazma` make sure that you added it to your layout file :\n\n```ini\n[gplazmaDomain]\n[gplazmaDomain/gplazma]\n```\n\nThe `gPlazma` configuration file `/etc/dcache/gplazma.conf` has to\nlook like:\n\n```\nauth    sufficient      kpwd  \"kpwd=/etc/dcache/dcache.kpwd\"\nmap     sufficient      kpwd  \"kpwd=/etc/dcache/dcache.kpwd\"\nsession sufficient      kpwd  \"kpwd=/etc/dcache/dcache.kpwd\"\n```\n\nAdd a user `admin` to the `/etc/dcache/dcache.kpwd` file using the\n`dcache` script.\n\n>    Example:\n>    ```console-user\n>    dcache kpwd dcuseradd admin -u 12345 -g 1000 -h / -r / -f / -w read-write -p password\n>    |writing to /etc/dcache/dcache.kpwd :\n>    |\n>    |done writing to /etc/dcache/dcache.kpwd :\n>    ```\n\nAfter you ran the above command the following like appears in\n`/etc/dcache/dcache.kpwd` file:\n\n```\npasswd admin 4091aba7 read-write 12345 1000 / /\n```\n\nFor more information about `gPlazma` see [Chapter 10, Authorization in dCache](config-gplazma.md).\n\nNow the user `admin` can login to the admin interface with his password `password` by:\n\n```console-user\nssh -l admin -p 22224 headnode.example.org\n|admin@headnode.example.org's password:\n|dCache (##VERSION##)\n|Type \"\\?\" for help.\n```\n\nTo utilize kerberos authentication mechanism the following lines need\nto be added to `/etc/dcache/dcache.kpwd` file:\n\n```\nmapping \"johndoe@EXAMPLE.ORG\" admin\n\nlogin admin read-write 0 0 / / /\n  johndoe@EXAMPLE.ORG\n```\n\nThen, you can access dCache having obtained kerberos ticket:\n\n```console-user\nkinit johndoe@EXAMPLE.ORG\nssh -l admin -p 22224 headnode.example.org\n|dCache (##VERSION##)\n|Type \"\\?\" for help.\n```\n\n\nTo allow other users access to the admin interface add them to the `/etc/dcache/dcache.kpwd` file as described above.\n\nJust adding a user in the `dcache.kpwd` file is not sufficient. The generated user also needs access priileges that can only be set within the admin interface itself.\n\nSee [the section called �Creating a new user�](#creating-a-new-user) to learn how to create the user in the admin interface and set the rights.\n\n\n## How to use the Admin Interface\n\nAdmin interface allows you to execute shell commands, connect to other cells and execute their supported commands or\nsend supported cell commands to other cells. Once logged in you are prompted to use help `Type \"\\?\" for help`.\n\n```\n[headnode] (local) admin > \\?\n\\? [command]...  # display help for shell commands\n\\c cell[@domain] [user]  # connect to cell\n\\exception [trace]  # controls display of stack traces\n\\h [command]...  # display help for cell commands\n\\l [cell[@domain]|pool/poolgroup]...  # list cells\n\\q # quit\n\\s [OPTIONS] (cell[@domain]|pool/poolgroup)[,(cell[@domain]|pool/poolgroup)]... command...  # send command\n\\sl [options] pnfsid|path command...  # send to locations\n\\sn [options] command...  # send pnfsmanager command\n\\sp [options] command...  # send poolmanager command\n\\timeout [seconds]  # sets the command timeout\n\n[headnode] (local) admin >\n```\n\nShell commands are always available at command prompt, whereas in order to execute cell commands you have to either connect to the cell\nusing `\\c cell[@domain]` and execute command or send command to the cell using `\\s [OPTIONS] (cell[@domain]|pool/poolgroup)[,(cell[@domain]|pool/poolgroup)]... command...`. For instance:\n\n```\n[headnode] (local) enstore > \\? \\c\nNAME\n       \\c -- connect to cell\n\nSYNOPSIS\n       \\c cell[@domain] [user]\n\nDESCRIPTION\n       Connect to new cell. May optionally switch to another user.\n\nARGUMENTS\n       cell[@domain]\n              Well known or fully qualified cell name.\n       user\n              Account to connect with.\n\n[headnode] (local) enstore >\n```\n\nThe `\\l` command executed without arguments lists all well-known cells in the system. In general cells are\naddressed by their full name `<name>@<domainName>`. For well-known cells the `@<domainName>` part can be omitted.\nExecuting `\\l *@*` will list everything running in your dCache system.\n\n> **NOTE**\n>\n> If the cells are well-known, they can be accessed without adding the domain-scope. See [Cell Message passing](config-config-message-passing.md) for more information.\n\n\nEach cell implements `help [command]` (also aliased as `\\h [command]`) which, when executed\nwithout parameters,  displays a set of commands supported by the cell. When provided with command name as an argument\nit shows that command syntax like so:\n\n```\n[headnode] (local) admin > \\s pool_1 help log set\nSets the log level of <appender>.\n```\n\n> **NOTE**\n>\n> You can send (`\\s`) `help` command to a cell but you can't send `\\h` command to a cell. The `\\h` command can be executed only after connecting to a cell using `\\c` command:\n\n```\n[headnode] (local) admin > \\c pool_1\n[headnode] (pool_1@poolDomain) admin > \\h log set\nSets the log level of <appender>.\n```\n\n> **WARNING**\n>\n> Some commands are dangerous. Executing them without understanding what they do may lead to data loss.\n\n\nThe command `\\q` exits the admin shell.\n\nIf you want to find out which cells are running on a certain domain, you can issue the command `ps` in the System `cell` of the domain.\n\nExample:\nFor example, if you want to list the cells running on the `poolDomain`, `\\c` to its `System` cell and issue the `ps` command.\n\n      (local) admin > \\c System@poolDomain\n      (System@poolDomain) admin > ps\n        Cell List\n      ------------------\n      c-dCacheDomain-101-102\n      System\n      pool_2\n      c-dCacheDomain-101\n      pool_1\n      RoutingMgr\n      lm\n\nThe cells in the domain can be accessed using `\\c` together with the cell-name scoped by the domain-name. So first, one has to   get back to the local prompt, as the `\\c` command will not work otherwise.\n\n> **NOTE**\n>\n> Note that `\\c` only works from the local prompt. If the cell you are trying to access does not exist, the `\\c` command will   complain.\n>\n>     Example:\n>     (local) admin > \\c nonsense\n>     Cell as it doesn't exist\n>\n\n\nConnect to the routing manager of the `dCacheDomain` and use `ls` command to get a list of all well-known cells, running in\neach domain:\n\n    Example:\n      (local) admin > \\c RoutingMgr@dCacheDomain\n      (RoutingMgr@dCacheDoorDomain) admin > ls\n      Our routing knowledge :\n       Local : [PoolManager, topo, LoginBroker, info]\n       adminDoorDomain : [pam]\n      gsidcapDomain : [DCap-gsi-example.dcache.org]\n      dcapDomain : [DCap-example.dcache.org]\n      utilityDomain : [gsi-pam, PinManager]\n      gPlazmaDomain : [gPlazma]\n      webdavDomain : [WebDAV-example.dcache.org]\n       gridftpDomain : [GFTP-example.dcache.org]\n      srmDomain : [RemoteTransferManager, CopyManager, SrmSpaceManager, SRM-example.dcache.org]\n       httpdDomain : [billing, srm-LoginBroker, TransferObserver]\n      poolDomain : [pool_2, pool_1]\n       namespaceDomain : [PnfsManager, dirLookupPool, cleaner]\n\nAll cells implement the `info` command to display  general information about the cell and `show pinboard` command\nfor listing the last lines of the [pinboard](rf-glossary.md#pinboard) of the cell. The output of these commands contains useful information\nfor troubleshooting.\n\nThe most useful command of the pool cells is [rep ls](reference.md#rep-ls). It lists the file replicas\n which are stored in the pool by their `pnfs` IDs:\n\n```\n(RoutingMgr@dCacheDoorDomain) admin >  \\s pool_1  rep ls\n000100000000000000001120 <-P---------(0)[0]> 485212 si={myStore:STRING}\n000100000000000000001230 <C----------(0)[0]> 1222287360 si={myStore:STRING}\n(RoutingMgr@dCacheDoorDomain) admin >\n\n\n(RoutingMgr@dCacheDoorDomain) admin > \\c pool_1\n(pool_1) admin > rep ls\n000100000000000000001120 <-P---------(0)[0]> 485212 si={myStore:STRING}\n000100000000000000001230 <C----------(0)[0]> 1222287360 si={myStore:STRING}\n```\n\nEach file replica  in a pool has one of the 4 primary states: �cached� (<C---), �precious� (<-P--), �from client� (<--C-), and        �from store� (<---S).\n\n\nSee [the section called �How to Store-/Restore files via the Admin Interface�](config-hsm.md#how-to-store-restore-files-via-the-admin-interface) for more information about `rep ls`.\n\nThe most important commands in the `PoolManager` are: `rc ls` and `cm ls -r`.\n\n`rc ls` lists the requests currently handled by the `PoolManager`. A typical line of output for a read request with an error condition is (all in one line):\n\n```\n(pool_1) admin > \\c PoolManger\n(PoolManager) admin > rc ls\n000100000000000000001230@0.0.0.0/0.0.0.0 m=1 r=1 [<unknown>]\n[Waiting 08.28 19:14:16]\n{149,No pool candidates available or configured for 'staging'}\n```\n\nAs the error message at the end of the line indicates, no pool was found containing the file replica\n and no pool could be used for staging the file from a tertiary storage system.\n\n\n\nSee [the section called �Obtain information via the dCache Command Line Admin Interface�](config-hsm.md#how-to-monitor-whats-going-on) for more information about the command `rc ls`\n\nFinally, [cm ls](reference.md#cm-ls) with the option `-r` gives the information about the pools currently stored in the cost module of the pool manager. A typical output is:\n\n```\n(PoolManager) admin > cm ls -r\npool_1={R={a=0;m=2;q=0};S={a=0;m=2;q=0};M={a=0;m=100;q=0};PS={a=0;m=20;q=0};PC={a=0;m=20;q=0};\n    (...continues...)   SP={t=2147483648;f=924711076;p=1222772572;r=0;lru=0;{g=20000000;b=0.5}}}\npool_1={Tag={{hostname=example.org}};size=0;SC=0.16221282938326134;CC=0.0;}\npool_2={R={a=0;m=2;q=0};S={a=0;m=2;q=0};M={a=0;m=100;q=0};PS={a=0;m=20;q=0};PC={a=0;m=20;q=0};\n    (...continues...)   SP={t=2147483648;f=2147483648;p=0;r=0;lru=0;{g=4294967296;b=250.0}}}\npool_2={Tag={{hostname=example.org}};size=0;SC=2.7939677238464355E-4;CC=0.0;}\n```\n\n\nWhile the first line for each pool gives the information stored in the cache of the cost module, the second line gives the    costs (SC: [space cost](rf-glossary.md#space-cost), CC: [performance cost](rf-glossary.md#performance-cost)) calculated for a (hypothetical) file of zero size. For details on how these are calculated and their meaning, see [the section called �Classic Partitions�](#config-poolmanager.md#classic-partitions).\n\n## Creating a new user\n\nTo create a new user, <new-user> and set a new password for the user `\\c` from the local prompt `((local) admin >)` to the acm, the access control manager, and run following command sequence:\n\n```\n(local) admin > \\c acm\n(acm) admin > create user <new-user>\n(acm) admin > set passwd -user=<new-user> <newPasswd> <newPasswd>\n```\n\nFor the newly created users there will be an entry in the directory\n`/etc/dcache/admin/users/meta`.\n\n> **NOTE**\n>\n> As the initial user `admin` has not been created with the above\n> command you will not find him in the directory\n> `/etc/dcache/admin/users/meta`.\n\nGive the new user access to the PnfsManager.\n\n```\n(acm) admin > create acl cell.<cellName>.execute\n(acm) admin > add access -allowed cell.<cellName>.execute <new-user>\n```\n\nExample:\nGive the new user access to the PnfsManager.\n\n```\n(acm) admin > create acl cell.PnfsManager.execute\n(acm) admin > add access -allowed cell.PnfsManager.execute <new-user>\n```\n\nNow you can check the permissions by:\n\n```\n(acm) admin > check cell.PnfsManager.execute <new-user>\nAllowed\n(acm) admin > show acl cell.PnfsManager.execute\n<noinheritance>\n<new-user> -> true\n```\n\nThe following commands allow access to every cell for a user <new-user>:\n\n```\n(acm) admin > create acl cell.*.execute\n(acm) admin > add access -allowed cell.*.execute <new-user>\n```\n\n\nThe following command makes a user as powerful as admin (dCache�s equivalent to the root user):\n\n```\n(acm) admin > create acl *.*.*\n(acm) admin > add access -allowed *.*.* <new-user>\n```\n\n## Direct command execution\n\nAdmin ssh server allows direct command execution like so:\n\n```console-user\nssh -p 22224 admin@adminNode  \"command1; command2; command3\"\n```\n\nThat is it accepts semicolon (';') separated list of commands. Spaces between commands and command\nseparators do not matter.\n\n\n## Use of the SSH Admin Interface by scripts\n\nIn scripts, one can use a �Here Document� to list the commands, or supply them to `ssh` as standard-input (stdin). The following demonstrates using a Here Document:\n\n```shell\n#!/bin/sh\n#\n#  Script to automate dCache administrative activity\n\noutfile=/tmp/$(basename $0).$$.out\n\nssh -p 22224 admin@adminNode  > $outfile << EOF\n\\c PoolManager\ncm ls -r\n\\q\nEOF\n```\n\nor, the equivalent as stdin.\n\n```shell\n#!/bin/bash\n#\n#   Script to automate dCache administrative activity.\n\necho -e '\\c pool_1\\nrep ls\\n(more commands here)\\n\\q' \\\n  | ssh -p 22224 admin@adminNode \\\n  | tr -d '\\r' > rep_ls.out\n```\n"], "reference": "The poolDomain is significant in the dCache system as it contains the pool cells, such as pool_1 and pool_2, which are responsible for managing file replicas. It allows administrators to connect to the System cell of the poolDomain and execute commands to list the cells running within it.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What are the advantages of using RDBMS for the alarms service compared to XML?", "reference_contexts": ["CHAPTER 15. THE ALARMS SERVICE\n==============================\n\ndCache has an `alarms` backend service which records failures (*alarms*)\nrequiring more or less urgent intervention. The service stores alarms by\neither in an XML file or an RDBMS. The service is turned off by default.\n\nThe alarms service is contacted by the frontend service; alarms are collected\nusing a simple time range query.  Further filtering of the alarms occurs\nvia the RESTful alarms API.   The web dCache-View alarm page provides for\nboth the timestamp range and the filtering and sorting of alarms by fields.\nAdmins also have access to buttons which allow them to\nmark alarms as closed or to delete them altogether.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## The basic setup\n\nIt is not necessary to run the `alarms` service in a separate domain, though\ndepending on the individual system configuration it may still be advisable not\nto embed the service in a domain already burdened with higher memory requirements.\n\nThe alarms service stores only logging messages which are marked as alarms,\nregardless of the logging level at which these alarms are generated (usually\nas ERROR, but it is theoretically possible to create an alarm even as a TRACE\nlogging event).\n\nIn order to capture any alarms from other domains at startup, it is also necessary\nto arrange for the alarm service to start before the other doors and pools.\n\nCurrently, no support is given for multiplexed alarms services; the frontend\ncan only communicate with one such service.\n\nAdd the `alarms` service to a domain in the layout file in the usual manner:\n\n```ini\n[alarmsDomain]\n[alarmsDomain/alarms]\nalarms.db.type=rdbms\n```\n\nOR\n\n```ini\n[someotherDomain]\n[someotherDomain/alarms]\nalarms.db.type=rdbms\n[someotherDomain/someotherservice]\n```\n\nNote that the storage type setting `alarms.db.type` must be defined\neither in the layout or `/etc/dcache/dcache.conf` file because its\ndefault value is `off`; this can be set to either `xml`, or\n`rdbms`. In the latter case, the standard set of properties can be\nused to configure the connection url, db user, and so forth. Before\nusing the `rdbms` option for the first time, be sure to run:\n\n            createdb -U alarms.db.user alarms\n\nto create the database; as usual, the actual schema will be initialized\nautomatically when the service is started.\n\nFor the XML option, the storage file is usually found in the shared\ndirectory for alarms (corresponding to `alarms.dir`); the usual path\nis `/var/lib/dcache/store.xml`, but the location can be changed by\nsetting `alarms.db.xml.path`. This will automatically be propagated to\n`alarms.db.url`.\n\nAs a rule of thumb, the choice between XML and RDBMS is dictated by\nhow much history is to be preserved. While the XML option is more lightweight\nand easier to configure, it is limited by performance, experiencing considerable\nread and write slowdown as the file fills (beyond 1000 entries or so).\nIf you do not need to maintain records of alarms (and either manually delete\nalarms which have been serviced, or use the built-in cleanup feature - see below),\nthen this option should be sufficient. Otherwise, the extra steps of installing\npostgreSQL on the appropriate node and creating the alarms database (as above)\nmay be worth the effort.\n\n### Configure where the alarms service is running\n\nThe alarms infrastructure is actually a wrapper around the logging\nlayer and makes use of a simple tcp socket logger to transmit logging\nevents to the server. In each domain, the `/etc/dcache/logback.xml`\nconfiguration references the following properties to control remote\nlogging:\n\n```ini\ndcache.log.level.remote=off\ndcache.log.server.host=localhost\ndcache.log.server.port=9867\n```\n\nAs with the `alarms` service database type, remote logging is turned off by\ndefault. Under normal circumstances it should be sufficient to set this to `warn`\nin order to receive alarms. All alarms are currently in fact guaranteed to be\nsent at this logging level or higher. Setting this to a lower level should not\naffect performance, because even if there were alarms generated at, say, the\nINFO level, all events are filtered for being marked as alarms in the client\nbefore being sent over the wire; however, at present there is no need to set\nthe level any lower.\n\nFor the alarms endpoint, if all of your dCache domains run on the same\nhost, then the default `localhost` value will work. But usually your\ndCache will not be configured to run on a single node, so each node\nwill need to know the destination of the remote logging events. On all\nthe nodes except where the actual `alarms` service resides, you will\nthus need to modify the `/etc/dcache/dcache.conf` file or the layout\nfile to set the `dcache.log.server.host` property (and restart dCache\nif it is already up).  The default port should usually not need to be\nmodified; in any case, it needs to correspond to whatever port the\nservice is running on. From inspection of the\n`/usr/share/dcache/alarms.properties` file, you can see that the\nalarms-specific properties mirror the logger properties:\n\n```ini\n#  ---- Host on which this service is running\nalarms.net.host=${dcache.log.server.host}\n#  ---- TCP port the alarms service listens on\nalarms.net.port=${dcache.log.server.port}\n```\n\nThe first property should not need any adjustment, but if `alarms.net.port`\nis modified, be sure to modify the `dcache.log.server.port` property on the other\nnodes to correspond to it. In general, it is advisable to work directly\nwith the `dcache.log.server` properties everywhere.\n\nExample:\nAn example of a dCache which consists of a head node, some door nodes and some pool nodes.\nIf the head node contains the alarms service, set the property\n`dcache.log.server.host` on the pool nodes and on the door nodes to:\n\n```ini\ndcache.log.server.host=<head-node>\n```\n\n### Types of Alarms\n\nThe dCache alarm system runs on top of the logging system\n(and more specifically, depends on the `ch.qos.logback` logging library).\nIt promotes normal logging events to alarm status by a special marker.\nThey all carry this general logging marker `ALARM` and also can carry sub-markers\nfor type and uniqueness identifiers. They also carry information indicating the\nhost, domain and service which emits them.   All alarms are internally\ndefined by dCache.  For getting the list of types, see the commands below.\n\n### Alarm Priority\n\nThe notion of alarm or alert carries the implication that this particular error\nor condition requires user attention/intervention; there may be, however,\ndifferences in urgency which permit the ordering of such notices in terms of\ndegree of importance. dCache allows the administrator complete control over\nthis prioritization.\n\nThe available priority levels are:\n\n-   CRITICAL\n-   HIGH\n-   MODERATE\n-   LOW\n\nAny alarm can be set to whatever priority level is deemed appropriate.\nThis can be done through the admin interface commands (see below). Without\nany customization, all alarms are given a default priority level.\nThis level can itself be changed via the value of\n<variable>alarms.priority-mapping.default</variable>, which by default\nis `critical`.\n\nFiltering based on priority is possible both in the webadmin page (see below), and for alarms sent via email (<variable>alarms.email.threshold</variable>; fuller discussion of how to enable email alarms is given in a later section).\n\n### Working with Alarms: Shell Commands\n\nSome basic alarm commands are available as part of the dCache shell.\nThe following is an abbreviated description; for fuller information,\nsee the dCache man page.\n\n**alarm send**\nSend an arbitrary alarm message to the alarm server.\nThe remote server address is taken from the local values for\n<variable>dcache.log.server.host</variable> and <variable>dcache.log.server.port</variable>.\nIf the [-t=TYPE] option is used, it must be a defined alarm type.\n\n**alarm list**\nDisplays a list of all alarm types currently defined in dCache code.\nSince these types can be modified with any incremental release,\na listing in this manual would be of limited value. It is easy enough to\ncheck which ones currently are defined using this command or\nthe predefined ls admin command.\n\n### Working with Alarms: Admin Commands\n\nA similar set of commands is available through the admin interface.\nTo see fuller information for each of these, do `\\h [command]`.\n\n**predefined ls**\nPrint a list of all internally defined alarms.\n\n**priority get default**\nGet the current default alarm priority value.\n\n**priority ls [type]**\nPrint a single priority level or sorted list of priority levels for all known alarms.\n\n**priority reload [path]**\nReinitialize priority mappings from saved changes.\n\n**priority restore all**\nSet all defined alarms to the current default priority value.\n\n**priority save [path]**\nSave the current priority mappings to persistent back-up.\n\n**priority set type low|moderate|high|critical**\nSet the priority of the alarm type.\n\n**priority set default low|moderate|high|critical**\nSet the default alarm priority value.\n\n**send [OPTIONS] message**\nSend an alarm to the alarm service.\n\n> **NOTE**\n>\n> It is possible to change the file location by setting the\nalarms.priority-mapping.path and/or alarms.dir properties\nin the layout or `/etc/dcache/dcache.conf`. As can be seen from the admin commands,\nit is also possible to specify the path as an option on the respective\nsave and reload commands. Note, however, that this is meant mainly for temporary\nor back-up purposes, as the path defined in the local dcache configuration\nwill remain unaltered after that command completes and the priority map will be\nreloaded from there once again whenever the domain is restarted.\n\n> **NOTE**\n>\n> Any changes made via the priority set default command are in-memory only.\nTo change this default permanently, set the <variable>alarms.priority-mapping.default</variable>\nproperty in the layout or `/etc/dcache/dcache.conf`.\n\n\n### Working with Alarms: the dCache-View Alarms tab\n\nThe Alarms View is available to all users currently as read-only.  To be\nable to open/close or delete alarms, the user must log in with the 'admin' role\nif that user has it.   Please refer to the dCache-View and frontend documentation\nfor further information.\n\nThe alarms table allows for filtering and sorting on all alarms fields, and\nalso timestamped ranges for the basic query.\n\n## Advanced Service Configuration: enabling Automatic Cleanup\n\nAn additional feature of the alarms infrastructure is automatic cleanup of\nprocessed alarms. An internal thread runs every so often, and purges all alarms\nmarked as `closed` with a timestamp earlier than the given window.\nThis daemon can be configured using the properties\n`alarms.enable.cleaner`,\n`alarms.cleaner.timeout`,\n`alarms.cleaner.timeout.unit`,\n`alarms.cleaner.delete-entries-before` and\n`alarms.cleaner.delete-entries-before.unit`.\nThe cleaner is off by default. This feature is mainly useful when running\nover an XML store, to mitigate slow-down due to bloat; nevertheless,\nthere is nothing prohibiting its use with RDBMS.\n\n## Advanced Service Configuration: enabling email alerts\n\nTo configure the server to send alarms via email, you need to set a series of\nalarm properties. No changes are necessary to any **logback.xml** file.\nThe most important properties:\n\n**alarms.enable.email, alarms.email.threshold**\nOff (false) and `critical` by default.\n\n**alarms.email.smtp-host, alarms.email.smtp-port**\nEmail server destination. The port defaults to 25.\n\n**SMTP authentication and encryption**\nThe SMTP client used by dCache supports authentication via plain user\npasswords as well as both the STARTTLS and SSL protocols.\nNote that STARTTLS differs from SSL in that, in STARTTLS, the connection\nis initially non-encrypted and only after the STARTTLS command\nis issued by the client (if the server supports it) does the connection\nswitch to SSL. In SSL mode, the connection is encrypted right from the start.\nWhich of these to use is usually determined by the server.\n\nIf username and password are left undefined, unauthenticated sends\nwill be attempted, which may not be supported by the server.\n\nThe values to use for plain user/password authentication default\nto undefined. NOTE: while using SSL will guarantee encryption over the wire,\nthere is currently no way of storing an encrypted password. Two possible\nworkarounds: a. Set up an admin account with a plaintext password that is\nprotected by root privileges but which can be shared among adminstrators or\nthose with access to the host containing this file; b. Set up a host-based\nauthentication to the server; the email admin will usually require the client\nIP, and it will need to be static in that case.\n\n**sender and recipient**\nOnly one sender may be listed, but multiple recipients can be indicated\nby a comma-separated list of email addresses.\n\nSee the shared defaults `/usr/share/dcache/alarms.properties` file for\nadditional settings.\n\n## Miscellaneous Properties of the Alarms Service\n\nThere are a number of other settings avaible for customization; check\nthe files `/usr/share/dcache/alarms.properties` for the complete list\nwith explanations.\n\n## Alarms SPI (Service Provider Interface)\n\nIt is possible to plug functionality into the alarms service by\nimplementing the SPI interface.  An example of this can be found\nhere:\n\nhttps://github.com/dCache/dcache-snow\n\n  [Regular Expressions]: http://docs.oracle.com/javase/tutorial/essential/regex\n\n\n## Alarms and Kakfa\n\nAlarms is not a replicable service because it uses a logback client which\nrequires a fixed URL. Enabling dcache.log.level.kafka to send alarms\nto a kafka service is, however, possible, though without the alarms service,\none loses the ability to receive alarms in dCache-View, not to mention\nthe deletion and close functions, and also the ability to use the SPI plugins.\n"], "reference": "The choice between XML and RDBMS for the alarms service is dictated by how much history is to be preserved. While the XML option is more lightweight and easier to configure, it is limited by performance, experiencing considerable read and write slowdown as the file fills (beyond 1000 entries or so). If you do not need to maintain records of alarms, the XML option may be sufficient. However, if you require more robust storage and performance, especially for maintaining a larger history of alarms, the extra steps of installing PostgreSQL and creating the alarms database may be worth the effort.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Can you explain how to set up the billing database for dCache and what steps are involved in enabling it?", "reference_contexts": ["CHAPTER 14. THE BILLING SERVICE\n===============================\n\ndCache has built-in monitoring capabilities which provide an overview of the activity and performance of the installation�s doors and pools. There are two options for how this data can be represented and stored:\n\n-   a set of log files written to a known location\n-   a database (the\n    billing\n    database).\n\nThese options can be enabled simultaneously. If the database option is selected, the data in those tables will also be displayed as a set of histogram plots on the installation's web page.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## The Billing log files\n\nIf you installed dCache following the instructions in the Chapter [Installing dCache](install.md) you enabled the BILLING in the domain where the HTTPD service is running (see the extract of the layout file).\n\n```ini\n[httpdDomain]\n[httpdDomain/billing]\n[httpdDomain/httpd]\n```\n\nUse the property `billing.text.dir` to set the location of the log files and the property `billing.enable.text` to control whether the plain-text log files are generated.\n\nBy default the log files are located in the directory\n`/var/lib/dcache/billing`. Under this directory the log files are\norganized in a tree data structure based on date (YYYY/MM). A separate\nfile is generated for errors. The log file and the error file are\ntagged with the date.\n\nExample:\n\n        log file: /var/lib/dcache/billing/##TODAY_YEAR##/09/billing-##TODAY_YEAR##.09.25\n        error file: /var/lib/dcache/billing/##TODAY_YEAR##/09/billing-error-##TODAY_YEAR##.09.25\n\nThe log files may contain information about the time, the pool, the pnfsID and size of the transferred file, the storage class, the actual number of bytes transferred, the number of milliseconds the transfer took, the protocol, the subject (identity of the user given as a collection of principals), the data transfer listen port, the return status and a possible error message. The logged information depends on the protocol.\n\nA log entry for a write operation has the default format:\n\n    <MM.dd> <HH:mm:ss> [pool:<pool-name>:transfer]\n    [<pnfsId>,<filesize>] [<path>]\n    <StoreName>:<StorageGroup>@<type-of-storage-system>\n    <transferred-bytes>  <connectionTime> <true/false> {<protocol>}\n    <initiator>  {<return-status>:\"<error-message>\"}\n\nExample:\nA typical logging entry would look like this for writing. In the log file each entry is in one line. For readability we split it into separate lines in this documentation.:\n\n    12.10 14:19:42 [pool:pool2@poolDomain-1:transfer]\n    [0000062774D07847475BA78AC99C60F2C2FC,10475] [Unknown]\n    <Unknown>:<Unknown>@osm 10475 40 true {GFtp-1.0 131.169.72.103 37850}\n    [door:WebDAV-example.org@webdavDomain:1355145582248-1355145582485] {0:\"\"}\n\nThe formatting of the log messages can be customized by redefining the\n<billing.format.someInfoMessage> properties in the layout configuration, where\n<billing.format.someInfoMessage> can be replaced by\n\n-   billing.text.format.mover-info-message\n-   billing.text.format.remove-file-info-message\n-   billing.text.format.door-request-info-message\n-   billing.text.format.storage-info-message\n\nA full explanation of the formatting is given in the\n`/usr/share/dcache/defaults/billing.properties` file. For syntax\nquestions please consult [StringTemplate v3\ndocumentation](http://www.antlr.org/wiki/display/ST/StringTemplate+3+Documentation)\nor the [cheat\nsheet](http://www.antlr.org/wiki/display/ST/StringTemplate+cheat+sheet).\n\nOn the web page generated by the `httpd` service (default port 2288), there is a link to `Action Log`. The table which appears there gives a summary overview extracted from the data contained in the billing log files.\n\n## Billing High Availability\n\nTo replicate the billing service, the underlying store should be shared,\notherwise one risks potentially dispersing text records over several nodes.\nHence, a shared rdbms database instance should be enabled.  Absent a database,\nenabling kafka may offer an alternative to centralized record-keeping\nwithout the bottleneck of a single dCache service.\n\n## The Billing database\n\nIn order to enable the database, the following steps must be taken.\n\n1.  If the billing database does not already exist (see further below\n    on migrating from an existing one), create it (we assume PSQL\n    here):\n\n    ```console-root\n    createdb -O dcache -U postgres billing\n    ```\n\n    If you are using a version of PostgreSQL prior to 8.4, you will\n    also need to do:\n\n    ```console-root\n    createlang -U dcache plpgsql billing\n    ```\n\n    No further manual preparation is needed, as the necessary tables,\n    indices, functions and triggers will automatically be generated\n    when you (re)start the domain with the billing database logging\n    turned on (see below).\n\n2.  The property `billing.enable.db` controls whether the billing cell sends\nbilling messages to the database. By default the option is disabled.\nTo activate, set the value to `true` and restart the domain.\n\n### Customizing the database\n\nIn most cases, the billing service will be run out-of-the-box;\nnevertheless, the administrator does have control, if this is desired,\nover the database configuration.\n\n-   Database name, host, user, and password can be easily modified using the properties:\n\n    -   billing.db.name\n    -   billing.db.host\n    -   billing.db.user\n    -   billing.db.password\n\n    The current database values can be checked with the `dcache database ls` command.\n\n    ```console-root\n    dcache database ls\n    |DOMAIN          CELL        DATABASE HOST      USER    MANAGEABLE AUTO\n    |namespaceDomain PnfsManager chimera  localhost dcache  Yes        Yes\n    |namespaceDomain cleaner     chimera  localhost dcache  No         No\n    |billingDomain   billing     billing  localhost dcache  Yes        Yes\n    ```\n\n-   Database inserts are batched for performance. Since 2.8, improvements\nhave been made to the way the billing service handles these inserts, which can\nnow be tuned by adjusting the queue sizes (there are four of them, each mapped\nto the four main tables: billinginfo, storageinfo, doorinfo, hitinfo),\nand the maximum database batch size.\n\n    -   billing.db.inserts.max-queue-size\n        (defaults to\n        100000\n        )\n    -   billing.db.inserts.max-batch-size\n        (defaults to\n        1000\n        )\n\n    There is further the option as to whether to drop messages (default is true)\n    or block when the queue maximum is exceeded.\n\n    -   billing.db.inserts.drop-messages-at-limit\n        (defaults to\n        true\n        )\n\n    The default settings should usually be sufficient.\n\n    You can now obtain statistics (printed to the billing log and pinboard)\n    via the dcache admin command: `display insert statistics <on/off>` command.\n    Activating this command logs the following once a minute:\n\n                    insert queue (last 0, current 0, change 0/minute)\n                    commits (last 0, current 0, change 0/minute)\n                    dropped (last 0, current 0, change 0/minute)\n                    total memory 505282560; free memory 482253512\n\n    \"insert queue\" refers to how many messages actually were put on the queue;\n    \"commits\" are the number of messages committed to the database;\n    \"dropped\" are the number of lost messages.\n    \"last\" refers to the figures at the last iteration.\n    For insert queue, this is the actual size of the queue;\n    for commits and dropped, these are cumulative totals.\n\n    You can also generate a Java thread dump by issuing the `\"dump threads\"` command.\n\n### Database automatic truncation of fine-grained tables\n\nIt may be useful to limit the growth of the billing database, which\ncan fill up quickly over the course of time if there is a lot of\ndoor activity.\n\nA built-in cron can be set to run every 24 hours in order to remove\nolder rows from the \"fine-grained\" tables (``billinginfo``, ``doorinfo``,\n``storageinfo``, ``hitinfo``).\n\nThe following properties are relevant:\n\n```\nbilling.enable.db-truncate (default = false)\n\nbilling.db.fine-grained-truncate-before (default = 365)\nbilling.db.fine-grained-truncate-before.unit (default = DAYS)\n```\n\n## Billing histogram data\n\nIf the database has been enabled, dCache's frontend will regularly collect\nhistograms from it, and make these available via several RESTful API\ncalls.  Please refer to the frontend documentation for further infomartion.\n\nSimilarly, dCache-View (the web interface) includes a publicly available\ntab for plots generated from this data. These provide aggregate views of\nthe data for 24-hour, 7-day, 30-day and 365-day periods.\n\nThe plot types are:\n\n-   (Giga)bytes read and written for both dCache and HSM backend (if any)\n\n-   Number of transactions/transfers for both dCache and HSM backend (if any)\n\n-   Maximum, minimum and average connection time\n\n-   Cache hits and misses\n\n    > **NOTE**\n    >\n    > The data for this last histogram is not automatically sent,\n    > since it contributes significantly to message traffic between\n    > the pool manager and the billing service. To store this data\n    > (and thus generate the relevant plots), the\n    > `poolmanager.enable.cache-hit-message` property must be set\n    > either in `dcache.conf` or in the layout file for the domain\n    > where the poolmanager runs:\n    >\n    >     poolmanager.enable.cache-hit-message=true\n\n## Billing records\n\nThe frontend also provides an API for viewing the billing records for a given\nfile.  This data is also available through dCache-View.  Once again, please\nconsult those parts of the documentation for further information.\n\n## Upgrading a previous installation\n\nBecause it is possible that the newer version may be deployed over an existing installation which already uses the billing database, the Liquibase change-set has been written in such a way as to look for existing tables and to modify them only as necessary.\n\nIf you start the domain containing the `billing` service over a pre-existing installation of the billing database, depending on what was already there, you may observe some messages like the following in the domain log having to do with the logic governing table initialization.\n\nExample:\n\n\n       INFO 8/23/12 10:35 AM:liquibase: Successfully acquired change log lock\n       INFO 8/23/12 10:35 AM:liquibase: Reading from databasechangelog\n       INFO 8/23/12 10:35 AM:liquibase: Reading from databasechangelog\n       INFO 8/23/12 10:35 AM:liquibase: Successfully released change log lock\n       INFO 8/23/12 10:35 AM:liquibase: Successfully released change log lock\n       INFO 8/23/12 10:35 AM:liquibase: Successfully acquired change log lock\n       INFO 8/23/12 10:35 AM:liquibase: Reading from databasechangelog\n       INFO 8/23/12 10:35 AM:liquibase: Reading from databasechangelog\n       INFO 8/23/12 10:35 AM:liquibase: ChangeSet org/dcache/services/billing/\n       db/sql/billing.changelog-1.9.13.xml::4.1.7::arossi ran successfully in 264ms\n       INFO 8/23/12 10:35 AM:liquibase: Marking ChangeSet: org/dcache/services/\n       billing/db/sql/billing.changelog-1.9.13.xml::4.1.8::arossi::(Checksum:\n       3:faff07731c4ac867864824ca31e8ae81) ran despite precondition failure due\n       to onFail='MARK_RAN': classpath:org/dcache/services/billing/db/sql/\n       billing.changelog-master.xml : SQL Precondition failed. Expected '0' got '1'\n       INFO 8/23/12 10:35 AM:liquibase: ChangeSet org/dcache/services/billing/db/sql/\n       billing.changelog-1.9.13.xml::4.1.9::arossi ran successfully in 14ms\n       INFO 8/23/12 10:35 AM:liquibase: Successfully released change log lock\n       INFO 8/23/12 10:35 AM:liquibase: Successfully released change log lock\n\nAnything logged at a level lower than `ERROR` is usually entirely normal. Liquibase regularly reports when the preconditions determining whether it needs to do something are not met. All this means is that the update step was not necessary and it will be skipped in the future.\n\nIf, on the other hand, there is an `ERROR` logged by Liquibase, it is possible there may be some other conflict resulting from the upgrade (this should be rare). Such an error will block the domain from starting. One remedy which often works in this case is to do a clean re-initialization by dropping the Liquibase tables from the database:\n\n```console-root\npsql -U dcache billing\n|\n|billing=> drop table databasechangelog\n|billing=> drop table databasechangeloglock\n|billing-> \\q\n```\n\nand then restarting the domain.\n\n> **NOTE**\n>\n> If the billing database already exists, but contains tables other than the following:\n>\n> ```console-root\n> psql -U dcache billing\n> |    billing=> \\dt\n> |                         List of relations\n> |     Schema\t|         Name          | Type  |   Owner\n> |     -------+-----------------------+-------+-----------\n> |     public\t| billinginfo           | table | dcache\n> |     public\t| billinginfo_rd_daily  | table | dcache\n> |     public\t| billinginfo_tm_daily  | table | dcache\n> |     public\t| billinginfo_wr_daily  | table | dcache\n> |     public\t| databasechangelog     | table | dcache\n> |     public\t| databasechangeloglock | table | dcache\n> |     public\t| doorinfo              | table | dcache\n> |     public\t| hitinfo               | table | dcache\n> |     public\t| hitinfo_daily         | table | dcache\n> |     public\t| storageinfo           | table | dcache\n> |     public\t| storageinfo_rd_daily  | table | dcache\n> |     public\t| storageinfo_wr_daily  | table | dcache\n> |\n> |    billing-> \\q\n> ```\n>\n> that is, if it has been previously modified by hand or out-of-band to include custom tables not used directly by dCache, the existence of such extraneous tables should not impede dCache from working correctly, provided those other tables are `READ`-accessible by the database user for billing, which by default is `dcache`. This is a requirement imposed by the use of Liquibase. You thus may need explicitly to grant `READ` privileges to the billing database user on any such tables if they are owned by another database user.\n\n<!-- [Installing dCache]: #in\n  [StringTemplate v3 documentation]:\n         http://www.antlr.org/wiki/display/ST/StringTemplate+3+Documentation\n  [cheat sheet]: http://www.antlr.org/wiki/display/ST/StringTemplate+cheat+sheet\n  [DataNucleus]: http://www.datanucleus.org --!>\n\n\n"], "reference": "To set up the billing database for dCache, you need to follow these steps: First, if the billing database does not already exist, create it using the command 'createdb -O dcache -U postgres billing'. If you are using a version of PostgreSQL prior to 8.4, you will also need to run 'createlang -U dcache plpgsql billing'. No further manual preparation is needed, as the necessary tables, indices, functions, and triggers will be automatically generated when you restart the domain with the billing database logging turned on. Next, the property 'billing.enable.db' controls whether the billing cell sends billing messages to the database. By default, this option is disabled, so you need to set the value to 'true' and restart the domain to activate it.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of postgres in the configuration of the bulk service?", "reference_contexts": ["CHAPTER 18. THE BULK SERVICE\n===============================\n\nThe purpose of the bulk service is to manage mass file system operations involving lists of files\nand directories, the latter possibly treated recursively.  In one sense it is meant to incorporate\ninto dCache proper some of the functionality provided by SRM (such as prestaging of files).\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Configuring\n\nThe service can run in a shared domain, but it is probably best to isolate it in its own.\n\n```\n[bulkDomain]\n[bulkDomain/bulk]\n```\nThe bulk service has not yet been rendered replicable, but this should not be\ndifficult and may be in future versions.\n\nThe new version of bulk (version 2, as of dCache 8.2) requires a database consisting of\nthree tables.  As is customary, before the first startup of the service, be\nsure to create the database (e.g., using the postgres command-line tool):\n\n```\ncreatedb -U <user> bulk\n```\n\nThe rest of the configuration takes place automatically on startup.\n\nWhile things like thread queue sizes, cache expiration, consumer sweep interval and\nsuch can be adjusted as desired using the service properties, bulk is configured\nto run out of the box for most purposes.  See the ``bulk.properties`` file\nfor further information on settings.\n\nA number of user-facing (policy) settings can also be controlled from the admin\nshell without requiring service restart (see below).\n\n## Requests\n\nA bulk request consists of a list of targets (file paths) and an activity that\nshould be applied to each, along with options for directory expansion or\nrecursion, pre- and post-processing of the request.\n\nThe submission of a request by a user occurs via RESTful interfaces;\ncurrently, there is a general resource, ``bulk-requests``, and two resources\nwhich specifically support the WLCG Tape API, ``stage`` and ``release``.\nFor the differences, along with a description of the options controlling\nthe execution of the request, see the User Manual [Chapter 3. Frontend: Bulk Requests].\n\nThe bulk service receives these requests from the frontend service via dCache messaging.\nThe order in which they are processed is determined by a scheduler.  Currently, only\none such scheduler has been implemented,\n\n```\nbulk.request-scheduler=org.dcache.services.bulk.manager.scheduler.LeastRecentFirstScheduler\n```\n\nwhich selects the least recent requests to run in order of arrival (first-come first-served).\n\nOnly a fixed number of requests can be running at a given time; the default corresponds\nto the number of threads made available for processing container jobs,\n\n```\nbulk.limits.container-processing-threads=110\n```\n\nbut can be adjusted upwards or downwards via the admin shell command ``request policy,``\nin which case the thread pool will be either over- or under-subscribed; this adjustment\nmay be desirable under certain conditions (for example, increasing when there are many small\nhigh-latency jobs or decreasing when there are many large low-latency jobs). The number\nof requests which can be active or queued at a given time per user is also configurable.\n\n## Activities\n\nAn activity consists of some kind of action which can be taken on a file path.  Depending\non the activity, there may be a restriction as to whether it can take a directory target\n(these are hard-coded).\n\nActivities are defined via an SPI (\"Service Provider Interface\") as plugins to\nthe service.  The following are presently available:\n\n- **DELETE** : file deletion, with or without removal of empty directories.\n- **PIN/UNPIN** : by default, the pin uses the request id, but can optionally be given\na different id. The default lifetime is five minutes (the same as for the NFS dot-command).\n- **STAGE/RELEASE** : specialized WLCG versions of PIN/UNPIN.\n- **UPDATE_QOS** : disk-to-tape and tape-to-disk transitions; communicates with\nthe [QoS Engine](config-qos-engine.md).\n- **LOG_TARGET** : logs metadata for each target at the INFO level.\n\nEach activity is associated with a retry policy (currently the only retry policy is a NOP, i.e., no retry).\nShould other retry policies become available, these can be set via a property.\n\n## Container Design\n\nVersion 2 of the bulk service has introduced improvements for better scalability and recovery.\n\nAll requests are now processed by a single container job which executes the targets serially\nwhile the callbacks on any target futures are executed asynchronously.  A container can\nbe configured to store initial target information in the database either when the target\nis acted upon (on the fly) or in advance (= _prestore_).  The latter incurs some overhead,\nparticularly for jobs in excess of 10K targets, but may be desirable for tasks\nlike pinning where the time to completion is long, because it allows any queued targets to be seen\nahead of time.  All of the WLCG operations are hard-coded to prestore, but the general\ndefault is no prestorage.\n\nIn order to handle directories consistently, a container will queue them and delay their processing\nuntil all regular file targets have completed.  Directories are always processed\nserially depth-first.\n\nThe following illustrates the structure of the service and its interactions when processing\na request.\n\n![Bulk Design v2](images/bulk-design-v2.png)\n\nCancellation is best effort, in the sense that the cessation of the bulk activity is\nnot guaranteed to stop the activity in other dCache components.  Flushing, for instance,\ncannot be easily cancelled.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n NOTE: The user-driven attribute 'delay' for indicating how long to wait\n       before clearing a request on success or failure has been deprecated\n       and is no longer supported.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n## Storage and Recovery\n\nRequests are stored when they are received. When the service fails or goes down,\nan attempt will be made to replay incomplete requests upon restart.  This replay will leave\nuntouched request targets that are already in some terminal state (COMPLETED, FAILED, CANCELLED)\nand will retry only those that had not reached completion or had not yet been acted upon.\nIf the request was doing directory expansion, the directories are walked again and missing\ntargets are added if any.\n\n## Admin Commands\n\nThe bulk service comes with a full set of admin shell commands which allow one\nto list the requests and targets, to launch and cancel requests and individual targets,\nto list the available activity types and inspect their arguments, and to monitor the\nstate of the service itself.  The ``request reset`` command removes all request targets\nfrom the database, returns the state to QUEUED and resubmits the request (i.e., this\nis a clean-slate retry of the request).\n\nParticularly useful are the following policy properties which can be changed without\nrequiring service restart using the admin shell command ``request policy``:\n\n![Bulk Request Policy Command](images/bulk-request-policy.png)\n\n## Deprecation of the \"prestore\" option\n\nAs of 9.2, the `prestore` semantics (option and container type) have been eliminated.\n\nThere are two principal reasons for this.\n\nFirst, it is only relevant for recursive requests.  This is because\noriginally the initial targets were not automatically stored.  This\nwas discovered to be incorrect behavior.  Once initial targets are\nstored immediately, the rationale for storing the entire tree of\ntargets before beginning the processing no longer makes much sense.\n\nSecondly, the time-to-completion is significantly increased (by at\nleast an order of magnitude) because of the additional overhead\ninvolved in doing this on a sizeable tree of discovered targets.\n\nThe lazy storage of discovered targets, and the deferring of directories\nin memory until they are handled, which is the behavior of the default\ncontainer, does not pose problems for recovery, because the tree\nis rescanned in that case, and only the stored targets which have\nsuccessfully completed or failed are skipped; hence, the entire\ndirectory list is reconstituted on restart for any requests that\nhad not been completed before shutdown of the service.\n\n## Archiving of completed requests\n\nAs of 9.2, the archiving of completed or cancelled requests is available.\n\nThe archiver is set by default to run every 6 hours, checking requests whose last\nmodified timestamp is more than 14 days in the past and which are in a terminal\nstate. For each of these requests, a summary info object is constructed which\nrecords the original configuration and initial targets, with counts of the\nnumber of successful and failed targets and types of exceptions encountered.\nThis summary is stored in a special table, and the original request is deleted\nfrom the main tables.\n\nThe archiver is meant to provide a configurable way of relieving some of the\npressure on database querying with respect to active requests by allowing for\nsuch periodic deletion.   The relevant parameters can be initially controlled by\nthe two properties:\n\n```\n#  ---- Distance in the past from current time serving as threshold for archiving\n#       older completed or cancelled requests.  The archiver will archive and delete\n#       all requests older than this window.\n#\nbulk.limits.archiver-window=14\n(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)bulk.limits.archiver-window.unit=DAYS\n\n#  ---- How often the archiver should run.\n#\nbulk.limits.archiver-period=6\n(one-of?MILLISECONDS|SECONDS|MINUTES|HOURS|DAYS)bulk.limits.archiver-period.unit=HOURS\n```\n\nbut can also be changed dynamically in memory using the admin command:\n\n```\n[fndcatemp2] (bulk@bulkDomain) admin > \\h archiver reset\nNAME\n       archiver reset -- Change the archiver settings.\n\nSYNOPSIS\n       archiver reset [-period=long]\n       [-periodUnit=milliseconds|seconds|minutes|hours|days]\n       [-window=long]\n       [-windowUnit=milliseconds|seconds|minutes|hours|days]\n\nDESCRIPTION\n       Interrupts the archiver and resets the period and/or window.\n\nOPTIONS\n         -period=long\n              how often to run the archiver.\n         -periodUnit=milliseconds|seconds|minutes|hours|days\n              unit for how often to run the archiver.\n         -window=long\n              how far into the past from now the cutoff should be;\n              requests whose last modification time is before this point\n              and have completed will be deleted.\n         -windowUnit=milliseconds|seconds|minutes|hours|days\n              unit for the cutoff window.\n\n```\n\nThe archiver can also be forced to run using:\n\n```\n[fndcatemp2] (bulk@bulkDomain) admin > \\h archiver run\nNAME\n       archiver run -- Run the archiver now.\n\nSYNOPSIS\n       archiver run\n\nDESCRIPTION\n       Interrupts the archiver, schedules it to run now, and then\n       periodically, as before.\n```\n\n**Note that clearing the request either through the `REST` API or the admin command\nwill now similarly archive the request before deleting it from the active tables.**\n\nThe admin commands available for listing and retrieving archived requests are:\n\n```\n[fndcatemp2] (bulk@bulkDomain) admin > \\h request archived ls\nNAME\n       request archived ls -- List archived requests\n\nSYNOPSIS\n       request archived ls [-activity=string[,string]...]...\n       [-after=yyyy/mm/dd-hh:mm:ss] [-before=yyyy/mm/dd-hh:mm:ss]\n       [-limit=integer] [-owner=string[,string]...]...\n       [-status=completed|cancelled[,completed|cancelled]...]...\n\nDESCRIPTION\n       Display the uid, owner, last modified, activity and status for\n       the archived request.\n\nOPTIONS\n         -activity=string[,string]...\n              The request activity.\n         -after=yyyy/mm/dd-hh:mm:ss\n              Select requests with last modified date-time after\n              date-time.\n         -before=yyyy/mm/dd-hh:mm:ss\n              Select requests with last modified date-time before\n              date-time.\n         -limit=integer\n              Return maximum of this many entries. Defaults to 5000.\n         -owner=string[,string]...\n              The owner of the request.\n         -status=completed|cancelled[,completed|cancelled]...\n              Status of the request.\n```\n\nwhose output looks like this:\n\n```\n[fndcatemp2] (bulk@bulkDomain) admin > request archived ls\n346b2a47-264e-47da-91ea-1dd4a9bdf6c3     |         0:0 | Sat Aug 05 14:28:43 CDT 2023 |   LOG_TARGET | COMPLETED\n8fd1d7cf-2153-4572-8629-f761f7b6a382     |         0:0 | Sat Aug 05 14:30:02 CDT 2023 |   LOG_TARGET | COMPLETED\nf5d885aa-e447-4d33-974b-5087e5abcf01     |         0:0 | Sat Aug 05 14:33:53 CDT 2023 |   LOG_TARGET | COMPLETED\n...\n\nTo see the full information for a given archived request, use:\n\n```\n\n```\n[fndcatemp2] (bulk@bulkDomain) admin > \\h request archived info\nNAME\n       request archived info -- Get archived request info\n\nSYNOPSIS\n       request archived info rid\n\nDESCRIPTION\n       Display the JSON info for a single archived request.\n\nARGUMENTS\n       rid\n              The id of the request.\n```\n\nwhich returns a JSON output:\n\n```\n[fndcatemp2] (bulk@bulkDomain) admin > request archived info 95b93210-e0d1-4cdd-9877-5250bbad2415\n{\n  \"initialTargets\" : [ \"/pnfs/fs/usr/fermilab/users/arossi/any/205/data-2023073107031690805037697499944\" ],\n  \"errors\" : {\n    \"diskCacheV111.util.TimeoutCacheException\" : 1\n  },\n  \"uid\" : \"95b93210-e0d1-4cdd-9877-5250bbad2415\",\n  \"owner\" : \"0:0\",\n  \"activity\" : \"UPDATE_QOS\",\n  \"depth\" : \"NONE\",\n  \"arrivedAt\" : 1691596463050,\n  \"startedAt\" : 1691596463271,\n  \"lastModified\" : 1691596523300,\n  \"status\" : \"COMPLETED\",\n  \"targetPrefix\" : null,\n  \"succeeded\" : 0,\n  \"cancelled\" : 0,\n  \"failed\" : 1\n}\n```\n\nThe corresponding `REST` API resources for these two commands are:\n\n```\n/bulk-requests/archived\nList the status information for an individual bulk request matching the query parameters (if any).\n\n/bulk-requests/archived/{id}\nGet the information for a bulk request which has been archived.\n```\n\nAs usual, listing is world-readable but `info` requires either ownership of the original request\nor admin privileges."], "reference": "The new version of bulk (version 2, as of dCache 8.2) requires a database consisting of three tables. Before the first startup of the service, it is necessary to create the database using the postgres command-line tool with the command: `createdb -U <user> bulk`.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What are the key components and functionalities of Chimera in the context of dCache?", "reference_contexts": ["CHIMERA\n=======\n\n-----\n[TOC bullet hierarchy]\n-----\n\nThe inner dCache components talk to the namespace via a module called `PnfsManager`, which in turn communicates with the Chimera database using a thin Java layer. In addition to `PnfsManager` a direct access to the file system view is provided by an `NFSv3` and `NFSv4.1` server. Clients can `NFS`-mount the namespace locally. This offers the opportunity to use OS-level tools like `ls, mkdir, mv` for Chimera. Direct I/O-operations like `cp` and `cat` are possible with the `NFSv4.1 door`.\n\nThe properties of Chimera are defined in\n`/usr/share/dcache/defaults/chimera.properties`. For customisation the\nfiles `/etc/dcache/layouts/mylayout.conf` or `/etc/dcache/dcache.conf`\nshould be modified (see [the section called �Defining domains and\nservices�](install.md#defining-domains-and-services).\n\nExample:\n\nThis example shows an extract of the\n`/etc/dcache/layouts/mylayout.conf` file in order to run dCache with\n`NFSv3`.\n\n```ini\n[namespaceDomain]\n[namespaceDomain/pnfsmanager]\n[namespaceDomain/nfs]\nnfs.version=3\n```\n\nExample:\n\nIf you want to run the NFSv4.1 server you need to add the\ncorresponding nfs service to a domain in the\n`/etc/dcache/layouts/mylayout.conf` file and start this domain.\n\n```ini\n[namespaceDomain]\n[namespaceDomain/pnfsmanager]\n[namespaceDomain/nfs]\nnfs.version = 4.1\n```\n\nIf you wish dCache to access your Chimera with a PostgreSQL user other\nthan chimera then you must specify the username and password in\n`/etc/dcache/dcache.conf`.\n\n```ini\nchimera.db.user=myuser\nchimera.db.password=secret\n```\n\n## Attribute consistency policy\n\nOn new filesystem object creation in a directory the `modification` and\n`change id` attributes must be updated to provide a consistent, up-to-date view of the changes.\nIn highly concurrent environments such updates might create so-called `hot inodes`\nand serialize all updates in a single directory, thus, reducing the namespace throughput. \n\nAs such strong consistency is not always required, to improve concurrent updates to  a single directory\nthe POSIX constraints can be relaxed. The `chimera.attr-consistency` attribute controls the namespace attribute\nupdate bahaviour of a parent directory on update:\n\n| policy | behaviour                                                                                                                                                                                                                                   |\n|:-------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| strong | a creation of a filesystem object will right away update parent directory's mtime, ctime, nlink and generation attributes                                                                                                                   |\n| weak   | a creation of a filesystem object will eventually update (after 30 seconds) parent directory's mtime, ctime, nlink and generation attributes. Multiple concurrent modifications to a directory are aggregated into single attribute update. |\n| soft   | same as weak, however, reading of directory attributes will take into account pending attribute updates.                                                                                                                                    |\n\nRead-write exported NFS doors SHOULD run with `strong consistency` or `soft consistency` to maintain POSIX\ncompliance. Read-only NFS doors might run with `weak consistency` if non-up-to-date directory attributes can\nbe tolerated, for example when accessing existing data, or  `soft consistency`, if up-to-date information\nis desired, typically when seeking for newly arrived files through other doors.\n\n```\nchimera.attr-consistency=strong\n```\n\n## Mounting Chimera through NFS\n\ndCache does not need the Chimera filesystem to be mounted, but a mounted file system is convenient for administrative access. This offers the opportunity to use OS-level tools like `ls` and `mkdir` for Chimera. However, direct I/O-operations like `cp` are not possible, since the `NFSv3` interface provides the namespace part only. This section describes how to start the Chimera `NFSv3` server and mount the name space.\n\nIf you want to mount Chimera for easier administrative access, you\nneed to edit the `/etc/exports` file as the Chimera `NFS` server uses\nit to manage exports. If this file doesn�t exist it must be\ncreated. The typical exports file looks like this:\n\n    / localhost(rw)\n    /data\n    # or\n    # /data *.my.domain(rw)\n\nAs any RPC service Chimera `NFS` requires `rpcbind` service to run on the host. Nevertheless rpcbind has to be configured to accept requests from Chimera NFS.\n\nOn RHEL6 based systems you need to add\n\n    RPCBIND_ARGS=\"-i\"\n\ninto `/etc/sysconfig/rpcbind` and restart `rpcbind`. Check your OS\nmanual for details.\n\n```console-root\nservice rpcbind restart\n|Stopping rpcbind:                     [  OK  ]\n|Starting rpcbind:                     [  OK  ]\n```\n\nIf your OS does not provide `rpcbind` Chimera `NFS` can use an embedded `rpcbind`. This requires to disable the `portmap` service if it exists.\n\n```console-root\n/etc/init.d/portmap stop\n|Stopping portmap: portmap\n```\n\nand restart the domain in which the `NFS` server is running.\n\n```console-root\ndcache restart namespaceDomain\n```\n\nNow you can mount Chimera by\n\n```console-root\nmount localhost:/ /mnt\n```\n\nand create the root of the CHIMERA namespace which you can call **data**:\n\n```console-root\nmkdir -p /mnt/data\n```\n\nIf you don�t want to mount chimera you can create the root of the Chimera namespace by\n\n```console-root\nchimera mkdir /data\n```\n\nYou can now add directory tags. For more information on tags see [the section called �Directory Tags�](config-chimera.md#directory-tags).\n\n```console-root\nchimera writetag /data sGroup \"chimera\"\nchimera writetag /data OSMTemplate \"StoreName sql\"\n```\n\n\n### Using DCAP with a mounted file system\n\nIf you plan to use `dCap` with a mounted file system instead of the\nURL-syntax (e.g. `dccp /data/file1 /tmp/file1`), you need to mount\nthe root of Chimera locally (remote mounts are not allowed yet). This\nwill allow us to establish wormhole files so `dCap` clients can\ndiscover the `dCap` doors.\n\n```console-root\nmount localhost:/ /mnt\nmkdir /mnt/admin/etc/config/dCache\ntouch /mnt/admin/etc/config/dCache/dcache.conf\ntouch /mnt/admin/etc/config/dCache/'.(fset)(dcache.conf)(io)(on)'\necho \"<door host>:<port>\" > /mnt/admin/etc/config/dCache/dcache.conf\n```\n\nThe default values for ports can be found in [Chapter 29, dCache\nDefault Port Values](rf-ports.md) (for `dCap` the default port is\n22125) and in the file\n`/usr/share/dcache/defaults/dcache.properties`. They can be altered in\n`/etc/dcache/dcache.conf`\n\nCreate the directory in which the users are going to store their data and change to this directory.\n\n```console-root\nmkdir -p /mnt/data\ncd /mnt/data\n```\n\nNow you can copy a file into your dCache\n\n```console-root\ndccp /bin/sh test-file\n|735004 bytes (718 kiB) in 0 seconds\n```\n\nand copy the data back using the `dccp` command.\n\n```console-root\ndccp test-file /tmp/testfile\n|735004 bytes (718 kiB) in 0 seconds\n```\n\nThe file has been transferred succesfully.\n\nNow remove the file from the dCache.\n\n```console-root\nrm  test-file\n```\n\nWhen the configuration is complete you can unmount Chimera:\n\n```console-root\numount /mnt\n```\n\n> **NOTE**\n>\n> Please note that whenever you need to change the configuration, you\n> have to remount the root `localhost:/` to a temporary location like\n> `/mnt`.\n\n## Communicating with Chimera\n\nMany configuration parameters of Chimera and the application specific\nmeta data is accessed by reading, writing, or creating files of the\nform `.(command)(para)`. For example, the following prints the\nChimeraID of the file `/data/some/dir/file.dat`:\n\n```console-user\ncat /data/any/sub/directory/'.(id)(file.dat)'\n|0004000000000000002320B8\n```\n\nFrom the point of view of the `NFS` protocol, the file\n**.(id)(file.dat)** in the directory `/data/some/dir/` is\nread. However, Chimera interprets it as the command id with the\nparameter file.dat executed in the directory `/data/some/dir/`. The\nquotes are important, because the shell would otherwise try to\ninterpret the parentheses.\n\nSome of these command files have a second parameter in a third pair of\nparentheses. Note, that files of the form `.(command)(para)` are not\nreally files. They are not shown when listing directories with\n`ls`. However, the command files are listed when they appear in the\nargument list of `ls` as in\n\n```console-user\nls -l '.(tag)(sGroup)'\n|-rw-r--r-- 11 root root 7 Aug 6 2010 .(tag)(sGroup)\n```\n\nOnly a subset of file operations are allowed on these special command files. Any other operation will result in an appropriate error. Beware, that files with names of this form might accidentally be created by typos. They will then be shown when listing the directory.\n\n## IDs\n\nEach file in Chimera has a unique 18 byte long ID. It is referred to as ChimeraID or as pnfsID. This is comparable to the inode number in other filesystems. The ID used for a file will never be reused, even if the file is deleted. dCache uses the ID for all internal references to a file.\n\nExample:\n\nThe ID of the file `/example.org/data/examplefile` can be obtained by\nreading the command-file `.(id)(examplefile)` in the directory of the\nfile.\n\n```console-user\ncat /example.org/data/'.(id)(examplefile)'\n|0000917F4A82369F4BA98E38DBC5687A031D\n```\n\nA file in Chimera can be referred to by the ID for most operations.\n\nExample:\n\nThe name of a file can be obtained from the ID with the command `nameof` as follows:\n\n```console-user\ncd /example.org/data/\ncat '.(nameof)(0000917F4A82369F4BA98E38DBC5687A031D)'\n|examplefile\n```\n\nAnd the ID of the directory it resides in is obtained by:\n\n```console-user\ncat '.(parent)(0000917F4A82369F4BA98E38DBC5687A031D)'\n|0000595ABA40B31A469C87754CD79E0C08F2\n```\n\nThis way, the complete path of a file may be obtained starting from the ID.\n\n## Directory tags\n\nIn the Chimera namespace, each directory can have a number of tags. These directory tags may be used within dCache to control the file placement policy in the pools (see [the section called �The Pool Selection Mechanism�](config-PoolManager.md#the-pool-selection-mechanism)). They might also be used by a [tertiary storage system](config-hsm.md) for similar purposes (e.g. controlling the set of tapes used for the files in the directory).\n\n> **NOTE**\n>\n> Directory tags are not needed to control the behaviour of dCache. dCache works well without directory tags.\n\n### Create, list and read directory tags if the namespace is not mounted\n\nYou can create tags with\n\n```console-root\nchimera writetag <directory> <tagName> \"<content>\"\n```\n\nlist tags with\n\n```console-root\nchimera lstag <directory>\n```\n\nand read tags with\n\n```console-root\nchimera readtag <directory> <tagName>\n```\n\nExample:\nCreate tags for the directory `data` with\n\n```console-root\nchimera writetag /data sGroup \"myGroup\"\nchimera writetag /data OSMTemplate \"StoreName myStore\"\n```\n\nlist the existing tags with\n\n```console-root\nchimera lstag /data\n|Total: 2\n|OSMTemplate\n|sGroup\n```\n\nand their content with\n\n```console-root\nchimera readtag /data OSMTemplate\n|StoreName myStore\nchimera readtag /data sGroup\n|myGroup\n```\n\n### Create, list and read directory tags if the namespace is mounted\n\nIf the namespace is mounted, change to the directory for which the tag\nshould be set and create a tag with\n\n```console-user\ncd <directory>\necho '<content1>' > '.(tag)(<tagName1>)'\necho '<content2>' > '.(tag)(<tagName2>)'\n```\n\n\nThen the existing tags may be listed with\n\n```console-user\ncat '.(tags)()'\n|.(tag)(<tagname1>)\n|.(tag)(<tagname2>)\n```\n\nand the content of a tag can be read with\n\n```console-user\ncat '.(tag)(<tagname1>)'\n|<content1>\ncat '.(tag)(<tagName2>)'\n|<content2>\n```\n\nIn the following example, two tags are created, listed and their\ncontents shown.\n\nFirst, create tags for the directory `data` with\n\n```console-user\ncd data\necho 'StoreName myStore' > '.(tag)(OSMTemplate)'\necho 'myGroup' > '.(tag)(sGroup)'\n```\n\nlist the existing tags with\n\n```console-user\ncat '.(tags)()'\n|.(tag)(OSMTemplate)\n|.(tag)(sGroup)\n```\n\nand their content with\n\n```console-user\ncat '.(tag)(OSMTemplate)'\n|StoreName myStore\ncat '.(tag)(sGroup)'\n|myGroup\n```\n\nA nice trick to list all tags with their contents is\n\n```console-user\ngrep \"\" $(cat  \".(tags)()\")\n|.(tag)(OSMTemplate):StoreName myStore\n|.(tag)(sGroup):myGroup\n```\n\n### Directory tags and command files\n\nWhen creating or changing directory tags by writing to the command\nfile as in\n\n```console-user\necho '<content>' > '.(tag)(<tagName>)'\n```\n\none has to take care not to treat the command files in the same way as\nregular files, because tags are different from files in the following\naspects:\n\n1. The `tagName` is limited to 62 characters and the `content` to 512 bytes. Writing more to the command file, will be silently ignored.\n\n2. If a tag which does not exist in a directory is created by writing to it, it is called a *primary* tag.\n\n3. Tags are *inherited* from the parent directory by a newly created subdirectory. Changing a primary tag in one directory will change the tags inherited from it in the same way. Creating a new primary tag in a directory will not create an inherited tag in its subdirectories.\n\n    Moving a directory within the CHIMERA namespace will not change the inheritance. Therefore, a directory does not necessarily inherit tags from its parent directory. Removing an inherited tag does not have any effect.\n\n4. Empty tags are ignored.\n\n### Directory tags for dCache\n\nThe following directory tags appear in the dCache context:\n\n*OSMTemplate*:\n\nMust contain a line of the form `StoreName <storeName>` and specifies\nthe name of the store that is used by dCache to construct the [storage\nclass](#storage-class-and-directory-tags) if the [HSM\nType](rf-glossary.md#hsm-type) is `osm`.\n\n*HSMType*:\n\nThe [`HSMType`](rf-glossary.md#hsm-type) tag is normally determined\nfrom the other existing tags. E.g., if the tag `OSMTemplate` exists,\n`HSMType`=`osm` is assumed. With this tag it can be set explicitly. A\nclass implementing that HSM type has to exist. Currently the only\nimplementations are `osm` and `enstore`.\n\n*sGroup*:\n\nThe storage group is also used to construct the [storage\nclass](#storage-class-and-directory-tags) if the\n[`HSMType`](rf-glossary.md#hsm-type) is `osm`.\n\n*cacheClass*:\n\nThe cache class is only used to control on which pools the files in a\ndirectory may be stored, while the storage class (constructed from the\ntwo above tags) might also be used by the HSM. The cache class is only\nneeded if the above two tags are already fixed by HSM usage and more\nflexibility is needed.\n\n*hsmInstance*:\n\nIf not set, the `hsmInstance` tag will be the same as the `HSMType`\ntag. Setting this tag will only change the name as used in the\n[storage class](#storage-class-and-directory-tags) and in the pool\ncommands.\n\n*WriteToken*:\n\nAssign a `WriteToken` tag to a directory in order to be able to write\nto a space token without using the SRM.\n\n### Storage class and directory tags\n\nThe [storage class](config-PoolManager.md#storage-classes) is a string of the form `StoreName`:`StorageGroup`@`hsm-type`, where `StoreName`is given by the OSMTemplate tag, `StorageGroup` by the sGroup tag and `hsm-type` by the HSMType tag. As mentioned above the HSMType tag is assumed to be osm if the tag OSMTemplate exists.\n\nIn the examples above two tags have been created.\n\nExample:\n\n```console-root\nchimera lstag /data\n|Total: 2\n|OSMTemplate\n|sGroup\n```\n\nAs the tag OSMTemplate was created the tag HSMType is assumed to be\nosm.  The storage class of the files which are copied into the\ndirectory `/data` after the tags have been set will be\n`myStore:myGroup@osm`.\n\nIf directory tags are used to control the behaviour of dCache and/or a tertiary storage system, it is a good idea to plan the directory structure in advance, thereby considering the necessary tags and how they should be set up. Moving directories should be done with great care or even not at all. Inherited tags can only be created by creating a new directory.\n\nExample:\n\nAssume that data of two experiments, experiment-a and experiment-b is\nwritten into a namespace tree with subdirectories `/data/experiment-a`\nand `/data/experiment-b`. As some pools of the dCache are financed by\nexperiment-a and others by experiment-b they probably do not like it\nif they are also used by the other group. To avoid this the\ndirectories of experiment-a and experiment-b can be tagged.\n\n```console-root\nchimera writetag /data/experiment-a OSMTemplate \"StoreName exp-a\"\nchimera writetag /data/experiment-b OSMTemplate \"StoreName exp-b\"\n```\n\nData from experiment-a taken in 2010 shall be written into the\ndirectory `/data/experiment-a/2010` and data from experiment-a taken\nin 2011 shall be written into `/data/experiment-a/2011`. Data from\nexperiment-b shall be written into `/data/experiment-b`. Tag the\ndirectories correspondingly.\n\n```console-root\nchimera writetag /data/experiment-a/2010 sGroup \"run2010\"\nchimera writetag /data/experiment-a/2011 sGroup \"run2011\"\nchimera writetag /data/experiment-b sGroup \"alldata\"\n```\n\nList the content of the tags by\n\n```console-root\nchimera readtag /data/experiment-a/2010 OSMTemplate\n|StoreName exp-a\nchimera readtag /data/experiment-a/2010 sGroup\n|run2010\nchimera readtag /data/experiment-a/2011 OSMTemplate\n|StoreName exp-a\nchimera readtag /data/experiment-a/2011 sGroup\n|run2011\nchimera readtag /data/experiment-b/2011 OSMTemplate\n|StoreName exp-b\nchimera readtag /data/experiment-b/2011 sGroup\n|alldata\n```\n\nAs the tag OSMTemplate was created the HSMType is assumed to be osm.\nThe storage classes of the files which are copied into these directories after the tags have been set will be\n\n- `exp-a:run2010@osm` for the files in `/data/experiment-a/2010`\n- `exp-a:run2011@osm` for the files in `/data/experiment-a/2011`\n- `exp-b:alldata@osm` for the files in `/data/experiment-b`\n\nTo see how storage classes are used for pool selection have a look at\nthe example �Reserving Pools for Storage and Cache Classes� in the\nPoolManager chapter.\n\nThere are more tags used by dCache if the `HSMType` is `enstore`.\n\n<!--\n  [???]: #in-install-layout\n  [section\\_title]: #chimera-tags\n  [1]: #rf-ports\n  [2]: #cf-pm-psu\n  [tertiary storage system]: #cf-tss\n  [storage class]: #chimera-tags-storageClass\n  [3]: #secStorageClass\n--!>\n"], "reference": "Chimera is a crucial component of dCache that facilitates communication between the namespace and the database through a module called `PnfsManager`, which uses a thin Java layer. It allows for direct access to the file system view via `NFSv3` and `NFSv4.1` servers, enabling clients to `NFS`-mount the namespace locally. This setup permits the use of OS-level tools like `ls`, `mkdir`, and `mv` for managing files in Chimera. Additionally, Chimera supports various configurations through properties defined in `chimera.properties` and allows customization via layout and dcache configuration files. It also implements an attribute consistency policy to manage updates in highly concurrent environments, offering options for strong, weak, or soft consistency. Furthermore, Chimera enables the creation and management of directory tags, which can control file placement policies within dCache.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How does dCache ensure high availability for the cleaner service?", "reference_contexts": ["THE CLEANER SERVICE\n==================================\n\nThe cleaner service is a two-cell component consisting of `cleaner-disk` and `cleaner-hsm` that watches for files being deleted in the namespace. When files are deleted, the `cleaner-disk` cell will notify the pools that hold a copy of the deleted files' data and tell the pools to remove that data. Optionally, the `cleaner-hsm`, when present, can instruct HSM-attached pools to remove copies of the file's data stored on tape.\n\nThe cleaner components run periodically, so there may be a delay between a file being deleted in the namespace and the corresponding deletion of that file's data in pools and on tape.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Configuration\n\nThe cleaner services can be run in high availability (HA) mode (coordinated via [ZooKeeper](config-zookeeper.md)) by having several `cleaner-disk` and `cleaner-hsm` cells in a dCache instance, which need to share the same database. Only one such cleaner instance per type (disk or hsm) will be active at any point in time. If the currently active one (\"master\") is unavailable, another one is automatically taking its place.\n\n### Disk Cleaner\n\nThe `cleaner-disk` is responsible for removing disk-resident file replicas.\n\nThe property `cleaner-disk.limits.threads` controls the number of pools processed in parallel.\nThe `cleaner-disk.limits.batch-size` property places an upper limit on the number of files' data to be deleted in a message. If more than this number of files are to be deleted then the pool will receive multiple messages.\n\n### HSM Cleaner\n\nThe `cleaner-hsm` is responsible for removing tape-resident file replicas by instructing HSM-attached pools to remove deleted files' data stored in the HSM. To enable this feature, the property must be enabled at all the pools that are supposed to delete files from an HSM.\n\nThe `cleaner-hsm` component runs sequentially by sending `cleaner-hsm.limits.batch-size` delete targets in a message to an hsm-connected pool. It does so per hsm.\n\nRegularly, it fetches hsm locations of files to delete from the database and caches them for clustering by hsm. In order to not overload the memory of a `cleaner-hsm` cell, the number of hsm delete locations that are cached at any point in time should be limited. The `cleaner-hsm.limits.max-cached-locations = 12000` allows to set such a limit.\n\nWhen a pool reports back to `cleaner-hsm` that it was unable to delete certain files, `cleaner-hsm` removes these entries from the local cache. They will be re-loaded into the locations cache eventually so that they can be retried.\n\n## Database\n\nCleaner services use a database table called `t_locationinfo_trash`, often referred to as 'trash table'.\nThe field `ipnfsid` contains the pnfsid and `ilocation` the pool on which a replica is located.\nThe field `itype` stores the kind of delete entry:\n- `0`: `hsm`\n- `1`: `disk`\n- `2`: `inode`\n\nThe service `cleaner-disk` takes care of removing both `disk` locations from the database as well as `inode` ones once no other `itype` for a pnfsid exists.\n"], "reference": "dCache ensures high availability (HA) for the cleaner service by coordinating several `cleaner-disk` and `cleaner-hsm` cells via ZooKeeper. These cells must share the same database, and only one cleaner instance per type (disk or hsm) is active at any time. If the currently active instance, referred to as the 'master', becomes unavailable, another instance automatically takes its place.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the role of PnfsManager in the dCache system?", "reference_contexts": ["CHAPTER 16. dCache Frontend Service\n===================================\n\n-----\n[TOC bullet hierarchy]\n-----\n\nThe Frontend service is the dCache service (cell) responsible for\nserving data to clients via HTTP/REST.  The default port on which it\nruns is 3880.  The default protocol is https (TLS).  As usual, these,\nas well as other values for timeouts, enabling proxies, and anonymous\nuser access, can be configured; see\n`/usr/share/dcache/defaults/frontend.properties` for details.\n\nThe API delivered by the frontend is easily consulted once the service\nis running.   It is provided by a Swagger page found at\n\n    https://example.org:3880/api/v1\n\nAs can be seen there, these methods range over namespace access, allowing users\nto view files and directories, monitoring data for the dCache system, and\nevent subscription.   The Swagger documentation provides full descriptions of\nthe methods and their data types, and can be used to test the calls to the\nservice.  Each path also provides example `curl` commands, example responses\nand error code descriptions.\n\n## Configuring the Frontend service\n\nSome in-memory caching is done by this service in order to optimize the\ndelivery of monitoring/administrative data, but the memory requirements\nare not excessive (it is just text/JSON).  Add the service to an existing\ndomain or create a separate one for it:\n\n```ini\n[frontendDomain]\n[frontendDomain/frontend]\n```\n\nThe service can be run out-of-the-box without changing default property\nvalues.  There are a few properties affecting the admin/monitoring components\nwhich should, however, be noted.\n\n## Properties controlling monitoring data collection\n\nThe number of threads which are available to collect data from\nthe pools is set to 10:\n\n```ini\n# Used for processing updates on messages returned from pools\nfrontend.service.pool-info.update-threads=10\n```\n\nThis should usually be sufficient, but it is possible that for extremely\nlarge numbers of pools more threads may be necessary.  One could, alternatively,\nincrease the refresh interval for pool data collection.\n\nWhen RESTful calls are made for admin or monitoring information, some of them\ntranslate into a direct (blocking) call to a backend service to deliver\nthe data to the frontend, which then delivers it to the REST client.  These\n\"pass through\" calls usually involve queries concerning a specific file (pnfsid)\nto either the namespace or billing.   The remainder of the data, however,\nis served from cached data which the frontend has collected from backend\nservices.  How often this data is collected can be controlled by adjusting timeout\nproperties (for alarms, billing, cells, pools, transfers, restores, history)\nin the configuration files, or directly through the admin interface.\n\nAside from collecting data directly from the pools, the frontend also relies\non the history service for its histogram data.  Without that service, you\nwill not be able to request time-related statistics for billing, pool queues or\nfile lifetime.  The plots generated from this data by dCache-View will also\nnot be available.  Please refer to the section on\nthe [dCache History Service](config-history.md) for configuration\n(which is very simple).\n\n## Properties controlling monitoring data access\n\nThe following property should be noted.\n\n```ini\n#  ---- Determines whether operations exposing file information\n#       can be viewed by non-admin users.\n#\n#       When false (default), restores, queue operations on the pools,\n#       billing records, and transfers which are not owned by the user\n#       and are not anonymous can only be seen by admins.\n#\n#       Setting this value to true allows all users access\n#       to this information.\n#\n(one-of?true|false)frontend.authz.unlimited-operation-visibility=false\n```\n\nIf you wish all authenticated (non-anonymous) users to be be able to see\nthe full range of file-related information through either the RESTful api\nor in dCache-View, this property should be set to _true_.\n\n## Configuring and using the _admin_ role\n\nThe above property has to do with `HTTP GET`.  `HTTP PUT`, `POST`, `PATCH`\nor `DELETE`, however, are always limited to those who have the _admin role_.\nHence, this role must be defined for the dCache installation.  Please see\nthe documentation under [gPlazma](config-gplazma.md#roles) for how to set\nthis up.\n\nWhen issuing a ```curl``` command, one can indicate the role using a '#'\nafter the username; e.g.,\n\n```console\ncurl -k -u arossi#admin https://fndcatemp1.fnal.gov:3880/api/v1/restores\nEnter host password for user 'arossi#admin':\n```\n\nNote that currently, the assertion of the admin role requires a password.\nWe realize that this extra step is clunky and we are working\non allowing role assertion on the basis of the credential.\n\nFor the moment, however, you will need to add a .kpwd module to your\ngplazma setup and enable login and passwd entries for the user in question.\nExamples of how to do this may be found in the gPlazma section of this\ndocument; see, for instance,\n[Enabling Username/Password Access for WebDAV](#enabling-username-password-access-for-webdav).\n\nThe same procedure applies when enabling the admin role in dCache-View.\nAt the upper right hand corner of the dCache-View landing page,\nyou will see the user icon.  Click on it and select \"add another credential\"\nType in the user name and password, and check the box which says \"assert all roles\".\n\nSee the [dCache-View] documentation for further information.\n\n##### A Note on the RESTful API for tape restores\n\nThe data retrieved via the REST path\n\n```\n/api/v1/restores ...\n```\n\ncorresponds to the admin command\n\n```\n\\sp rc ls\n```\n\nfor all available pool managers.   This means that the restores listed in the\noutput are those initiated by an actual protocol through a door.  The restore\ninitiated by the pool command:\n\n\n```\n\\s <pool> rh restore <pnfsid>\n```\n\ndoes not show up in this list because the pool manager knows nothing about it.\n\nIn order to get all the restores (stages) on a given pool, the REST path\n\n```\n/api/v1/pools/{pool}/nearline/queues?type=stage\n```\n\nmust be used.\n\n##### A Note on the RESTful resource for QoS transitions\n\nAs of dCache 6.1, we have begun to integrate management of QoS with\nservices such as PnfsManager and Resilience.  The Frontend resource\nhas now been modified to send messages requesting ACCESS LATENCY\nand RETENTION POLICY changes to PnfsManager, which then broadcasts\nthe resulting change, to which Resilience will respond.\n\nThis is only a partial, first-step toward a full-blown QoS infrastructure,\nbut at least allows Resilience to manage such transitions for resilient\nfiles.  For other files, the current mechanism remains (QoS state maintained\nby the Frontend resource).\n\n##### RESTful QoS transitions as of 8.1\n\nThe restful commands now communicate with the [QoS Engine](config-qos-engine.md)\n"], "reference": "PnfsManager is involved in managing Quality of Service (QoS) transitions within the dCache system. It receives messages from the Frontend resource requesting changes to ACCESS LATENCY and RETENTION POLICY, and then broadcasts the resulting changes.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is anonymous FTP and how is it configured in dCache?", "reference_contexts": ["dCache as an FTP Server\n=======================\n\nThis chapter explains how to configure dCache to allow FTP, a common\nnetwork protocol that many clients support.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Introduction\n\nFTP is a long established protocol that allows clients to transfer\nfiles, and manage files and directories within dCache.  FTP was\noriginally specified without any encryption, with later standards\nadding support for encrypted communication.  FTP differs from many\nother protocols by using separate TCP connections for issuing commands\n(the control channel) and transferring file data (the data channel).\n\nVarious extensions to FTP exist to support additional functionality.\nThese are typically backwards compatible, allowing the ftp door to\nwork with clients that support the extension in addition to those that\ndon't.\n\n## The Control channel\n\nThe control channel is the TCP connection established by the client\nover which the client issues commands and receives replies indicating\nwhether those commands were successful.\n\nIn general, dCache supports four flavours of control channel: `plain`,\n`tls` (also known as FTPS), `gsiftp` (also known as GridFTP), and\n`kerberos`.  Each FTP door supports exactly one of these flavours.\nThese flavours differ in how the control channel is handled.  In plain\nFTP, the control channel is unencrypted; in many cases, this is\ninsecure and requires additional protection.  With tls, gsiftp and\nKerberos FTP, the control channel is encrypted, preventing\neavesdropping or interfering with requests.  Authentication with tls\nis based on username and password, gsiftp is based on X.509\ncredentials, while Kerberos FTP uses Kerberos.\n\nAlthough tls and gsi FTP doors are both X.509 based, they differ in\nhow the encryption is handled.  Support for tls FTP is more common and\nis often referred to as FTPS, FTP(E)S, FTPS-explicit or FTPES. Support\nfor gsi FTP is limited to grid tools.\n\n### Limiting access\n\nThe door may be configured to accept network connections only from\nspecific clients.  This is perhaps most useful with plain\n(unencrypted) FTP, but may be used with all flavours.  The\nconfiguration property `ftp.net.allowed-subnets` is a space-separated\nlist of either IP addresses of subnets (written in CIDR notation).\n\n### Directory listing\n\nA client may request a directory listing.  In the original FTP\nspecification, the format of a directory listing was unspecified.\nDifferent FTP servers could respond in different ways.  Returning the\noutput of 'ls -l' became a de facto standard, although different\nimplementations of the 'ls' command also different in their response.\n\nSome clients exist that attempt to parse the directory listing, using\nvarious heuristics to guess in which format the server is replying.\n\ndCache supports two formats: a legacy format and a format that more\nclosely emulates the output from 'ls -l'.  The `ftp.list-format`\nconfiguration property controls which format is returned.\n\nSubsequent extensions to FTP support directory listing in a precise,\nprescribed format.  This extension removes any ambiguity and allows\nclients to work with different servers.  dCache supports this\nextension.\n\n### High-availability\n\nIt is possible for dCache to run several ftp doors of the same type.\nThese doors could be accessed through a DNS alias or through an\nhaproxy server.\n\nIf a load-balancer is used then many support sending the client's IP\naddress when establishing the control channel using the ha-proxy\nprotocol.  If the door receives such messages then must be configured\nto process such message via the `ftp.enable.proxy-protocol`\nconfiguration property.  The door accepts both version 1 and version 2\nof the ha-proxy protocol.\n\n### Concurrent connections\n\nThe FTP door will limit the number of concurrent connections.  This is\ncontrolled by the `ftp.limits.clients` configuration property.\n\n### Anonymous access\n\nAnonymous access (also know as anonymous FTP or anonFTP) is a long\nestablished practice where publicly available data is made available\nto anyone who wants it.  The client authenticates with a specific\nusername (typically 'anonymous').  Although there is no specific\npassword for these accounts, it is common practice that the client\nsends the user's email address as the password, as a courtesy.\n\ndCache supports anonymous FTP for the plain and tls FTP doors.  This\nis disabled by default, but may be enabled using the\n`ftp.enable.anonymous-ftp` configuration property.  When enabled,\nusers may access dCache as user NOBODY; e.g., world-readable files may\nbe downloaded and world-readable directories may be listed.\n\ndCache may be configured to expose only part of the namespace by\nconfiguring an anonymous-ftp specific root directory.  This is\ncontrolled by the `ftp.anonymous-ftp.root` property.  When someone\nuses the anonymous FTP service then the root directory they see is\nwhichever path is configured in this property.\n\nThe name of the anonymous account is configured with the\n`ftp.authn.anonymous.user` property.  If a regular dCache user has the\nsame username as this property then that dCache user will no longer be\nable to log into their account via FTP username and password\nauthentication.  The default value is \"anonymous\", which is the widely\naccepted account name for anonymous access: many FTP clients will use\nthis name automatically.\n\nThe `ftp.authn.anonymous.require-email-password` configuration\nproperty controls whether to reject anonymous login attempts where the\npassword is not a valid email address.  Note that, Globus transfer\nservice currently sends \"dummy\" as the password, which is not a valid\nemail address.\n\nIf the plain or tls FTP door should be used only for anonymous access\nthen regular username and password access may be disabled by\nconfiguring the `ftp.enable.username-password` property.  This is\nperhaps most useful with plain FTP doors to prevent normal dCache\nusers from typing in their password unencrypted.\n\n\n## Data transfers\n\nTransferring a file with FTP involves establishing one (or more) TCP\nconnect over which the data will travel.  These are independent of the\ncontrol channel.\n\nThe TCP connection over which a file's data travels (the data\nchannels) are either established by dCache or by the client.\nTransfers where the data channel is establish by dCache are called\nactive transfers; those where the client establishes the data channel\nare called passive transfers.\n\n### Overwriting existing files\n\nThe FTP specification is clear that an upload that targets an existing\nfile should overwrite that file (provided the user is authorisation to\ndo so).  However, historically, dCache has preferred to fail such\ntransfers.\n\nCurrently, the `ftp.enable.overwrite` configuration property controls\nwhether or not dCache allows clients to overwrite existing files when\nuploading data.  By default, this is allowed.\n\n### Proxies\n\nThere are extensions to basic the FTP protocol (called GridFTP) that\nallow the ftp door to redirect the client to the pool.  Clients that\nsupport this GridFTP extension will advertise their support, allowing\ndCache to establish a transfer directly between the client and the\npool.\n\nIf the client does not support GridFTP, or is configured not to\nredirect, or if the ftp door is configured not to redirect the client\nthen the door will create a proxy for data transfers.  The data\nchannel(s) will be established between the door and the client and an\nadditional TCP connection is established between the door and the\npool.\n\nThere are two configuration options that control whether a proxy is\nused for transfers: ftp.proxy.on-passive and ftp.proxy.on-active.\nThese control whether a proxy is used for passive and active data\nchannels, respectively.  If set to 'true' then those transfers will\nalways use a proxy, irrespective of whether the client supports the\nGridFTP extension.\n\nA transfer involving a proxy requires more CPU and memory on the node\nhosting the ftp door.  It also increases the network traffic that the\ndoor sees, which may become a performance bottleneck.  Therefore, it\nis desirable not to proxy transfers, if possible.  If proxying is\nnecessary then this increased requirements may be managed by spread\nthe load over multiple ftp doors.\n\n### Transfer Modes\n\nData transfers may use one of two modes: MODE S and MODE E.\n\nIn MODE S, a single TCP connection is established to transfer a file's\ndata.  The TCP connection is closed once all the file's data is sent.\nThis is the standard way of delivering data and most commonly\nsupported.\n\nMODE E is part of the GridFTP extension.  It allows multiple TCP\nconnection to be used when transferring a file's data.  It also allows\nthose TCP connections to be reused when sending multiple files.\n\nThe configuration property `ftp.limits.streams-per-client` controls\nthe maximum number of TCP connections that dCache will establish for\nactive MODE E transfers.  For passive MODE E transfers, the client\nestablishes the data channels, so dCache cannot control the number of\nstreams.\n\nFor proxied transfers involving MODE E, dCache will try to keep any\nestablished data channels open.  This is an optimisation especially\nfor smaller files, where the time needed to establish a data channel\nis a significant compared to the time needed to transfer the file's\ndata.\n\n### Logging aborted transfers\n\nAn aborted transfer is one where the client requested a new file be\nuploaded or all data from an existing file be downloaded, but only\npart of the file's contents were transferred.  Such aborted transfers\nnormally indicate a problem.  As the problem may be transitory, it may\nnot be clear what caused it and therefore it may be hard to recreate\nthe problem.\n\nTo aid in fixing aborted transfers, the door can provide considerable\ninformation about a transfer at the point the transfer is aborted.\nThis is currently disabled by default, but may be controlled with the\n`ftp.enable.log-aborted-transfers` configuration property.\n\n\n## Configuration examples\n\nHosting an FTP server in a domain is as simple as:\n\n\n```ini\n[<domainName>/ftp]\n```\n\nThe 'ftp.authn.protocol' configuration property controls which flavour\nof FTP server should be started; for example, to start a gsiftp server:\n\n\n```ini\n[<domainName>/nfs]\nftp.authn.protocol = gsi\n```\n\nThere are distinct default TCP ports on which the different flavours\nof FTP server will listen, so a single host may run multiple FTP doors\nwithout requiring any port configuration:\n\n\n```ini\n[<domainName>/nfs]\nftp.authn.protocol = plain\n[<domainName>/nfs]\nftp.authn.protocol = gsi\n[<domainName>/nfs]\nftp.authn.protocol = kerberos\n[<domainName>/nfs]\nftp.authn.protocol = tls\n```\n"], "reference": "Anonymous FTP, also known as anonFTP, is a practice where publicly available data is accessible to anyone. In dCache, clients authenticate using a specific username, typically 'anonymous', and while there is no specific password, it is common for clients to send their email address as the password. dCache supports anonymous FTP for both plain and tls FTP doors, but it is disabled by default. To enable it, the configuration property 'ftp.enable.anonymous-ftp' must be set. When enabled, users can access dCache as user NOBODY, allowing them to download world-readable files and list world-readable directories. The root directory for anonymous FTP can be configured using 'ftp.anonymous-ftp.root', and the username for anonymous access is set with 'ftp.authn.anonymous.user', which defaults to 'anonymous'.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What happened to gPlazma1 in dCache?", "reference_contexts": ["Chapter 9. Authorization in dCache\n===================================\n\nTo limit access to data, dCache comes with an authentication and authorization interface called `gPlazma2`. gPlazma is an acronym for Grid-aware PLuggable AuthorZation Management. Earlier versions of dCache worked with `gPlazma1` which has now been completely removed from dCache. So if you are upgrading, you have to reconfigure `gPlazma` if you used `gPlazma1` until now.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Basics\n\nThough it is possible to allow anonymous access to dCache it is usually desirable to authenticate users. The user then has to connect to one of the different doors (e.g., `GridFTP door, dCap door`) and login with credentials that prove his identity. In Grid-World these credentials are very often `X.509` certificates, but dCache also supports other methods like username/password and kerberos authentication.\n\nThe door collects the credential information from the user and sends a login request to the configured authorization service (i.e., `gPlazma`) Within `gPlazma` the configured plug-ins try to verify the users identity and determine his access rights. From this a response is created that is then sent back to the door and added to the entity representing the user in dCache. This entity is called `subject`. While for authentication usually more global services (e.g., ARGUS) may be used, the mapping to site specific UIDs has to be configured on a per site basis.\n\n## Configuration\n\n`gPlazma2` is configured by the PAM-style configuration file\n`/etc/dcache/gplazma.conf`. Each line of the file is either a comment\n(i.e., starts with #, is empty, or defines a plugin. Plugin defining\nlines start with the plugin stack type (one of `auth, map, account,\nsession identity`), followed by a PAM-style modifier (one of\n`optional, sufficient, required, requisite`), the plugin name and an\noptional list of key-value pairs of parameters. During the login\nprocess they will be executed in the order `auth, map, account` and\n`session`. The `identity` plugins are not used during login, but later\non to map from UID+GID back to user names (e.g., for NFS). Within\nthese groups they are used in the order they are specified.\n\n    auth|map|account|session|identity optional|required|requisite|sufficient plug-in [\"key=value\" ...]\n\nA complete configuration file will look something like this:\n\nExample:\n\n    # Some comment\n    auth    optional  x509\n    auth    optional  voms\n    map     requisite vorolemap\n    map     requisite authzdb authzdb=/etc/grid-security/authzdb\n    session requisite authzdb\n\n**auth**\n`auth`-plug-ins are used to read the users public and private credentials and ask some authority, if those are valid for accessing the system.\n\n**map**\n`map`-plug-ins map the user information obtained in the `auth` step to UID and GIDs. This may also be done in several steps (e.g., the `vorolemap` plug-in maps the users DN+FQAN to a username which is then mapped to UID/GIDs by the `authzdb` plug-in.\n\n**account**\n`account`-plug-ins verify the validity of a possibly mapped identity of the user and may reject the login depending on information gathered within the map step.\n\n**session**\n`session` plug-ins usually enrich the session with additional attributes like the user�s home directory.\n\n**identity**\n`identity` plug-ins are responsible for mapping UID and GID to user names and vice versa during the work with dCache.\n\nThe meaning of the modifiers follow the PAM specification:\n\nModifiers\n\n**optional**\nThe success or failure of this plug-in is only important if it is the only plug-in in the stack associated with this type.\n\n**sufficient**\nSuccess of such a plug-in is enough to satisfy the authentication requirements of the stack of plug-ins (if a prior required plug-in has failed the success of this one is ignored). A failure of this plug-in is not deemed as fatal for the login attempt. If the plug-in succeeds `gPlazma2` immediately proceeds with the next plug-in type or returns control to the door if this was the last stack.\n\n**required**\nFailure of such a plug-in will ultimately lead to `gPlazma2` returning failure but only after the remaining plug-ins for this type have been invoked.\n\n**requisite**\nLike `required`, however, in the case that such a plug-in returns a failure, control is directly returned to the door.\n\n### Plug-ins\n\n`gPlazma2` functionality is configured by combining different types of plug-ins to work together in a way that matches your requirements. For this purpose there are five different types of plug-ins. These types correspond to the keywords `auth, map, account, session` and `identity` as described in the previous section. The plug-ins can be configured via properties that may be set in `dcache.conf`, the layout-file or in `gplazma.conf`.\n\n#### auth Plug-ins\n\n##### kpwd\n\nThe `kpwd` plug-in authorizes users by username and password, by pairs of DN and FQAN and by `Kerberos` principals.\n\n\nProperties\n\n**gplazma.kpwd.file**\n\nPath to   **dcache.kpwd**\nDefault:  `/etc/dcache/dcache.kpwd`\n\n\n##### voms\n\nThe `voms` plug-in is an `auth` plug-in. It can be used to verify `X.509` credentials. It takes the certificates and checks their validity by testing them against the trusted CAs. The verified certificates are then stored and passed on to the other plug-ins in the stack.\n\n\n\nProperties\n\n**gplazma.vomsdir.ca**\n\n   Path to ca certificates\n   Default: `/etc/grid-security/certificates`\n\n\n\n**gplazma.vomsdir.dir**\n\n  Path to **vomsdir**\n  Default: `/etc/grid-security/vomsdir`\n\n##### X.509 plug-in\n\nThe X.509 is a auth plug-in that extracts X.509 certificate chains from the credentials of a user to be used by other plug-ins.\n\n##### jaas\n\nThe `jaas` uses _Java Authentication and Authorization Service_ and implements username and password based authentication against JAAS configured service. The typical usesage of `jaas` plugin is authentication with kerberos5. Though it's possible to use `jaas` plugin for ldap authentication it's recommended to use `ldap` plugin directly.\n\nTo configure `jaas` plugin an extra configuration file defined by **dcache.authn.jaas.config** property is required. For kerberos5 configuration:\n\n```\nKrb5Gplazma {\n   com.sun.security.auth.module.Krb5LoginModule required debug=false useTicketCache=false;\n};\n```\n\nwhere _Krb5Gplazma_ is the section name to be referenced by **gplazma.jaas.name** property. For other JAAS -based configurations check desired login module documentation. A single configuration file you may have multiple sections if required.\n\nProperties:\n\n**dcache.authn.jaas.config**\n\n  Path to jass config file\n  Default: `/etc/dcache/jgss.conf`\n\n**gplazma.jaas.name**\n\n  jass configuration section name\n\nTo complete authentication `jaas` plugin should be combined with `krb5` or `mutator` mapping plugin.\n\nExample:\n\n    # krb5 with ldap configuration\n    auth    optional        jaas gplazma.jaas.name=Krb5Gplazma\n    map     optional        krb5\n    map     sufficient      ldap\n    identity requisite      ldap\n    session optional        ldap\n\n#### oidc\n\nDelegated authentication is a relatively common paradigm.  This is\nwhere a service (such as a website) allows users to authenticate via\nsome other service (e.g., \"login with Google\" or \"login with GitHub\").\nWhen the user wants to log in, they are redirected from the website to\nthe authenticating service (Google, GitHub, etc).  If the user is not\nalready logged in (to the authenticating service) then they are\nprompted to do so; typically by entering their username and password.\nThe user is then redirected back to the original website.  In doing\nthis, the service learns that the user correctly authenticated with\nthe authenticating service (Google, GitHub, ...).  The authenticating\nservice will provide a unique identifier for that user and possibly\nother information about the user; e.g., a name or an email address.\n\nThere are a few interesting points here.  First, the website does not\nhave to worry about how the user authenticates (e.g., they don't need\nto implement a password reset mechanism for users who have forgotten\ntheir password), although the authentication service may provide\ndetails on how the user authenticated.  Second, the website must\nstrongly trust the authenticating service: a compromised\nauthentications service could strongly impact traceability for the\nwebsite).\n\nOpenID-Connect (OIDC) is a standard mechanism to support delegate\nauthentication.  OIDC itself is based on another standard called\nOAuth2.  OAuth2 is a way to support delegated authorisation: a way to\ngrant limited access without having to share username+password.  In\nOAuth2, the OAuth2 Provider (OP) issues an \"access token\" that allows\nanyone with that token to access a protected URL: one with which\nnon-authenticated users cannot interact.\n\nIn OIDC, the OP issues an access token that allows access to the\nuserinfo endpoint.  Querying the userinfo endpoint with the access\ntoken provides information about the user that authenticated,\nexpressed as a set of claims.  A claim is a key-value pair, that\nprovides some information about the user; for example, a claim might\nhave \"email\" as the key and \"user@example.org\" as the value.\n\nThe `oidc` gPlazma plugin is an authentication phase plugin that\nallows dCache to support OIDC-based authentication.  It accepts an\naccess token (as supplied to dCache via a WebDAV or xrootd door) and\n(at a high-level) does three things:\n\n 1. Verify that the token is valid.\n 2. Obtain OIDC information about the user: a set of claims.\n 3. Convert OIDC claims into corresponding dCache information.\n\n##### Verifying the token\n\nOne way to check the validity of the access token is to send the token\nto the OP's userinfo endpoint.  The OP will return an error if the\ntoken is invalid; for example, if the token not yet valid (embargoed)\nor has expired.\n\nAlthough this works, it has can result in a large number of HTTP\nrequests (at least one for each new token), which can stress the OP.\nIt introduces latency (as dCache must wait for the response) and it\nalso requires that the OP is highly available (any down-time would\nresult in dCache not accepting tokens).\n\nIn OIDC and OAuth2, the access token is opaque: there is no way to\nobtain information about the user (or, more generally, about the\ntoken) by simply looking at the token.  Under this model, the dCache\nmust query the userinfo endpoint.\n\nHowever, many OPs issue access tokens that conform to the JSON Web\nToken (JWT) standard.  JWTs are tokens that are cryptographically\nsigned.  If dCache knows the OP's public keys then it can verify the\ntoken without sending any HTTP requests.  This is called \"offline\nverification\".\n\nBy default, dCache will examine the token.  If it is a JWT then dCache\nwill use offline verification; otherwise, the token is sent to the\nuserinfo endpoint.  dCache will cache the response.  This behaviour\nmay be adjusted.\n\nPlease note that the OIDC plugin uses Java's built-in trust store\nto verify the certificate presented by the issuer when making\nTLS-encrypted HTTP requests (https://...).  Most issuers use\ncertificates issued by a CA/B-accredited certificate authority, and\nmost distributions of Java provide CA/B as a default list of\ntrusted certificate authorities.\n\n##### Obtaining OIDC information\n\nThe access token represents a logged in user; however, dCache needs to\nknow information about that user: the set of claims.  As with\nverifying the token, OIDC information may be obtained by querying the\nuserinfo-endpoint.\n\nThis suffers from the same problems as describe in verifying the\ntoken, with the same potential solution: JWTs.\n\nThere is an additional problem: the access token is often included in\nlocations that places a limit on its maximum size; for example, when\nused to authorise an HTTP request.  For opaque access tokens (which is\nusually just a randomly chosen number), this is not a problem;\nhowever, a JWT could become too large if it tries to include too much\ninformation.  To counter this problem, some OPs issue JWTs that\ninclude only a subset of the information available from the userinfo\nendpoint.\n\nBy default, dCache will look to see if the token is a JWT.  If it is,\ninformation is extracted from the JWT. If not then the token is sent\nto the userinfo endpoint to learn about the user.\n\nAs with verifying the token, this behaviour may be modified.\n\n##### Converting OIDC information\n\nThe OIDC information about a user is expressed as a set of claims.\nThese claims are not directly aligned with how dCache represents\ninformation about users.  Therefore, the claims must be converted into\na form with which other gPlazma modules can work.\n\nMoreover, there is some variation in what claims may be present, with\ndifferent collaborations choosing different sets of claims they will\nuse.\n\nRather than attempting to support all such behaviour, the oidc plugin\nwill convert OIDC claims to corresponding gPlazma principals according\nto a set of rules, called a profile.  The oidc plugin supports\nmultiple profiles.  Each configured OP uses exactly one profile when\nprocessing all tokens issued by that OP.  If no profile is specified\nthen a default profile is used.\n\nAnother feature that some OPs offer is direct- or explicit\nauthorisation statements.  This is where the token includes\ninformation regarding which operations are supported.  Other\noperations are forbidden simply by excluding them from the token.\n\ndCache will honour such explicit AuthZ statements, and switch off\nother permission checks (e.g., within the namespace permission).  In\nother words, if the token says the bearer is entitled to do\n\"something\" then dCache will allow such operations.\n\n##### General configuration\n\nConfiguration for the oidc plugin is via gPlazma configuration\nproperties that start `gplazma.oidc`.\n\nThe plugin will log a warning if an OP is taking too long to reply.\nThis is intended to help diagnose problems with poor login latency.\nThe `gplazma.oidc.http.slow-threshold` and\n`gplazma.oidc.http.slow-threshold.unit` configuration properties\ncontrol this behaviour.\n\nInformation about an OP is discovered through a standard mechanism\ncalled the discovery endpoint.  This information rarely changes, so\nmay be cached.  The `gplazma.oidc.discovery-cache` and\n`gplazma.oidc.discovery-cache.unit` configuration properties control\nfor how long the information is cached.\n\nIn order to reduce latency, the plugin will process tokens that\nrequire contacting the OP concurrently.  There is a maximum number of\nthreads available, as controlled by the\n`gplazma.oidc.concurrent-requests` configuration property.\n\nThe HTTP requests are also limited: to avoid overloading networks or\nOPs.  There is an overall maximum number of HTTP requests that is\ncontrolled by the `gplazma.oidc.http.total-concurrent-requests`\nconfiguration property, and a maximum number of concurrent HTTP\nrequests that an OP will see that is controlled by the\n`gplazma.oidc.http.per-route-concurrent-requests` configuration\nproperty.\n\nThe plugin will not wait indefinitely for the OP's response; rather,\nit will give up if the OP doesn't respond within some timeout period.\nThe duration of this timeout is controlled by the\n`gplazma.oidc.http.timeout` and `gplazma.oidc.http.timeout.unit`\nconfiguration properties.\n\nThe result of looking up an access token via the userinfo endpoint is\ncached.  The behaviour of that cache may be tuned.  The\n`gplazma.oidc.access-token-cache.size` limits the number of tokens\nstored in the cache.\n\nThe `gplazma.oidc.access-token-cache.refresh` and\n`gplazma.oidc.access-token-cache.refresh.unit` configuration\nproperties control when the plugin considers the userinfo information\nfor a token \"stale\".  After this time, a background activity is\ntriggered to fetch more up-to-date information about the token if the\ntoken is used.  This refreshing activity does not block a login:\nlogins will continue with the old information until the background\nactivity completes, with subsequent logins using the more up-to-date\ninformation.\n\nThe `gplazma.oidc.access-token-cache.expire` and\n`gplazma.oidc.access-token-cache.expire.unit` configuration properties\ncontrol when the plugin considers the userinfo information for a token\nas \"stale\".  Unlike the refresh behaviour, the expire behaviour forces\na login attempt to block until fresh information is available.\n\nOne of the security features with bearer tokens is to limit the damage\nif a token is \"stolen\" (if the token is acquired by some unauthorised\nsoftware or person).  To reduce the impact of this, it is possible to\nidentify that a token should only be used by specific services, as\nidentified by the `aud` (audience) claim.  If there is an `aud` claim\nvalue then any service that does not identify itself with the claim\nvalue must reject the token.\n\nIn dCache, the `gplazma.oidc.audience-targets` configuration property\ncontrols the list of audience claims that dCache will accept.  If the\ntoken has an `aud` claim and that value does not match any of the\nitems in `gplazma.oidc.audience-targets` then dCache will reject the\ntoken.\n\n##### Configuring the OPs.\n\nPerhaps the most central configuration is the list of OPs that the\nplugin will trust.  This is controlled by the `gplazma.oidc.provider`\nconfiguration property, which is a map property.\n\nThe map's key is an alias for the OP.  This can be any value, but\noften it is a name that is shorter and more memorable that the OP's\nURL.\n\nThe map's value is the URL of the OP.  This is the URL, as it appears\nin the `iss` (issuer) claim.  The plugin will use this URL to\nconstruct the discovery endpoint (following a standard procedure) and\nwill query that endpoint to learn more about the OP.\n\nThe following provides a minimal example of configuring an OP:\n\n    gplazma.oidc.provider!EXAMPLE = https://op.example.org/\n\nIn this example, an OP is configured with the alias `EXAMPLE`.  The\nOP's issuer URL is `https://op.example.org/`.\n\n> NOTE: The Issuer Identifier for the OpenID Provider **MUST** exactly match the value of the iss (issuer) Claim.\n\nThe OP configuration allows for some configuration of dCache's\nbehaviour when accepting tokens from that OP.  This configuration are\nkey-value pairs, with a dash (`-`) before the key and an equals (`=`)\nbetween the key-value pair.  If the value contains spaces then the\nvalue may be placed in double quotes (e.g., `-foo=\"bar baz\"`).\n\nIn the following example, the OP is supported with the alias EXAMPLE\nand with the behaviour configured by some control parameter `foo`,\ngiven `bar` as an argument.\n\n    gplazma.oidc.provider!EXAMPLE = https://op.example.org/ -foo=bar\n\nThe following control parameters are available:\n\n  * `-profile` describes with which profile the OP's tokens are\n    processed.  Valid values are `oidc`, `scitokens` and `wlcg`.  If\n    no `-profile` is specified then `oidc` is used.  The parameter\n    must not be repeated.\n\n  * `-suppress` is used to disable certain behaviour.  The value is a\n    comma separated list of keywords.  The `-suppress` argument may be\n    repeated, which is equivalent to a single `-suppress` with the all\n    the supplied keywords as the value; for example, `-suppress=A\n    -suppress=B` is equivalent to `-suppress=A,B`.\n\n    The following suppress keywords are supported:\n\n      * `audience` suppresses the audience claim check.  This is\n        provided as a work-around to allow dCache to work with badly\n        configured clients.  If a token contains an `aud` claim that\n        is not listed in `gplazma.oidc.audience-targets` and audience\n        suppression is enabled then the plugin will accept the token\n        and write a log entry.\n\n      * `offline` suppresses offline verification.  This is provided\n        as a work-around for OPs that issue JWTs with insufficient\n        information: the required information is only available by\n        querying the userinfo endpoint.\n\n  * `-prefix` is use by the scitokens and wlcg profiles to map\n    authorisation paths to dCache paths.  More details are available\n    under those profile descriptions.\n\n  * `-authz-id` and `-non-authz-id` is used by the wlcg profile to\n    identify authorisation-bearing and non-authorisation-bearing\n    tokens.  More details are available under the wlcg profile\n    description.\n\n##### Available profiles\n\nThis section describes the different profiles available when\nconfiguring an OP.  The default profile is `oidc`.\n\n###### Common profile behaviour.\n\nCertain behaviour is common to all profiles.  This is described here.\n\nThe `sub` (\"subject\") claim, which assigns a persistent and unique\n(within the OP) identifier for the user.  The claim value is mapped to\na value that includes the OP's identity; for example a `sub` claim of\n`user_1` in a token issue by an OP with alias `MY_OP` would have the\nvalue `user_1@MY_OP`.  This sub-based principal is generally ignored\nby dCache; to be useful, it needs to be mapped to some other\nprincipal; for example, the multimap line:\n\n    oidc:user_1@MY_OP username:paul\n\nconverts the sub claim `user_1` from a token issued by `MY_OP` to the\n(dCache) user with username `paul`.\n\nThe `jti` (\"JWT identifier\") claim, which assigns a unique identifier\nfor the token.  The value is used to mitigate reply attacked (if that\nfeature is enabled) and is mapped to the `JwtJtiPrincipal` and\navailable through some logged entries, but is otherwise ignored by\ndCache.\n\n###### The `oidc` profile\n\nThis profile adds support for the name, email address, groups,\nwlcg-groups, LoA, entitlement and username.\n\nA person's name may be represented in the token using different\nclaims: `given_name` `family_name` and `name`.  If the `name` claim is\npresent, it is used; otherwise if both the `given_name` and\n`family_name` are present then these are concatenated.  The result is\nconverted to a FullNamePrincipal.\n\nThe `preferred_username` claim is usually ignored.  However, if\n`-accept` argument contains the item `username` then the\n`preferred_username` is included as the (dCache) username principal.\nThis implies a strong trust relationship between the OP and dCache,\nwhich most likely exists when the dCache instance and the OP are run\nby the same organisation.\n\nThe `email` claim, if present, is converted to a corresponding\nEmailAddressPrincipal.\n\nThe `groups` claim is non-standard, but provided by INDIGO IAM.  The\nclaim value must be a JSON array of JSON Strings, with any initial `/`\nremoved.  Each group is converted to a GroupNamePrincipal.\n\nThe `wlcg.groups` claim is defined in the WLCG profile.  This provides\nbroadly similar information to the `groups` claim.  Each such group is\nconverted to OpenIdGroupPrincipal.  These principals have no affect on\ndCache: another plugin (e.g., multimap) may be used to associate an\nOIDC-group principal to a group principal.\n\nThe `eduperson_assurance` claim contains information on the Level of\nAssurance of the person.  These terms identify how confident is the OP\nof the user's identity.  The plugin supports the standard REFEDs terms\n(`IAP/medium`, `ATP/ePA-1m`, etc) and LoA profiles (e.g.,\n`cappuccino`, `espresso`), the IGTF policies (`aspen`, `birch`, etc),\nthe AARC policy (`assam`) and the EGI policies (`Low`, `Substantial`,\n`High`).  Each LoA term is converted to an equivalent LoAPrincipal.\n\nThe `eduperson_entitlement` claim is either a single JSON String or an\narray of JSON Strings.  In either case, each claim value is converted\nto an EntitlementPrincipal, provided the value is a valid URI.\n\n######  The `scitokens` profile\n\nIn addition to the common profile behaviour, the `scitokens` profile\nexamines the `scope` claim to look for authorisation statements.  This\nindicates what the user is allowed to do.\n\nThe oidc plugin will fail if dCache has been configured to to support\nan OP with the `scitokens` profile and a token from that OP has no\nauthorisation statements.\n\nIn the OP definition:\n\n  * The `-prefix` argument is required.  It is an absolute path within\n    dCache's namespace under which authorisation paths are resolved.\n    For example, if an OP is configured with `-prefix=/wlcg/CMS` and\n    the token allows reading under the `/users/paul` path\n    (`storage.read:/home/paul`) then the token is allowed to read\n    content under the `/wlcg/CMS/home/paul` path.\n\n###### The `wlcg` profile\n\nIn addition to the common profile behaviour, the `wlcg` profile\nadds support for the WLCG JWT Profile document.\n\nThe `wlcg.ver` claim must be present and must contain the value `1.0`.\nIf this is not true then the plugin fails.\n\nThe `wlcg.groups` claim is accepted only if the value is a JSON array\nof JSON strings.  Each group is mapped to an OpenIdGroupPrincipal.\nThis is ignored by dCache, so must be mapped to some\nGroupNamePrincipal by some other gPlazma plugin (e.g., the multimap plugin).\n\nIn addition, the `scope` claim is examined to see if it contains\nexplicit authorisation statements.  If the token contains any such\nexplicit authorisation statements then the token is considered an\nauthorisation token; if the token contains no such authorisation\nstatements then it is considered a non-authorisation token.\n\nIn the OP definition:\n\n  * The `-prefix` argument is required.  It is an absolute path within\n    dCache's namespace under which authorisation paths are resolved.\n    For example, if an OP is configured with `-prefix=/wlcg/ATLAS` and\n    the token allows reading under the `/users/paul` path\n    (`storage.read:/users/paul`) then the token is allowed to read\n    content under the `/wlcg/ATLAS/users/paul` path.\n\n  * the `-authz-id` argument contains a space-separated list of\n    principals to add if the token is an authorisation tokens.  This\n    allows for custom behaviour if the token is bypassing dCache's\n    authorisation model.\n\n  * The `-non-authz-id` argument contains a space-separated list of\n    principals to add if the token is a non-authorised token.  This\n    allows for custom behaviour if the token is adhering to dCache's\n    authorisation model.\n\n\n#### map Plug-ins\n\n##### alise\n\n[ALISE](https://github.com/m-team-kit/alise/) is a service developed through\nthe interTwin project.  When deployed and configured, it allows a site's users\nto register their federated identities (e.g. EGI Check-In, Helmholtz ID, some\ncommunity-managed INDICO-IAM service) against their site-local identity.  This\nregistration process requires no admin intervention and a user typically does\nthis once.   Once this link (between a user's federated and site-local\nidentities) is registered, ALISE allows a service (such as dCache) to discover\nthe local identity (a username) when that service presents that user's\nfederated identity (an OIDC `sub` claim).\n\nALISE is intended for sites that have identity management (IAM) solutions that\ndo not support federated identities.  Other solutions may be preferable for\nsites that have IAM solutions that support account linking; e.g., sites running\nKeycloak may be able to provide the same functionality without running an\nadditional service.\n\nThe `alise` plugin processes a login request by taking the `sub` claim (e.g.,\nas provided by the `oidc` plugin) and sending a request to the ALISE service.\nIf that request is successful then the plugin will learn the user's username\nand (optionally) that user's display name.  The `alise` plugin will cache\nresult of the ALISE query for a short period.  This is to improve latency (of\nsubsequent queries) and to avoid placing too much load on the ALISE server.\n\nWhen processing a login request, the `alise` plugin succeeds if it queries the\nALISE service with a `sub` claim and receives a corresponding local identity: a\nusername.  The plugin fails if the ALISE service responds that no mapping is\nknown for this federated identity, if there is a problem making the request, or\nif the login attempt does not contain a `sub` claim (either no access token was\nprovided or the `sub` claim was not extracted from the access token by the\n`oidc` plugin).\n\n**Configuration properties**\n\n`gplazma.alise.endpoint`\n\nThis is a URL that forms the base for all HTTP queries to the ALISE service.\nThe default value is not valid; you must supply this configuration.\n\nA typical value would look like `https://alise.example.org/`.\n\nFor comparison, a typical HTTP request to ALISE would look like:\n\n    https://alise.example.org/api/v1/target/vega-kc/mapping/issuer/95d[...]\n\nThe `gplazma.alise.endpoint` value is this URL update (but not including) the\n`/api/v1` part.\n\n`gplazma.alise.target`\n\nA specific ALISE endpoint may serve multiple families of related services:\nthe targets.  Targets are independent of each other: the account mapping\ninformation of a target is independent of the account information any other\ntarget.\n\nThe primary use-case for targets is to allow a single ALISE service to support\nmultiple sites; however, the concept could also be useful if a ALISE service\nsupports only a single site.\n\nThe default value is not valid; you must supply this configuration.  A typical\nvalue would be a simple string; e.g., `vega-kc`.\n\n`gplazma.alise.apikey`\n\nThe API key is the authorisation that allows dCache to query ALISE for account\ninformation.  The [ALISE documentation](https://github.com/m-team-kit/alise/)\nprovides information on how to obtain the API key.\n\nThe following provides a quick summary of the process, using oidc-agent and\nother common command-line tools.  The\n\n    SOME_OP=EGI-CHECKIN # or whichever oidc-agent account is appropriate\n    TOKEN=$(oidc-token $SOME_OP)\n    TARGET=vega-kc # see gplazma.alise.target\n    APIKEY_ENDPOINT=https://alise.example.org/api/v1/target/$TARGET/get_apikey\n    APIKEY=$(curl -sH \"Authorization: Bearer $TOKEN\" $APIKEY_ENDPOINT \\\n        | jq -r .apikey)\n\n`gplazma.alise.timeout`\n\nThe time dCache will wait for the ALISE service to respond when requesting a\nuser's site-local identity.  If there is no response within that time then the\nalise plugin will fail the request.  This, in turn, will (depending on gPlazma\nconfiguration) likely result in gPlazma failing that login attempt.\n\nThe value is expressed as an ISO 8601 duration; for example, `PT5M` is five\nminutes and `PT10S` is ten seconds.\n\n`gplazma.alise.issuers`\n\nThe alise plugin can limit the federated identities that it will send to the\nALISE service, based on the issuer of the access token.  The\n`gplazma.alise.issuers` configuration property contains a space-separated list\nof issuers, where each issuer is specified as either the dCache alias (see\n`oidc` plugin) or the issuer's URI.  As a special case, if this list is empty\nthen all tokens are sent to the ALISE service for mapping.\n\n**Using with other plugins**\n\nWhen successful, the `alise` plugin provides information about the user; in\nparticular, the user's username and (optionally) a display name.  By itself,\nthis is insufficient for a successful gPlazma map phase, as the user's uid and\ngid must also be obtained.\n\nOut of the box, dCache supports multiple ways of obtaining the uid and gid from\na username. This could be done by querying an LDAP service (see `ldap` plugin),\nan NIS service (see `nis` plugin) or the dCache server's local user account\nlookup service (see `nsswitch` plugin).  It is also possible to use explicit\nconfiguration files (see `multimap` plugin).\n\n##### kpwd\n\nAs a `map` plug-in it maps usernames to UID and GID. And as a `session` plug-in it adds root and home path information to the session based on the user�s username.\n\nProperties\n\n  **gplazma.kpwd.file**\n\n     Path to **dcache.kpwd**\n     Default: `/etc/dcache/dcache.kpwd`\n\n\n\n##### authzdb\n\nThe GP2-AUTHZDB takes a username and maps it to UID+GID using the `storage-authzdb` file.\n\n\n\n**gplazma.authzdb.file**\n\n   Path to **storage-authzdb**\n   Default: `/etc/grid-security/storage-authzdb`\n\n\n\n##### GridMap\n\nThe `authzdb` plug-in takes a username and maps it to UID+GID using the **storage-authzdb** file.\n\n\n\nProperties\n\n**gplazma.gridmap.file**\n\n   Path to `grid-mapfile`\n   Default: `/etc/grid-security/grid-mapfile`\n\n\n\n##### vorolemap\n\nThe `voms` plug-in maps pairs of DN and FQAN to usernames via a [vorolemap](config-gplazma.md#preparing-grid-vorolemap) file.\n\n\n\nProperties\n\n**gplazma.vorolemap.file**\n\n   Path to **grid-vorolemap**\n   Default: `/etc/grid-security/grid-vorolemap`\n\n\n\n##### krb5\n\n The `krb5` plug-in maps a kerberos principal to a username by removing the domain part from the principal.\n\nExample: the Kerberos principal `user@KRB-DOMAIN.EXAMPLE.ORG` is\nmapped to the user `user`.\n\n\n##### nsswitch\n\nThe `nsswitch` plug-in uses the system�s `nsswitch` configuration to provide mapping.\n\nTypically `nsswitch` plug-in will be combined with `vorolemap` plug-in, `gridmap` plug-in or `krb5` plug-in:\n\nExample:\n\n    # Map grid users to local accounts\n    auth    optional  x509 #1\n    auth    optional  voms #2\n    map     requisite vorolemap #3\n    map     requisite nsswitch #4\n    session requisite nsswitch #5\n\nIn this example following is happening: extract user's DN (1), extract and verify VOMS attributes (2), map DN+Role to a local account (3), extract uid and gids for a local account (4) and, finally, extract users home directory (5).\n\n##### nis\n\nThe `nis` uses an existing `NIS` service to map username+password to a username.\n\nProperties\n\n**gplazma.nis.server**\n\n `NIS` server host\n Default: `nisserv.domain.com`\n\n\n**gplazma.nis.domain**\n\n`NIS` domain\nDefault: `domain.com`\n\nThe result of `nis` can be used by other plug-ins:\n\nExample:\n\n    # Map grid or kerberos users to local accounts\n    auth    optional  x509 #1\n    auth    optional  voms #2\n    map     requisite vorolemap #3\n    map     optional  krb5 #4\n    map     optional  nis #5\n    session requisite nis #6\n\nIn this example two access methods are considered: grid based and kerberos based. If user comes with grid certificate and VOMS role: extract user�s DN (1), extract and verify VOMS attributes (2), map DN+Role to a local account (3). If user comes with `Kerberos` ticket: extract local account (4). After this point in both cases we talk to `NIS` to get uid and gids for a local account (5) and, finally, adding users home directory (6).\n\n##### mutator\n\nThe `mutator` plugin is used to convert principal returned by third-party plugin into a principal, which is understood by gplazma plugins. For example, when the `jaas` plugin is configured to be used with an _ActiveMQ_ server, then login module specific principal is returned.\n\nExample:\n\n    # use activemq + mutator + authdb\n    auth jaas gplazma.jaas.name=ActiveMQ\n    map optional mutator gplazma.mutator.accept=org.apache.activemq.jaas.UserPrincipal gplazma.mutator.produce=username\n    map requisite authzdb\n\nProperties\n\n**gplazma.mutator.accept**\n\n  The fully qualified java class names of principal that have to the converted.\n\n  **gplazma.mutator.produce**\n\n  The gplazma internal principal short-hand name into which provided principal should be converted. The supported short-hand names:\n\n  | Short name| Produced Principal | Description |\n  |-----------|--------------------|-------------|\n  | dn | GlobusPrincipal| To be consumed by Grid specific plugins|\n  |kerberos| KerberosPrincipal | To be consumed bu Kerberos specific plugins|\n  |fqan| FQANPrincipal | To be consumed by Grid-VO specific plugins|\n  |name| LoginNamePrincipal| Login name which requires an aditional mapping to username|\n  |username| UserNamePrincipal|Principal which is associated with final login step|\n\n\n#### account Plug-ins\n\n##### argus\n\n The argus plug-in bans users by their DN. It talks to your site�s ARGUS system (see [https://twiki.cern.ch/twiki/bin/view/EGEE/AuthorizationFramework](https://twiki.cern.ch/twiki/bin/view/EGEE/AuthorizationFramework)) to check for banned users.\n\nProperties\n\n**gplazma.argus.hostcert**\n\n   Path to host certificate\n   Default: `/etc/grid-security/hostcert.pem`\n\n\n\n**gplazma.argus.hostkey**\n\n   Path to host key\n   Default: `/etc/grid-security/hostkey.pem`\n\n\n\n**gplazma.argus.hostkey.password**\n\n   Password for host key\n   Default:\n\n\n\n**gplazma.argus.ca**\n\n   Path to CA certificates\n   Default: `/etc/grid-security/certificates`\n\n\n\n**gplazma.argus.endpoint**\n\n   URL of PEP service\n   Default: `https://localhost:8154/authz`\n\n\n\n##### banfile\n\nThe `banfile` plug-in bans users by their principal class and the associated name. It is configured via a simple plain text file.\n\nExample:\n\n    # Ban users by principal\n    alias dn=org.globus.gsi.gssapi.jaas.GlobusPrincipal\n    alias kerberos=javax.security.auth.kerberos.KerberosPrincipal\n    alias fqan=org.dcache.auth.FQANPrincipal\n    alias name=org.dcache.auth.LoginNamePrincipal\n\n    ban name:ernie\n    ban kerberos:BERT@EXAMPLE.COM\n    ban com.example.SomePrincipal:Samson\n\nIn this example the first line is a comment. Lines 2 to 5 define aliases for principal class names that can then be used in the following banning section. The four aliases defined in this example are actually hard coded into CELL-GPLAZMA, therefore you can use these short names without explicitly defining them in your configuration file. Line 7 to 9 contain ban definitions. Line 9 directly uses the class name of a principal class instead of using an alias.\n\nPlease note that the plug-in only supports principals whose assiciated name is a single line of plain text. In programming terms this means the constructor of the principal class has to take exactly one single string parameter.\n\nThe plugin assumes that a missing file is equivalent to a file with no contents; i.e., that no one has been banned.\n\nAfter modifying the banfile, you may want to update gplazma.conf to trigger a reload to activate the changes. You can test the result with the `explain login` command in the gPlazma cell.\n\n\nProperties\n\n**gplazma.banfile.path**\n\n   Path to configuration file\n   Default: `/etc/dcache/ban.conf`\n\n\n\nTo activate the `banfile` it has to be added to `gplazma.conf`:\n\nExample:\n\n    # Map grid or kerberos users to local accounts\n    auth    optional  x509\n    auth    optional  voms\n    map     requisite vorolemap\n    map     optional  krb5\n    map     optional  nis\n    session requisite nis\n    account requisite banfile\n\n#### session Plug-ins\n\n\n\n##### kpwd\n\nThe `kpwd`plug-in adds root and home path information to the session, based on the username.\n\n\n\nProperties\n\n**gplazma.kpwd.file**\n\n   Path to **dcache.kpwd**\n   Default: `/etc/dcache/dcache.kpwd`\n\n\n\n##### authzdb\n\nThe `authzdb` plug-in adds root and home path information to the\nsession, based and username using the `storage-authzdb` file.\n\n\n\nProperties\n\n**gplazma.authzdb.file**\n\n   Path to **storage-authzdb**\n   Default: `/etc/grid-security/storage-authzdb`\n\n\n\n##### nsswitch\n\nThe `nsswitch` plug-in adds root and home path information to the session, based on the username using your system�s `nsswitch` service.\n\nTypically `nsswitch` plug-in will be combined with `vorolemap` plug-in, `gridmap` plug-in or `krb5` plug-in:\n\n\n\nExample:\n\n    # Map grid users to local accounts\n    auth    optional  x509 #1\n    auth    optional  voms #2\n    map     requisite vorolemap #3\n    map     requisite nsswitch #4\n    session requisite nsswitch #5\n\nIn this example following is happening: extract user's DN (1), extract and verify VOMS attributes (2), map DN+Role to a local account (3), extract uid and gids for a local account (4) and, finally, extract users home directory (5).\n\n\n\n##### nis\n\nThe `nis` plug-in adds root and home path information to the session, based on the username using your site�s `NIS` service.\n\n\n\nProperties\n\n**gplazma.nis.server**\n\n    `NIS` server host\n    Default: `nisserv.domain.com`\n\n\n\n**gplazma.nis.domain**\n\n    `NIS` domain\n    Default: `domain.com`\n\nThe result of `nis` can be used by other plug-ins:\n\nExample:\n\n    # Map grid or kerberos users to local accounts\n    auth    optional  x509 #1\n    auth    optional  voms #2\n    map     requisite vorolemap #3\n    map     optional  krb5 #4\n    map     optional  nis #5\n    session requisite nis #6\n\nIn this example two access methods are considered: grid based and kerberos based. If user comes with grid certificate and VOMS role: extract user's DN (1), extract and verify VOMS attributes (2), map DN+Role to a local account (3). If user comes with `Kerberos` ticket: extract local account (4). After this point in both cases we talk to NIS to get uid and gids for a local account (5) and, finally, adding users home directory (6).\n\n\n\n##### ldap\n\nThe `ldap` is a map, session and identity plugin. As a map plugin it maps user names to UID and GID. As a session plugin it adds root and home path information to the session. As an identity plugin it supports reverse mapping of UID and GID to user and group names repectively.\n\n\n\nProperties\n\n**gplazma.ldap.url**\n\n    `LDAP` server url. Use `ldap://` prefix to connect to plain `LDAP` and `ldaps://` for secured `LDAP`.\n    Example: `ldaps://example.org:389`\n\n\n\n**gplazma.ldap.organization**\n\n    Top level (`base DN`) of the `LDAP` directory tree\n    Example: `o=\"Example, Inc.\", c=DE`\n\n\n\n**gplazma.ldap.tree.people**\n\n`LDAP` subtree containing user information. The path to the user records will be formed using the `base\n                    DN` and the value of this property as a organizational unit (`ou`) subdirectory.\n\nDefault: `People`\n\nExample: Setting `gplazma.ldap.organization=o=\"Example, Inc.\", c=DE` and `gplazma.ldap.tree.people=People` will have the plugin looking in the LDAP directory `ou=People, o=\"Example, Inc.\", c=DE` for user information.\n\n\n**gplazma.ldap.tree.groups**\n\n`LDAP` subtree containing group information. The path to the group records will be formed using the `base\n                    DN` and the value of this property as a organizational unit (`ou`) subdirectory.\n\nDefault: `Groups`\n\nExample: Setting `gplazma.ldap.organization=o=\"Example, Inc.\",\n                    c=DE` and `gplazma.ldap.tree.groups=Groups` will have the plugin looking in the LDAP directory `ou=Groups, o=\"Example, Inc.\", c=DE` for group information.\n\n\n**gplazma.ldap.userfilter**\n\n`LDAP` filter expression to find user entries. The filter has to contain the `%s` exactly once. That occurence will be substituted with the user name before the filter is applied.\n\nDefault: `(uid=%s)`\n\n**gplazma.ldap.home-dir**\n\nthe user's home directory. `LDAP` attribute identifiers surrounded by `%` will be expanded to their corresponding value. You may also use a literal value or mix literal values and attributes.\n\nDefault: `%homeDirectory%`\n\n**gplazma.ldap.root-dir**\n\nthe user's root directory. LDAP attribute identifiers surrounded by `%` will be expanded to their corresponding value. You may also use a literal value or mix literal values and attributes.\n\nDefault: `/`\n\nAs a session plugin the GP2-LDAP assigns two directories to the user's session: the root directory and the home directory. The root directory is the root of the directory hierarchy visible to the user, while the home directory is the directory the user starts his session in. In default mode, the root directory is set to `/` and the home directory is set to `%homeDirectory%`, thus the user starts his session in the home directory, as it is stored on the LDAP server, and is able to go up in the directory hierarchy to `/`. For a different use-case, for example if dCache is used as a cloud storage, it may be desireable for the users to see only their own storage space. For this use case `home-dir` can be set to `/` and `root-dir` be set to `%homeDirectory%`. In both path properties any `%val%` expression will be expanded to the the value of the attribute with the name `val` as it is stored in the user record on the LDAP server.\n\n**gplazma.ldap.try-uid-mapping**\n\nDefault: `false`\n\nAllow the ldap plugin to use the user's (numerical) uid to identify the user if no username is known. If enabled, the plugin uses the `uidNumber` attribute in LDAP to establish the username for such login attempts.\n\n##### omnisession\n\nThe omnisession plugin provides an easy and convenient way to provide\nsession information for dCache users.  It uses a single configuration\nfile to describe which login attributes (such as home directory, root\ndirectory, read-only accounts, etc) are added for dCache users.\n\nIt provides a superset of functionality of some other session plugins;\nspecifically, the kpwd and storage-authzdb plugins when used as\nsession plugins: all deployments using a storage-authzdb or kpwd\nplugin during the session phase may be updated to use the omnisession\nplugin instead.\n\nThe omnisession plugin uses a single configuration file.  Without any\nexplicit configuration, this file is located at\n`${dcache.paths.etc}/omnisession.conf` (which expands to\n`/etc/dcache/omnisession.conf` by default) but the location may be\nadjusted by modifying the `dcache.paths.etc` configuration property or\nthe `gplazma.omnisession.file` configuration property.\n\nThe configuration file may have any number of empty lines.  Comments\nare also supported.  Any line that starts with a hash symbol `#` is\ntreated as a comment and ignored.\n\nAll other lines define login attributes that apply to one or more\nusers.\n\nThe general format of such lines is\n\n```\nPREDICATE ATTRIBUTE [ATTRIBUTE ...]\n```\n\nThe `PREDICATE` term describes to which users the attributes apply.\nThese mostly follow a simple `TYPE:VALUE` format; for example, the\npredicate `username:paul` matches all users with a username of `paul`,\nsimilarly, the predicate `gid:1000` matches all users with GID of\n1000.  A complete list of predicate types is given below.\n\nIn some cases, a predicate's value may have spaces.  To accomodate\nsuch cases, the value may be placed in double-quotes; for example, the\npredicate `dn:\"/C=DE/O=GermanGrid/OU=DESY/CN=Alexander Paul Millar\"`\nmatches all users with that specific distinguished name.\n\nThere is a special predicate `DEFAULT` that matches all users.  This\nmay be used to describe attributes that match all users.  A file may\nhave at most one `DEFAULT` line.\n\nThe remainder of a non-empty, non-comment line is a white-space\nseparated list of attribute terms.  There must be at least one\nattribute term, with each attribute term describing a login attribute\nthat should be added when the user logs in.\n\nAn attribute term generally has the form `TYPE:VALUE` where `TYPE`\ndescribes what kind of attribute is being defined and `VALUE`\ndescribes the specific value of that type; for example, `root:/`\nindicates that the user's root directory is `/` (the root of dCache's\nnamespace) and `home:/Users/paul` states that the user's home\ndirectory is `/Users/paul`.  A complete list of attribute types is\ngiven below.\n\nAlthough a line may contain multiple login attributes, there are\ncertain restrictions on those attributes contained on a single line.\nIt is not legal to express the same information more than once; for\nexample, a line with the two terms `root:/ root:/data` is not valid.\nThis is also not valid even if the values are consistent (e.g.,\n`root:/ root:/`).\n\nHere is a complete example\n\n```\nusername:paul  root:/ home:/Users/paul\n```\n\nIn this example, line matches user `paul` and adds two login\nattributes: a home directory (with value `/Users/paul`) and a root\ndirectory (with value `/`).\n\n\n###### Login attribute overriding\n\nDuring the login process, a user is identitied by a set of principals.\nThese principals may match multiple lines in the configuration file.\nWhen this happens, login attributes are added in file's order: a line\nthat matches the user's principals that appear nearer the top of the\nfile is processed before a matching line that appears further down the\nfile's contents.\n\nThis order matters if the multiple matching lines provide login\nattributes of the same type; for example, one matching line has a\n`home` attribute as does a subsequent matching line.\n\nIf this happens then the first attribute (the one nearest the top of\nthe file) is used.  Subsequent attributes are ignored for this login\nattempt.\n\nHere is an example of attribute overriding:\n\n```\nusername:paul  home:/Users/paul\ngroup:group-a  root:/ home:/Groups/group-a\n```\n\nIn this example, if a user with username `paul` and who is a member of\ngroup `group-a` logs in, the home directory will be `/Users/paul` and\nthe root directory will be `/`.  Other members of `group-a` will have\na home directory of `/Groups/group-a` and a root directory of `/`.\n\nThe `DEFAULT` is special.  It always matches last, irrespective of\nwhere it appears in the configuration file.  Therefore, if a login\nattempt matches any lines then those attributes will always take\nprecedence over the DEFAULT set of login attributes.\n\nAlthough dCache doesn't care where in the file the DEFAULT predicate\nis located, but it's recommended to put it at the end of the file.\n\nThe following is another example of attribute overriding, but this\ntime using DEFAULT.\n\n```\nusername:paul  home:/Users/paul\ngroup:group-a  home:/Groups/group-a\nDEFAULT        root:/ home:/\n```\n\nIn this example, the user with username `paul` has a home directory\n`/Users/paul`, members of `group-a` have a home directory\n`/groups/group-a` (except for user `paul`) and all dCache users have a\nhome directory of `/` (except for members of group-a and user `paul`).\nAll users have a root directory of `/`.\n\n\n###### Combining with other session plugins\n\nIf the omnisession plugin is combined with other session plugins then\nomniplugin will refrain from adding attributes that are already\ndefined.  Omnisession will add any other (undefined) attributes.\n\nHere's an example.  gPlazma is configured to use both `ldap` and\n`omnisession` as session plugins.  When user X logs in the ldap plugin\nadding a home directory and the omnisession configuration file\nindicates both a home directory and a root directory for user X.  This\nuser will receive the home directory from LDAP and the root directory\nfrom omnisession.\n\nThe omnisession plugin is successful if it adds at least one login\nattribute, and fails otherwise.  This affects gPlazma configuration\n(e.g., optional vs sufficient vs requisite).  For details, see the\nsection of gPlazma configuration.\n\nNote that users are required to have a home directory attribute and a\nroot directory attribute before their login will be successful.\n\n###### Handling bad configuration\n\nA line within the configuration file with bad attribute declarations\nwill be marked invalid.  The plugin will fail for any user who's\nprincipals match that invalid line's predicate.  The plugin will be\nsuccessful for users with predicates that only match valid lines and\nfor whom at least one attribute is added.\n\nHere is an example:\n\n```\nusername:paul  home:/ home:/Users/paul\nDEFAULT        root:/ home:/\n```\n\nThe first line is invalid because the `home` attribute is defined\ntwice.  Therefore, the plugin will fail for a login attempt with\nusername `paul`, but will succeed for other users.\n\nIf a line has a badly written predicate then the entire file is\nconsidered bad and the plugin will fail for all users.\n\nHere is an example:\n\n```\nuser:paul  home:/Users/paul\nDEFAULT    root:/ home:/\n```\n\nThe predicate `user:paul` is not valid (it should have be\n`username:paul`).  Because of this, the plugin will fail for all\nusers.\n\n###### Predicates catalogue\n\nThe omnisession plugin accepts the following predicates.\n\n- **dn** The user's Distinguished Name, which is typically obtained by\n   authenticating via X.509.  Example:\n   `dn:\"/C=DE/O=GermanGrid/OU=DESY/CN=Alexander Paul Millar\"`\n- **email** Predicate matches the user's email address, if known.\n   Example: `email:user@example.org` matches users with email address\n   `user@example.org`.\n- **gid** Predicate matches the user's gid.  There is an optional\n   field that describes whether the match should be limited to users\n   with a matching primary gid, a matching nonprimary gid, or a match\n   gid of either type.  Example: `gid:1000` matches a user that is a\n   member of the gid 1000 group.  `gid:1000,primary` matches members\n   of the group with gid 1000 if the user has this as their primary\n   gid, `gid:1000,nonprimary` matches members of the group with gid\n   1000 if the user has this as their non-primary gid.\n- **group** Predicate that matches members of the named group. There\n   is an optional field that describes whether the match should be\n   limited to users with a matching primary group, a matching\n   nonprimary group, or a matching group of either use.  Example:\n   `group:group-a` matches a user that is a member of the `group-a`\n   group.  `group:group-a,primary` matches members of the `group-a`\n   group if the user has this as their primary group,\n   `group:group-a,nonprimary` matches members of the `group-a` group\n   if the user has this as their non-primary gid.\n- **fqan** Predicate that matches a VOMS FQAN. There is an optional\n   field that describes whether the match should be limited to users\n   with a matching primary FQAN, a matching nonprimary FQAN, or a\n   matching FQAN of either use.  Example: `fqan:/atlas` matches a user\n   that is a member of the `/atlas` FQAN.  `fqan:/atlas,primary`\n   matches members of the `/atlas` FQAN if the user has this as their\n   primary FQAN, `fqan:/atlas,nonprimary` matches members of the\n   `/atlas` FQAN if the user has this as their non-primary FQAN.\n- **kerberos** Predicate that matches a Kerberos principal.  Example:\n   `kerberos:paul@DESY.DE` matches users with the Kerberos principal\n   `paul@DESY.DE`.\n- **oidc** Predicate that matches an OpenID-Connect `sub` (\"subject\")\n   claim from a specific OP.  Example:\n   `oidc:83326983-68a3-4f2c-8f0b-385ecd3e97b2@OP` matches users\n   identified by the `sub` claim with the value\n   `83326983-68a3-4f2c-8f0b-385ecd3e97b2` who have authenticated with\n   the OAuth2 Provider configured with the alias `OP`.\n- **oidcgrp** Predicate that matches the OpenID-Connect `groups` claim\n   values.  Example: `oidcgrp:/main-group` matches `groups` claim that\n   contains a value `/main-group`.\n- **uid** Predicate that matches the uid of a user.  Example:\n   `uid:15691` matches the user with uid 15691.\n- **username** Predicate that matches the username of a user.\n   Example: `username:paul` matches the user with username `paul`.\n- **entitlement** Predicate that matches an eduPersonEntitlement\n   claim.  Example:\n   `entitlement:urn:geant:example.org:group:EXAMPLE#example.org`\n   matches users that have the eduPersonEntitlement claim\n   `urn:geant:helmholtz.de:group:EXAMPLE#login.helmholtz.de`\n\n###### Login attribute catalogue\n\nThis section provides a list of all login attributes that the\nomnisession plugin supports.\n\nThe `root`, `home` and `prefix` attributes accept an absolute path as\na value.  This means those values must start with a `/`.\n\n- **read-only** This attribute limits the user so that the may not\n   modify dCache.  Example: `read-only`.\n- **root** The user's root directory.  The user will see only a\n   subtree of dCache's namespace.  This works similarly to the chroot\n   command. Example: `root:/data/experiment-A` that limits the user's\n   view of the namespace to the `/data/expriment-A` subtree.\n- **home** The user's home directory. In general, this is a directory\n   that is somehow special for the user, although the precise\n   semantics of this attribute is protocol specific.  There are\n   network protocols or clients that have no concept of a home\n   directory. Example: `home:/Users/paul` indicates the user has a\n   home directory of `/Users/paul`.\n- **prefix** This attribute limits which part of the namespace a user\n   may access.  Unlike the `root` attribute, the user sees parent\n   directories; however, parent directories appear as if they have a\n   single entry.  Example: `prefix:/data/experiment-A` allows the user\n   access to a specific subtree while preserving paths.\n- **max-upload** Limit the size of any single uploaded file.  The\n   value is a file size that may optionally ISO symbols.  Examples:\n   `max-upload:5GiB` limits uploads to a maximum of five gibibyte\n   (~5.4e9 bytes), `max-upload:1TB` limits uploads to a maximum of one\n   terabyte (1e12 bytes), `max-upload:1048576` limits users to one\n   mibibyte files.\n\n#### identity Plug-ins\n\n##### nsswitch\n\nThe `nsswitsch` provides forward and reverse mapping for `NFSv4.1` using your system's `nsswitch` service.\n\n##### nis\n\nThe `nis` plug-in forward and reverse mapping for `NFSv4.1` using your site's NIS service.\n\nProperties\n\n**gplazma.nis.server**\n\n   `NIS` server host\n   Default: `nisserv.domain.com`\n\n**gplazma.nis.domain**\n\n   `NIS` domain\n    Default: domain.com\n\n## Using X509 Certificates\n\nMost plug-ins of `gPlazma` support `X.509` certificates for authentication and authorisation. `X.509` certificates are used to identify entities (e.g., persons, hosts) in the Internet. The certificates contain a DN (Distinguished Name) that uniquely describes the entity. To give the certificate credibility it is issued by a CA (Certificate Authority) which checks the identity upon request of the certificate (e.g., by checking the persons id). For the use of X.509 certificates with dCache your users will have to request a certificate from a CA you trust and you need host certificates for every host of your dCache instance.\n\n\n### CA Certificates\n\nTo be able to locally verify the validity of the certificates, you need to store the CA certificates on your system. Most operating systems come with a number of commercial CA certificates, but for the *Grid* you will need the certificates of the Grid CAs. For this, CERN packages a number of CA certificates. These are deployed by most grid sites. By deploying these certificates, you state that you trust the CA's procedure for the identification of individuals and you agree to act promptly if there are any security issues.\n\nTo install the CERN CA certificates follow the following steps:\n\n```console-root\ncd /etc/yum.repos.d/\nwget http://grid-deployment.web.cern.ch/grid-deployment/glite/repos/3.2/lcg-CA.repo\nyum install lcg-CA\n```\n\nThis will create the directory `/etc/grid-security/certificates` which\ncontains the Grid CA certificates.\n\nCertificates which have been revoked are collected in certificate\nrevocation lists (CRLs). To get the CRLs install the `fetch-crl`\ncommand as described below.\n\n```console-root\nyum install fetch-crl\nfetch-crl\n```\n\n`fetch-crl` adds X.509 CRLs to `/etc/grid-security/certificates`. It\nis recommended to set up a cron job to periodically update the CRLs.\n\n### User Certificate\n\nIf you do not have a valid grid user certificate yet, you have to\nrequest one from your CA. Follow the instructions from your CA on how\nto get a certificate. After your request was accepted you will get a\nURL pointing to your new certificate. Install it into your browser to\nbe able to access grid resources with it. Once you have the\ncertificate in your browser, make a backup and name it\n`userCertificate.p12`. Copy the user certificate to the directory\n`~/.globus/` on your worker node and convert it to `usercert.pem`\nand `userkey.pem` as described below.\n\n```console-user\nopenssl pkcs12 -clcerts -nokeys -in <userCertificate>.p12 -out usercert.pem\n|Enter Import Password:\n|MAC verified OK\n```\n\nDuring the backup your browser asked you for a password to encrypt the certificate. Enter this password here when asked for a password. This will create your user certificate.\n\n```console-user\nopenssl pkcs12 -nocerts -in <userCertificate>.p12 -out userkey.pem\n|Enter Import Password:\n|MAC verified OK\n|Enter PEM pass phrase:\n```\n\nIn this step you need to again enter the backup password. When asked for the PEM pass phrase choose a secure password. If you want to use your key without having to type in the pass phrase every time, you can remove it by executing the following command.\n\n```console-root\nopenssl rsa -in userkey.pem -out userkey.pem\n|Enter pass phrase for userkey.pem:\n|writing RSA key\n```\n\nNow change the file permissions to make the key only readable by you and the certificate world readable and only writable by you.\n\n```console-root\nchmod 400 userkey.pem\nchmod 644 usercert.pem\n```\n\n### Host Certificate\n\nTo request a host certificate for your server host, follow again the instructions of your CA.\n\nThe conversion to `hostcert.pem` and `hostkey.pem` works analogous to\nthe user certificate. For the hostkey you have to remove the pass\nphrase. How to do this is also explained in the previous\nsection. Finally copy the `host*.pem` files to `/etc/grid-security/`\nas `root` and change the file permissions in favour of the user\nrunning the grid application.\n\n### VOMS Proxy Certificate\n\nFor very large groups of people, it is often more convenient to authorise people based on their membership of some group. To identify that they are a member of some group, the certificate owner can create a new short-lived `X.509` certificate that includes their membership of various groups. This short-lived certificate is called a proxy-certificate and, if the membership information comes from a VOMS server, it is often referred to as a VOMS-proxy.\n\n```console-root\ncd /etc/yum.repos.d/\nwget http://grid-deployment.web.cern.ch/grid-deployment/glite/repos/3.2/glite-UI.repo\nyum install glite-security-voms-clients\n```\n\n#### `Creating a VOMS proxy`\n\nTo create a VOMS proxy for your user certificate you need to execute\nthe `voms-proxy-init` as a user.\n\nExample:\n\n```console-user\nexport PATH=/opt/glite/bin/:$PATH\nvoms-proxy-init\n|Enter GRID pass phrase:\n|Your identity: /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\n|\n|Creating proxy ..................................Done\n|Your proxy is valid until Mon Mar  7 22:06:15 2011\n```\n\n\n##### Certifying your membership of a VO\n\n You can certify your membership of a VO by using the command\n `voms-proxy-init -voms <yourVO>`. This is useful as in dCache\n authorization can be done by VO (see [the section called �Authorizing\n a VO�](#authorizing-a-vo)). To be able to use the extension `-voms\n <yourVO>` you need to be able to access VOMS servers. To this end you\n need the the VOMS server�s and the CA�s DN. Create a file\n `/etc/grid-security/vomsdir/<VO>/<hostname>.lsc` per VOMS server\n containing on the 1st line the VOMS server�s DN and on the 2nd line,\n the corresponding CA�s DN. The name of this file should be the fully\n qualified hostname followed by an `.lsc` extension and the file must\n appear in a subdirectory `/etc/grid-security/vomsdir/<VO>` for each\n VO that is supported by that VOMS server and by the site.\n\nAt [http://operations-portal.egi.eu/vo](https://operations-portal.egi.eu/vo) you can search for a VO and find this information.\n\n\nExample:\n\nFor example, the file `/etc/grid-security/vomsdir/desy/grid-voms.desy.de.lsc` contains:\n\n    /C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de\n    /C=DE/O=GermanGrid/CN=GridKa-CA\n\nwhere the first entry is the DN of the DESY VOMS server and the second entry is the DN of the CA which signed the DESY VOMS server's certificate.\n\nIn addition, you need to have a file `/opt/glite/etc/vomses`\ncontaining your VO's VOMS server.\n\nExample:\n\nFor DESY the file `/opt/glite/etc/vomses` should contain the entry\n\n    \"desy\" \"grid-voms.desy.de\" \"15104\" \"/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de\" \"desy\" \"24\"\n\nThe first entry �desy� is the real name or a nickname of your VO. �grid-voms.desy.de� is the hostname of the VOMS server. The number �15104� is the port number the server is listening on. The forth entry is the DN of the server's VOMS certificate. The fifth entry, �desy�, is the VO name and the last entry is the globus version number which is not used anymore and can be omitted.\n\n\nExample:\n\nUse the command `voms-proxy-init -voms` to create a VOMS proxy with VO\n�desy�.\n\n```console-user\nvoms-proxy-init -voms desy\n|Enter GRID pass phrase:\n|Your identity: /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\n|Creating temporary proxy ....................................................... Done\n|Contacting  grid-voms.desy.de:15104 [/C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de] \"desy\" Done\n|Creating proxy .................................... Done\n|Your proxy is valid until Mon Mar  7 23:52:13 2011\n```\n\nView the information about your VOMS proxy with `voms-proxy-info`\n\n```console-user\nvoms-proxy-info\n|subject   : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe/CN=proxy\n|issuer    : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\n|identity  : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\n|type      : proxy\n|strength  : 1024 bits\n|path      : /tmp/x509up_u500\n|timeleft  : 11:28:02\n```\n\nThe last line tells you how much longer your proxy will be valid.\n\nIf your proxy is expired you will get\n\n```console-user\nvoms-proxy-info\n|subject   : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe/CN=proxy\n|issuer    : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\n|identity  : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\n|type      : proxy\n|strength  : 1024 bits\n|path      : /tmp/x509up_u500\n|timeleft  : 0:00:00\n```\n\nThe command `voms-proxy-info -all` gives you information about the\nproxy and about the VO.\n\n```console-user\nvoms-proxy-info -all\nsubject   : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe/CN=proxy\nissuer    : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\nidentity  : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\ntype      : proxy\nstrength  : 1024 bits\npath      : /tmp/x509up_u500\ntimeleft  : 11:24:57\n=== VO desy extension information ===\nVO        : desy\nsubject   : /C=DE/O=GermanGrid/OU=DESY/CN=John Doe\nissuer    : /C=DE/O=GermanGrid/OU=DESY/CN=host/grid-voms.desy.de\nattribute : /desy/Role=NULL/Capability=NULL\nattribute : /desy/test/Role=NULL/Capability=NULL\ntimeleft  : 11:24:57\nuri       : grid-voms.desy.de:15104\n```\n\nUse the command `voms-proxy-destroy` to destroy your VOMS proxy.\n\n```console-user\nvoms-proxy-destroy\nvoms-proxy-info\n|\n|Couldn't find a valid proxy.\n```\n\n\n## Using OpenID Connect\n\ndCache also supports the use of OpenID Connect bearer tokens as a means of authentication.\n\nOpenID Connect is a federated identity system.  The dCache users see federated identity as a way to use their existing username & password safely.  From a dCache admin's point-of-view, this involves \"outsourcing\" responsibility for checking users identities to some external service: you must trust that the service is doing a good job.\n\n> OpenID Connect is a simple identity layer on top of the OAuth 2.0 protocol. It enables Clients to verify the identity of the End-User based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the End-User in an interoperable and REST-like manner.\n>\n> --<cite>http://openid.net/specs/openid-connect-core-1_0.html<cite>\n\nCommon examples of Authorisation servers are Google, Indigo-IAM, Cern-Single-Signon etc.\n\nAs of version 2.16, dCache is able to perform authentication based on [OpendID Connect](http://openid.net/specs/openid-connect-core-1_0.html) credentials on its HTTP end-points. In this document, we outline the configurations necessary to enable this support for OpenID Connect.\n\nOpenID Connect credentials are sent to dCache with Authorisation HTTP Header as follows\n`Authorization: Bearer  <yaMMeexxx........>`. This bearer token is extracted, validated and verified against a **Trusted Authorisation Server** (Issue of the bearer token) and is used later to fetch additional user identity information from the corresponding Authorisation Server.\n\n### Steps for configuration\n\nIn order to configure the OpenID Connect support, we need to\n\n1. configure the gplazma plugins providing the support for authentication using OpenID credentials and mapping a verified OpenID credential to dCache specific `username`, `uid` and `gid`.\n2. enabling the plugins in gplazma\n\n### Gplazma Plugins for OpenId Connect\n\nThe support for OpenID Connect in Cache is achieved with the help of two gplazma plugins.\n\n#### OpenID Authenticate Plugin (oidc)\nIt takes the extracted OpenID connect credentials (Bearer Token) from the HTTP requests and validates it against a OpenID Provider end-point. The admins need to obtain this information from their trusted OpenID Provider such as Google.\n\nIn case of Google, the provider end-point can be obtained from the url of its [Discovery Document](http://openid.net/specs/openid-connect-core-1_0.html#OpenID.Discovery), e.g. https://accounts.google.com/.well-known/openid-configuration. Hence, the provider end-point in this case would be **accounts.google.com**.\n\nThis end-point has to be appended to the gplazma property **gplazma.oidc.hostnames**, which should be added to the layouts file. Multiple trusted OpenID providers can be added with space separated list as below.\n\n`gplazma.oidc.hostnames = accounts.google.com iam-test.indigo-datacloud.eu`\n\n#### MultiMap plugin (multimap)\n\ndCache requires that authenticated credentials be mapped to posix style `username`, `uid` and `gid`. In case of OpenID credentials, it can be achieved through the new gplazma multimap plugin. This plugin is able to take a verified OpenID credentials in the form of OpenID Subject or the corresponding Email address, and map it to a username, uid, gid etc.\n\nFor example,\n\n> oidc:9889-1231-2999-12312@GOOGLE    username:kermit\n>\n> email:kermit.the.frog@email.com     username:thefrog\n\nIn this example, the first line matches users with `sub` claim\n`9889-1231-2999-12312` from the OAuth2 Provider `GOOGLE` and adds the\nusername `kermit`.  The second example matches the email address\n`kermit.the.frog@email.com` and adds the username `thefrog`.  In both\ncases, it is assumed there is an additional mapping from username to\nuid, gid etc in files like storage-autzdb.\n\nThis mapping as shown above can be stored in a gplazma multi-map configuration file. The location of the multimap configuration file can be specified with another gplazma property **gplazma.multimap.file**. By default it is configured to be located in /etc/dcache/multi-mapfile.\n\n#### Enable the gplazma plugins\n\nThe two plugins above must be enabled in the gplazma.conf.\n\n> auth optional oidc\n\n> map optional  multimap\n\nRestart dCache and check that there are no errors in loading these gplazma plugins.\n\n#### Third-Party Transfer with OpenID Connect Credentials\n\nThird-party transfer with OpenID Connect Credentials are also possible. dCache performs a token-exchange with the OpenID provider in order and obtain a new delegated bearer token for itself, which it can use (and refresh) to perform Third-party transfer.\n\nIn order to perform a token-exchange, it requires the id and secret of a client registered to the OpenID Provider same as that of the bearer token (that was received with the COPY request).\n\nThe client-id and client-secret of such a client can be set in the webdav properties as follows,\n\n> webdav.oidc.client.ids!provider.hostname : id of a client registered to an OpenId Provider\n\n> webdav.oidc.client.secrets!provider.hostname : client-secret corresponding the client-id specified above\n\nHere, **provider.hostname** must be replaced with a supported OpenID provider like accounts.google.com.\n\n## Configuration files\n\nIn this section we explain the format of the the **storage-authzdb, kpwd** and **vorolemap** files. They are used by the `authzdb` plug-in, `vorolemap` plug-in,and `kpwd` plug-in.\n\n### `storage-authzdb`\n\nIn `gPlazma`, except for the `kpwd` plug-in, authorization is a two-step process. First, a username is obtained from a mapping of the user�s DN or his DN and role, then a mapping of username to UID and GID with optional additional session parameters like the root path is performed. For the second mapping usually the file called **storage-authzdb** is used.\n\n#### Preparing **storage-authzdb**\n\nThe default location of the **storage-authzdb** is\n`/etc/grid-security`. Before the mapping entries there has to be a\nline specifying the version of the used file format.\n\nExample:\n\n    version 2.1\n\ndCache supports versions 2.1 and to some extend 2.2.\n\nExcept for empty lines and comments (lines start with `#`) the configuration lines have the following format:\n\n      authorize <username> (read-only|read-write) <UID> <GID>[,<GID>]* <homedir> <rootdir>\n\nFor legacy reasons there may be a third path entry which is ignored by dCache. The username here has to be the name the user has been mapped to in the first step (e.g., by his DN).\n\nExample:\n\n    authorize john read-write 1001 100 / /data/experiments /\n\nIn this example user <john> will be mapped to UID 1001 and GID 100 with read access on the directory `/data/experiments`. You may choose to set the user's root directory to `/`.\n\nExample:\n\n     authorize adm read-write 1000 100 / / /\n\nIn this case the user <adm> will be granted read/write access in any path, given that the file system permissions in CHIMERA also allow the transfer.\n\nThe first path is nearly always left as �`/`�, but it may be used as a home directory in interactive session, as a subdirectory of the root path. Upon login, the second path is used as the user's root, and a �cd� is performed to the first path. The first path is always defined as being relative to the second path.\n\nMultiple GIDs can be assigned by using comma-separated values for the GID file, as in\n\nExample:\n\n    authorize john read-write 1001 100,101,200 / / /\n\nThe lines of the `storage-authzdb` file are similar to the �login� lines of the `dcache.kpwd` file. If you already have a `dcache.kwpd` file, you can easily create `storage-authzdb` by taking the lines from your `dcache.kpwd` file that start with the word `login`, for example,\n\n    login john read-write 1001 100 / /data/experiments /\n\nand replace the word `login` with `authorize`. The following line does this for you.\n\n```console-root\nsed \"s/^ *login/authorize/\" dcache.kpwd|grep \"^authorize\" > storage-authzdb\n```\n\n### The gplazmalite-vorole-mapping plug-in\n\nThe second is the **storage-authzdb** used in other plug-ins. See the above documentation on [`storage-authdb`](config-gplazma.md#storage-authzdb) for how to create the file.\n\n#### Preparing `grid-vorolemap`\n\nThe file is similar in format to the `grid-mapfile`, however there is an additional field following the DN (Certificate Subject), containing the FQAN (Fully Qualified Attribute Name).\n\n    \"/C=DE/O=GermanGrid/OU=DESY/CN=John Doe\" \"/some-vo\" doegroup\n    \"/C=DE/DC=GermanGrid/O=DESY/CN=John Doe\" \"/some-vo/Role=NULL\" doegroup\n    \"/C=DE/DC=GermanGrid/O=DESY/CN=John Doe\" \"/some-vo/Role=NULL/Capability=NULL\" doegroup\n\nTherefore each line has three fields: the user's DN, the user's FQAN, and the username that the DN and FQAN combination are to be mapped to.\n\nThe FQAN is sometimes semantically referred to as the �role�. The same user can be mapped to different usernames depending on what their FQAN is. The FQAN is determined by how the user creates their proxy, for example, using [`voms-proxy-init`](config-gplazma.md#voms-proxy-certificate). The FQAN contains the user's Group, Role (optional), and Capability (optional). The latter two may be set to the string �NULL�, in which case they will be ignored by the plug-in. Therefore the three lines in the example above are equivalent.\n\nExample:\n\nIf a user is authorized in multiple roles, for example\n\n    \"/DC=org/DC=doegrids/OU=People/CN=John Doe\" \"/some-vo/sub-grp\" vo_sub_grp_user\n    \"/DC=org/DC=doegrids/OU=People/CN=John Doe\" \"/some-vo/sub-grp/Role=user\" vouser\n    \"/DC=org/DC=doegrids/OU=People/CN=John Doe\" \"/some-vo/sub-grp/Role=admin\" voadmin\n    \"/DC=org/DC=doegrids/OU=People/CN=John Doe\" \"/some-vo/sub-grp/Role=prod\" voprod\n\nhe will get the username corresponding to the FQAN found in the proxy that the user creates for use by the client software. If the user actually creates several roles in his proxy, authorization (and subsequent check of path and file system permissions) will be attempted for each role in the order that they are found in the proxy.\n\nIn a `GRIDFTP` URL, the user may also explicitly request a username.\n\n    gsiftp://doeprod@ftp-door.example.org:2811/testfile1\n\nin which case other roles will be disregarded.\n\n### Authorizing a VO\n\nInstead of individual DNs, it is allowed to use `*` or `\"*\"` as the first field, such as\n\nExample:\n\n    \"*\" \"/desy/Role=production/\" desyprod\n\nIn that case, any DN with the corresponding role will match. It should be noted that a match is first attempted with the explicit DN. Therefore if both DN and `\"*\"` matches can be made, the DN match will take precedence.\n\nThus a user with subject `/C=DE/O=GermanGrid/OU=DESY/CN=John Doe` and\nrole `/desy/Role=production` will be mapped to username `desyprod` via\nthe above `storage-authzdb` line with `\"*\"` for the DN, except if\nthere is also a line such as\n\n    \"/C=DE/O=GermanGrid/OU=DESY/CN=John Doe\" \"/desy/Role=production\" desyprod2\n\nin which case the username will be `desyprod2`.\n\n#### More Examples\n\nSuppose that there are users in production roles that are expected to\nwrite into the storage system data which will be read by other\nusers. In that case, to protect the data the non-production users\nwould be given read-only access. Here in\n`/etc/grid-security/grid-vorolemap` the production role maps to\nusername `cmsprod`, and the role which reads the data maps to\n`cmsuser`.\n\n    \"*\" \"/cms/uscms/Role=cmsprod\" cmsprod \"*\" \"/cms/uscms/Role=cmsuser\" cmsuser\n\nThe read-write privilege is controlled by the third field in the lines\nof `/etc/grid-security/storage-authzdb`\n\n    authorize cmsprod  read-write  9811 5063 / /data /\n    authorize cmsuser  read-only  10001 6800 / /data /\n\nExample:\n\nAnother use case is when users are to have their own directories\nwithin the storage system. This can be arranged within the\nCELL-GPLAZMA configuration files by mapping each user's DN to a unique\nusername and then mapping each username to a unique root path. As an\nexample, lines from `/etc/grid-security/grid-vorolemap` would\ntherefore be written\n\n    \"/DC=org/DC=doegrids/OU=People/CN=Selby Booth\" \"/cms\" cms821\n    \"/DC=org/DC=doegrids/OU=People/CN=Kenja Kassi\" \"/cms\" cms822\n    \"/DC=org/DC=doegrids/OU=People/CN=Ameil Fauss\" \"/cms\" cms823\n\nand the corresponding lines from `/etc/grid-security/storage-authzdb`\nwould be\n\n    authorize cms821 read-write 10821 7000 / /data/cms821 /\n    authorize cms822 read-write 10822 7000 / /data/cms822 /\n    authorize cms823 read-write 10823 7000 / /data/cms823 /\n\n### The kpwd plug-in\n\nThe section in the `gPlazma` policy file for the kpwd plug-in\nspecifies the location of the `dcache.kpwd` file, for example\n\nExample:\n    # dcache.kpwd\n    kpwdPath=\"/etc/dcache/dcache.kpwd\"\n\nTo maintain only one such file, make sure that this is the same\nlocation as defined in `/usr/share/dcache/defaults/dcache.properties`.\n\nUse `/usr/share/dcache/examples/gplazma/dcache.kpwd` to create this\nfile.\n\nTo be able to alter entries in the `dcache.kpwd` file conveniantly the\ndcache script offers support for doing this.\n\nExample:\n\n```console-root\ndcache kpwd dcuseradd testuser -u 12345 -g 1000 -h / -r / -f / -w read-write -p password\n```\n\nadds this to the kpwd file:\n\n```\npasswd testuser ae39aec3 read-write 12345 1000 / /\n```\n\nThere are many more commands for altering the kpwd-file, see the dcache-script help for further commands available.\n\n### The gridmap plug-in\n\nTwo file locations are defined in the policy file for this plug-in:\n\n    # grid-mapfile\n    gridMapFilePath=\"/etc/grid-security/grid-mapfile\"\n    storageAuthzPath=\"/etc/grid-security/storage-authzdb\"\n\n#### Preparing the `grid-mapfile`\n\nThe `grid-mapfile` is the same as that used in other applications. It can be created in various ways, either by connecting directly to VOMS or GUMS servers, or by hand.\n\nEach line contains two fields: a DN (Certificate Subject) in quotes, and the username it is to be mapped to.\n\nExample:\n\n    \"/C=DE/O=GermanGrid/OU=DESY/CN=John Doe\" johndoe\n\nWhen using the `gridmap`, the `storage-authzdb` file must also be\nconfigured. See [the section called\n�storage-authzdb�](config-gplazma.md#storage-authzdb) for details.\n\n## gPlazma specific dCache configuration\n\ndCache has many parameters that can be used to configure the systems\nbehaviour. You can find all these parameters well documented and\ntogether with their default values in the properties files in\n`/usr/share/dcache/defaults/`. To use non-default values, you have to\nset the new values in `/etc/dcache/dcache.conf` or in the layout\nfile. Do not change the defaults in the properties files! After\nchanging a parameter you have to restart the concerned cells.\n\nRefer to the file `gplazma.properties` for a full list of properties\nfor `gPlazma` One commonly used property is\n`gplazma.cell.limits.threads`, which is used to set the maximum number\nof concurrent requests to gPlazma. The default value is `30`.\n\nSetting the value for `gplazma.cell.limits.threads` too high may result in large spikes of CPU activity and the potential to run out of memory. Setting the number too low results in potentially slow login activity.\n\n### Enabling Username/Password Access for WEBDAV\n\nThis section describes how to activate the Username/Password access\nfor `WebDAV`. It uses `dcache.kwpd` file as an example format for\nstoring Username/Password information. First make sure `gPlazma2` is\nenabled in the `/etc/dcache/dcache.conf` or in the layout file.\n\nExample:\n\nCheck your WebDAV settings: enable the HTTP access, disallow the\nanonymous access, disable requesting and requiring the client\nauthentication and activate basic authentication.\n\n```ini\nwebdav.authn.protocol=http\nwebdav.authz.anonymous-operations=NONE\nwebdav.authn.accept-client-cert=false\nwebdav.authn.require-client-cert=false\nwebdav.authn.basic=true\n```\n\nAdjust the `/etc/dcache/gplazma.conf` to use the `kpwd` plug-in (for\nmore information see also [the section called\n�Plug-ins�](config-gplazma.md#plug-ins).\n\nIt will look something like this:\n\n    auth optional kpwd\n    map requisite kpwd\n    session requisite kpwd\n\nThe `/etc/dcache/dcache.kpwd` file is the place where you can specify\nthe username/password record. It should contain the username and the\npassword hash, as well as UID, GID, access mode and the home, root and\nfsroot directories:\n\n    # set passwd\n    passwd tanja 6a4cd089 read-write 500 100 / / /\n\nThe passwd-record could be automatically generated by the dCache kpwd-utility, for example:\n\n```console-root\ndcache kpwd dcuseradd -u 500 -g 100 -h / -r / -f / -w read-write -p dickerelch tanja\n```\n\nSome file access examples:\n\n    curl -u tanja:dickerelch http://webdav-door.example.org:2880/pnfs/\n\n    wget --user=tanja --password=dickerelch http://webdav-door.example.org:2880/pnfs/\n\n### Roles\n\nRoles are a way of describing what capabilities a given user has.  They constitute\na set of operations defined either explicitly or implicitly which the user who\nis assigned that role is permitted to exercise.\n\nRoles further allow users to act in more than one capacity without having to\nchange their basic identity. For instance, a \"superuser\" may wish to act as _janedoe_\nfor some things, but as an administrator for others, without having to reauthenticate.\n\nWhile the role framework in dCache is designed to be extensible, there\ncurrently exists only one recognized role, that of _admin_.\n\nTo activate the use of the _admin_ role, the following steps are necessary.\n\n1) Define the admin role using the property:\n\n```ini\ngplazma.roles.admin-gid=<gid>\n```\n\n2) Add the _admin_ gid to the set of gids for any user who should have this capability.\n\n3) Add the roles plugin to your gPlazma configuration (usually 'requisite' is sufficient):\n\n    session requisite roles\n\nRoles are currently used by the [dCache Frontend Service](config-frontend.md)\nto distinguish between regular and admin access.  Please refer to that section of\nthis document for further information.\n\n<!--  [vorolemap]: #cf-gplazma-plug-inconfig-vorolemap-gridvorolemap\n  [section\\_title]: #cf-gplazma-plug-inconfig-voauth\n  []: http://operations-portal.egi.eu/vo\n  [`storage-authdb`]: #cf-gplazma-plug-inconfig-authzdb\n  [`voms-proxy-init`]: #cb-voms-proxy-glite\n  [1]: #cf-gplazma-gp2-configuration-plug-ins\n  [2]: #cf-gplazma-kpwd\n"], "reference": "gPlazma1 has been completely removed from dCache. If you are upgrading, you must reconfigure gPlazma if you used gPlazma1 until now.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How does the RESTful API interact with the dCache History Service to provide data?", "reference_contexts": ["CHAPTER 17. dCache History Service\n=====================================\n\n-----\n[TOC bullet hierarchy]\n-----\n\nThe purpose of this service is to provide a disk-backed cache for time-windowed\nstate data extracted from backend dCache components, most importantly, pools.\n\nThe service executes collection updates periodically to its write-through cache\nso that the data is both stored on disk and held in memory.  In the case of\nrestart, the existing files are read back.\n\nThe Frontend service contacts the history service using normal dCache messaging\nin order to serve this data through the RESTful API.\n\nCurrently, pool state information is saved by the History service into\na set of .json files; their default location is\n`/var/lib/dcache/pool-history`\n\non the file system local to the node running the service.  This\nlocation, along with the collection intervals, can be configured from\nproperties; see `/usr/share/dcache/defaults/history.properties`.\n\nRunning this service is essential for the delivery of histogram data from the\nfrontend.  It can be run out of the box with no special properties\nset:\n\n```ini\n[historyDomain]\n[historyDomain/history]\n```\n\nWhile it is not required to run this service in its own domain, it is usually\ngood practice to isolate a non-essential service this way, allowing \na restart which does not affect other services, should that be necessary.\n"], "reference": "The Frontend service contacts the history service using normal dCache messaging in order to serve data through the RESTful API.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What dCache mean for file hopping?", "reference_contexts": ["CHAPTER 8. FILE HOPPING\n=======================\n\n-----\n[TOC bullet hierarchy]\n-----\n\nFile hopping is a collective term in dCache, summarizing the possibility of having files being transferred between dCache pools triggered by a variety of conditions. The most prominent examples are:\n\n-   If a file is requested by a client but the file resides on a pool from which this client, by configuration, is not allowed to read data, the dataset is transferred to an �allowed� pool first.\n\n-   If a pool encounters a steady high load, the system may, if configured, decide to replicate files to other pools to achieve an equal load distribution.\n\n-   HSM restore operations may be split into two steps. The first one reads data from tertiary storage to an �HSM connected� pool and the second step takes care that the file is replicated to a general read pool. Under some conditions this separation of HSM and non-HSM pools might become necessary for performance reasons.\n\n-   If a dataset has been written into dCache it might become necessary to have this file replicated instantly. The reasons can be, to either have a second, safe copy, or to make sure that clients don't access the file for reading on the write pools.\n\n## File hopping on arrival from outside dCache\n\n*File Hopping on arrival* is a term, denoting the possibility of initiating a pool to pool transfer as the result of a file successfully arriving on a pool from some external client. Files restored from HSM or arriving on a pool as the result of a pool to pool transfer will not yet be forwarded.\n\nForwarding of incoming files can be enabled by setting the\n`pool.destination.replicate` property in the `/etc/dcache/dcache.conf`\nfile or per pool in the layout file. It can be set to on,\n`PoolManager` or `HoppingManager`, where on does the same as\n`PoolManager`.\n\nThe pool is requested to send a `replicateFile` message to either the `PoolManager` or to the `HoppingManager`, if available. The different approaches are briefly described below and in more detail in the subsequent sections.\n\n-   The `replicateFile` message is sent to the `PoolManager`. This happens for all files arriving at that pool from outside (no restore or p2p). No intermediate `HoppingManager` is needed. The restrictions are\n\n    -   All files are replicated. No pre-selection, e.g. on the storage class can be done.\n\n    -   The mode of the replicated file is determined by the destination pool and cannot be overwritten. See [the section called �File mode of replicated files�](#file-mode-of-replicated-files)\n\n-   The `replicateFile` message is sent to the `HoppingManager`. The `HoppingManager` can be configured to replicate certain storage classes only and to set the mode of the replicated file according to rules. The file mode of the source file cannot be modified.\n\n### File mode of replicated files\n\nThe mode of a replicated file can either be determined by settings in the destination pool or by the `HoppingManager`. It can be `cached` or `precious`.\n\n-   If the `PoolManager` is used for replication, the mode of the replicated file is determined by the destination pool. The default setting is `cached`.\n\n-   If a `HoppingManager` is used for file replication, the mode of the replicated file is determined by the `HoppingManager` rule responsible for this particular replication. If the destination mode is set to `keep` in the rule, the mode of the destination pool determines the final mode of the replicated file.\n\n### File hopping managed by the poolmanager\n\nTo enable replication on arrival by the `PoolManager` set the property `pool.destination.replicate` to `PoolManager` for the particular pool\n\n```ini\n[exampleDomain]\n[exampleDomain/pool]\npool.destination.replicate=PoolManager\n```\n\nor for several pools in the `/etc/dcache/dcache.conf` file.\n\n```ini\npool.destination.replicate=PoolManager\n```\n\nFile hopping configuration instructs a pool to send a `replicateFile` request to the `PoolManager` as the result of a file arriving on that pool from some external client. All arriving files will be treated the same. The `PoolManager` will process this transfer request by trying to find a matching link (Please find detailed information at [Chapter 7, The poolmanager Service](config-PoolManager.md).\n\nIt is possible to configure the CELL-POOLMNGR such that files are replicated from this pool to a special set of destination pools.\n\nExample:\nAssume that we want to have all files, arriving on pool `ocean` to be immediately replicated to a subset of read pools. This subset of pools is described by the poolgroup `ocean-copies`. No other pool is member of the poolgroup `ocean-copies`.\nOther than that, files arriving at the pool `mountain` should be replicated to all read pools from which farm nodes on the `131.169.10.0/24` subnet are allowed to read.\nThe layout file defining the pools `ocean` and `mountain` should read like this:\n\n```ini\n[exampleDomain]\n[exampleDomain/pool]\nname=ocean\npath=/path/to/pool-ocean\npool.wait-for-files=${path}/data\npool.destination.replicate=PoolManager\n\n[exampleDomain/pool]\nname=mountain\npath=/path/to/pool-mountain\npool.wait-for-files=${path}/data\npool.destination.replicate=PoolManager\n```\n\nIn the layout file it is defined that all files arriving on the pools\n`ocean` or `mountain` should be replicated immediately. The following\n`PoolManager.conf` file contains instructions for the CELL-POOLMNGR\nhow to replicate these files. Files arriving at the `ocean` pool will\nbe replicated to the `ocean-copies` subset of the read pools and files\narriving at the pool `mountain` will be replicated to all read pools\nfrom which farm nodes on the 131.169.10.0/24 subnet are allowed to\nread.\n\n    #\n    # define the units\n    #\n    psu create unit -protocol   */*\n    psu create unit -net        0.0.0.0/0.0.0.0\n    psu create unit -net        131.169.10.0/255.255.255.0\n    # create the faked net unit\n    psu create unit -net        192.1.1.1/255.255.255.255\n    psu create unit -store      *@*\n    psu create unit -store      ocean:raw@osm\n    #\n    #\n    #  define unit groups\n    #\n    psu create ugroup  any-protocol\n    psu create ugroup  any-store\n    psu create ugroup  ocean-copy-store\n    psu create ugroup farm-network\n    psu create ugroup ocean-copy-network\n    #\n    psu addto ugroup any-protocol */*\n    psu addto ugroup any-store    *@*\n    psu addto ugroup ocean-copy-store ocean:raw@osm\n    psu addto ugroup farm-network  131.169.10.0/255.255.255.0\n    psu addto ugroup ocean-copy-network  192.1.1.1/255.255.255.255\n    psu addto ugroup allnet-cond 0.0.0.0/0.0.0.0\n    psu addto ugroup allnet-cond 131.169.10.0/255.255.255.0\n    psu addto ugroup allnet-cond 192.1.1.1/255.255.255.255\n    #\n    #\n    #  define the write-pools\n    #\n    psu create pool ocean\n    psu create pool mountain\n    #\n    #\n    #  define the write-pools poolgroup\n    #\n    psu create pgroup write-pools\n    psu addto pgroup write-pools ocean\n    psu addto pgroup write-pools mountain\n    #\n    #\n    #  define the write-pools-link, add write pools and set transfer preferences\n    #\n    psu create link write-pools-link any-store any-protocol allnet-cond\n    psu addto link write-pools-link write-pools\n    psu set link farm-read-link -readpref=0 -writepref=10 -cachepref=0 -p2ppref=-1\n    #\n    #\n    #  define the read-pools\n    #\n    psu create pool read-pool-1\n    psu create pool read-pool-2\n    psu create pool read-pool-3\n    psu create pool read-pool-4\n    #\n    #\n    #  define the farm-read-pools poolgroup and add pool members\n    #\n    psu create pgroup farm-read-pools\n    psu addto pgroup farm-read-pools read-pool-1\n    psu addto pgroup farm-read-pools read-pool-2\n    psu addto pgroup farm-read-pools read-pool-3\n    psu addto pgroup farm-read-pools read-pool-4\n    #\n    #\n    #  define the ocean-copy-pools poolgroup and add a pool\n    #\n    psu create pgroup ocean-copy-pools\n    psu addto pgroup ocean-copy-pools  read-pool-1\n    #\n    #\n    # define the farm-read-link, add farm-read-pools and set transfer preferences\n    #\n    psu create link farm-read-link any-store any-protocol farm-network\n    psu addto link farm-read-link farm-read-pools\n    psu set link farm-read-link -readpref=10 -writepref=0 -cachepref=10 -p2ppref=-1\n    #\n    #\n    # define the ocean-copy-link, add ocean-copy-pools and set transfer preferences\n    #\n    psu create link ocean-copy-link ocean-copy-store any-protocol ocean-copy-network\n    psu addto link ocean-copy-link ocean-copy-pools\n    psu set link ocean-copy-link -readpref=10 -writepref=0 -cachepref=10 -p2ppref=-1\n    #\n    #\n\nWhile `131.169.10.1` is a legal IP address e.g. of one of your farm nodes, the `192.1.1.1` IP address must not exist anywhere at your site.\n\n### File hopping managed by the Hoppingmanager\n\nWith the `HoppingManager` you have several configuration options for file `hopping on arrival`, e.g.:\n\n-   With the `HoppingManager` you can define a rule such that only the files with a specific storage class should be replicated.\n-   You can specify the protocol the replicated files can be accessed with.\n-   You can specify from which ip-adresses it should be possible to access the files.\n\n#### Starting the FileHopping Manager service\n\nAdd the `hoppingManager` service to a domain in your layout file and restart the domain.\n\n```ini\n[<DomainName>]\n[<DomainName>/hoppingmanager]\n```\n\nInitially no rules are configured for the HoppingManager. You may add\nrules by either edit the file\n`/var/lib/dcache/config/HoppingManager.conf` and restart the\nhoppingmanager service, or use the admin interface and `save` the\nmodifications by the save command into the `HoppingManager.conf`\n\n\n\n#### Configuring pools to use the HoppingManager\n\nTo enable replication on arrival by the CELL-HOPMNGR set the property `pool.destination.replicate` to `HoppingManager` for the particular pool\n\n```ini\n[exampleDomain]\n[exampleDomain/pool]\npool.destination.replicate=HoppingManager\n```\n\nor for several pools in the `/etc/dcache/dcache.conf` file.\n\n```ini\npool.destination.replicate=HoppingManager\n```\n\n#### HoppingManager Configuration Introduction\n\n-   The `HoppingManager` essentially receives `replicateFile` messages from pools, configured to support file hopping, and either discards or modifies and forwards them to the `PoolManager`, depending on rules described below.\n\n-   The `HoppingManager` decides on the action to perform, based on a set of configurable rules. Each rule has a name. Rules are checked in alphabetic order concerning their names.\n\n-   A rule it triggered if the storage class matches the storage class pattern assigned to that rule. If a rule is triggered, it is processed and no further rule checking is performed. If no rule is found for this request the file is not replicated.\n\n-   If for whatever reason, a file cannot be replicated, NO RETRY is being performed.\n\n-   Processing a triggered rule can be :\n\n    -   The message is discarded. No replication is done for this particular storage class.\n\n    -   The rule modifies the `replicateFile` message, before it is forwarded to the `PoolManager`.\n\n        An ip-number of a farm-node of the farm that should be allowed to read the file can be added to the `replicateFile` message.\n\n        The mode of the replicated file can be specified. This can either be `precious`, `cached` or `keep`. `keep` means that the pool mode of the source pool determines the replicated file mode.\n\n        The requested protocol can be specified.\n\n#### HoppingManager Configuration Reference\n\n    define hop OPTIONS <name> <pattern> precious|cached|keep\n    OPTIONS\n        -destination=<cellDestination> # default : PoolManager\n        -overwrite\n        -continue\n        -source=write|restore|*   #  !!!! for experts only      StorageInfoOptions\n        -host=<destinationHostIp>\n        -protType=dCap|ftp...\n        -protMinor=<minorProtocolVersion>\n        -protMajor=<majorProtocolVersion>\n\n**name**\nThis is the name of the hopping rule. Rules are checked in alphabetic order concerning their names.\n\n**pattern**\n`pattern` is a [storage class](config-PoolManager.md#storage-classes) pattern. If the incoming storage class matches this pattern, this rule is processed.\n\n**precious|cached|keep**\n`precious|cached|keep` determines the mode of the replicated file. With `keep` the mode of the file will be determined by the mode of the destination pool.\n\n**-destination**\nThis defines which `cell` to use for the pool to pool transfer. By default this is the CELL-POOLMNGR and this should not be changed.\n\n**-overwrite**\nIn case, a rule with the same name already exists, it is overwritten.\n\n**-continue**\nIf a rule has been triggered and the corresponding action has been performed, no other rules are checked. If the `continue` option is specified, rule checking continues. This is for debugging purposes only.\n\n**-source**\n`-source` defines the event on the pool which has triggered the hopping. Possible values are `restore` and `write`. `restore` means that the rule should be triggered if the file was restored from a tape and `write` means that it should be triggered if the file was written by a client.\n\n**-host**\nChoose the id of a node of the farm of worker-nodes that should be allowed to access the file. Configure the POOLMNGR respectively.\n\n**-protType, -protMajor, -protMinor**\nSpecify the protocol which should be used to access the replicated files.\n\n#### HoppingManager configuration examples\n\nIn order to instruct a particular pool to send a `replicateFile` message to the HOPPINGMANAGER service, you need to add the line `pool.destination.replicate=HoppingManager` to the layout file.\n\n```ini\n[exampleDomain]\n[exampleDomain/pool]\nname=write-pool\npath=/path/to/write-pool-exp-a\npool.wait-for-files=${path}/data\npool.destination.replicate=HoppingManager\n```\n\nAssume that all files of experiment-a will be written to an expensive write pool and subsequently flushed to tape. Now some of these files need to be accessed without delay. The files that need fast acceess possibility will be given the storage class `exp-a:need-fast-access@osm`.\n\nIn this example we will configure the file hopping such that a user who wants to access a file that has the above storage info with the NFSv4.1 protocol will be able to do so.\n\nDefine a rule for hopping in the\n`/var/lib/dcache/config/HoppingManager.conf` file.\n\n    define hop nfs-hop exp-a:need-fast-access@osm cached -protType=nfs -protMajor=4 -protMinor=1\n\nThis assumes that the storage class of the file is `exp-a:nfs@osm`. The mode of the file, which was `precious` on the write pool will have to be changed to `cached` on the read pool.\n\nThe corresponding `/var/lib/dcache/config/poolmanager.conf` file could\nread like this:\n\n    #\n    # define the units\n    #\n    psu create unit -protocol   */*\n    psu create unit -net        0.0.0.0/0.0.0.0\n    psu create unit -store      exp-a:need-fast-access@osm\n    #\n    #\n    #  define unit groups\n    #\n    psu create ugroup  any-protocol\n    psu create ugroup  exp-a-copy-store\n    psu create ugroup allnet-cond\n    #\n    psu addto ugroup any-protocol */*\n    psu addto ugroup exp-a-copy-store    exp-a:need-fast-access@osm\n    psu addto ugroup allnet-cond 0.0.0.0/0.0.0.0\n    #\n    #\n    #  define the write-pool\n    #\n    psu create pool write-pool\n    #\n    #\n    #  define the read-pool\n    #\n    psu create pool read-pool\n    #\n    #\n    #  define the exp-a-read-pools poolgroup and add a pool\n    #\n    psu create pgroup exp-a-read-pools\n    psu addto pgroup exp-a-read-pools read-pool\n    #\n    #\n    #  define the exp-a-write-pools poolgroup and add a pool\n    #\n    psu create pgroup exp-a-write-pools\n    psu addto pgroup exp-a-write-pools write-pool\n    #\n    #\n    # define the exp-a-read-link, add exp-a-read-pools and set transfer preferences\n    #\n    psu create link exp-a-read-link exp-a-copy-store any-protocol allnet-cond\n    psu addto link exp-a-read-link exp-a-read-pools\n    psu set link exp-a-read-link -readpref=10 -writepref=0 -cachepref=10 -p2ppref=-1\n    #\n    #\n    # define the exp-a-write-link, add exp-a-write-pools and set transfer preferences\n    #\n    psu create link exp-a-write-link exp-a-copy-store any-protocol allnet-cond\n    psu addto link exp-a-write-link exp-a-write-pools\n    psu set link exp-a-write-link -readpref=0 -writepref=10 -cachepref=0 -p2ppref=-1\n    #\n    #\n    #\n\n  [section\\_title]: #cf-hopping-onarrival-file-mode\n  [???]: #cf-pm\n  [storage class]: #secStorageClass\n"], "reference": "dCache is a system that allows files to be transferred between pools based on various conditions, such as client access permissions or load balancing.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is Enstore in the context of Tertiary Storage Systems?", "reference_contexts": ["Chapter 6: The dCache Tertiary Storage System Interface\n============================================\n\nOne of the features dCache provides is the ability to migrate files from its disk repository to one or more connected Tertiary Storage Systems (TSS) and to move them back to disk when necessary. Although the interface between dCache and the TSS is kept simple, dCache assumes to interact with an intelligent TSS. dCache does not drive tape robots or tape drives by itself. More detailed requirements to the storage system are described in one of the subsequent paragraphs.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Scope of this chapter\n\nThis document describes how to enable a standard dCache installation to interact with a Tertiary Storage System. In this description we assume that\n\n-   every dCache disk pool is connected to only one TSS instance.\n-   all dCache disk pools are connected to the same TSS instance.\n-   the dCache instance has not yet been populated with data, or only with a negligible amount of files.\n\nIn general, not all pools need to be configured to interact with the same Tertiary Storage System or with a storage system at all. Furthermore pools can be configured to have more than one Tertiary Storage System attached, but all those cases are not in the scope of the document.\n\n## Requirements for a tertiary storage system\n\ndCache can only drive intelligent Tertiary Storage Systems. This essentially means that tape robot and tape drive operations must be done by the TSS itself and that there is some simple way to abstract the file `PUT, GET and REMOVE` operation.\n\n### Migrating Tertiary Storage Systems with a file system interface.\n\nMost migrating storage systems provide a regular POSIX file system interface. Based on rules, data is migrated from primary to tertiary storage (mostly tape systems). Examples for migrating storage systems are:\n\n-   [HPSS](http://www.hpss-collaboration.org/)\n    (High Performance Storage System)\n-   [DMF](http://www.sgi.com/products/storage/tiered/dmf.html?)\n    (Data Migration Facility)\n\n### Tertiary Storage Systems with a minimalistic PUT, GET and REMOVE interface\n\nSome tape systems provide a simple PUT, GET, REMOVE interface. Typically, a copy-like application writes a disk file into the TSS and returns an identifier which uniquely identifies the written file within the Tertiary Storage System. The identifier is sufficient to get the file back to disk or to remove the file from the TSS. Examples are:\n\n-   [OSM](http://www.qstar.com/qstar-products/qstar-object-storage-manager)\n    (Object Storage Manager)\n-   [Enstore](http://www-ccf.fnal.gov/enstore/)\n    (FERMIlab)\n\n## How dCache interacts with a Tertiary Storage System\n\nWhenever dCache decides to copy a file from disk to tertiary storage a user-provided [executable](#example-of-an-executable-to-simulate-a-tape-backend) which can be either a script or a binary is automatically started on the pool where the file is located. That `executable` is expected to write the file into the Backend Storage System and to return a URI, uniquely identifying the file within that storage system. The format of the URI as well as the arguments to the `executable`, are described later in this document. The unique part of the URI can either be provided by the storage element, in return of the `STORE FILE` operation, or can be taken from dCache. A non-error return code from the `executable` lets dCache assume that the file has been successfully stored and, depending on the properties of the file, dCache can decide to remove the disk copy if space is running short on that pool. On a non-zero return from the `executable`, the file doesn't change its state and the operation is retried or an error flag is set on the file, depending on the error [return code](#summary-of-return-codes) from the `executable`.\n\nIf dCache needs to restore a file to disk the same `executable` is launched with a different set of arguments, including the URI, provided when the file was written to tape. It is in the responsibility of the `executable` to fetch the file back from tape based on the provided URI and to return `0` if the `FETCH FILE` operation was successful or non-zero otherwise. In case of a failure the pool retries the operation or dCache decides to fetch the file from tape using a different pool.\n\nDetails about [writing an HSM plugin](cookbook-writing-hsm-plugins.md) can be found in the cookbook section of this book.\n\n## Details on the TSS-support executable\n\n### Summary of command line options\n\n\tThis part explains the syntax of calling the EXECUTABLE that supports STOREFILE, `FETCH\n\tFILE` and REMOVEFILE operations.\n\n\tput<pnfsID><filename>-si=<storage-information> [<other-options>...]\n\n\tget <pnfsID> <filename> -si=<storage-information> -uri=<storage-uri> [<other-options>...]\n\n\tremove -uri=<storage-uri> [<other-options>...]\n\n   \t- put / get / remove: these keywords indicate the operation to be performed.\n        put: copy file from disk to TSS.\n        get: copy file back from TSS to disk.\n        remove: remove the file from TSS.\n  \t - <pnfsID>: The internal identifier (i-node) of the file within dCache. The <pnfsID> is unique within a single dCache \tinstance and globally unique with a very high probability.\n   \t- <filename>: is the full path of the local file to be copied to the TSS (for put) and respectively into which the file from the TSS should be copied (for get).\n   \t- <storage-information>: the storage information of the file, as explained below.\n  \t - <storage-uri>: the URI, which was returned by the executable, after the file was written to tertiary storage. In order to get the file back from the TSS the information of the URI is preferred over the information in the <storage-information>.\n  \t - <other-options>: -<key> = <value> pairs taken from the TSS configuration commands of the pool 'setup' file. One of the options, always provided is the option -command=<full path of this executable>.\n\n\n\n#### Storage Information\n\nThe `<storage-information>` is a string in the format\n`-si=size=bytes;new=true/false;stored=true/false;sClass=StorageClass;cClass0CacheClass;hsm=StorageType;key=value;[key=value;[...]]`\n\nHere is an example:\n\n    -si=size=1048576000;new=true;stored=false;sClass=desy:cms-sc3;cClass=-;hsm=osm;Host=desy;\n\nMandatory storage information�s keys\n\n    - <size>: Size of the file in bytes\n    - <new>: False if file already in the dCache; True otherwise\n    - <stored>: True if file already stored in the TSS; False otherwise\n    - <sClass>: HSM depended, is used by the poolmanager for pool attraction.\n    - <cClass>: Parent directory tag (cacheClass). Used by the poolmanager for pool attraction. May be '-'.\n    - <hsm>: Storage manager name (enstore/osm). Can be overwritten by the parent directory tag (hsmType).\n\n\nOSM specific storage information�s keys\n\n\t- <group>: The storage group of the file to be stored as specified in the \".(tag)(sGroup)\" tag of the parent directory of the file to be stored.\n\t- <store>: The store name of the file to be stored as specified in the \".(tag)(OSMTemplate)\" tag of the parent directory of the file to be stored.\n\t- <bfid>: Bitfile ID (get and remove only) (e.g. 000451243.2542452542.25424524)\n\nEnstore specific storage information�s keys\n\n \t- <group>: The storage group (e.g. cdf, cms ...)\n \t- <family>: The file family (e.g. sgi2test, h6nxl8, ...)\n\t - <bfid>: Bitfile ID (get only) (e.g. B0MS105746894100000)\n\t - <volume>: Tape Volume (get only) (e.g. IA6912)\n\t - <location>: Location on tape (get only) (e.g. : 0000_000000000_0000117)\n\nThere might be more key values pairs which are used by the dCache internally and which should not affect the behaviour of the   executable.\n\n#### Storage URI\n\nThe storage-uri is formatted as follows:\n\n    hsmType://hsmInstance/?store=storename&group=groupname&bfid=bfid\n\n    <hsmType>: The type of the Tertiary Storage System\n    <hsmInstance>: The name of the instance\n    <storename> and <groupname> : The store and group name of the file as provided by the arguments to this executable.\n    <bfid>: The unique identifier needed to restore or remove the file if necessary.\n\nExample:\n\nA storage-uri:\n\n    osm://osm/?store=sql&group=chimera&bfid=3434.0.994.1188400818542\n\n### Summary of return codes\n\n| Return Code         | Meaning                 | Behaviour for PUT FILE | Behaviour for GET FILE                             |\n|---------------------|-------------------------|------------------------|----------------------------------------------------|\n| 30 &lt;= rc &lt; 40 | User defined            | Deactivates request    | Reports problem to POOLMNGR                   |\n| 41                  | No space left on device | Pool retries           | Disables pool and reports problem to POOLMNGR |\n| 42                  | Disk read I/O error     | Pool retries           | Disables pool and reports problem to POOLMNGR |\n| 43                  | Disk write I/O error    | Pool retries           | Disables pool and reports problem to POOLMNGR |\n| other               | -                       | Pool retries           | Reports problem to POOLMNGR                   |\n\n### The EXECUTABLE and the STORE FILE operation\n\nWhenever a disk file needs to be copied to a Tertiary Storage System dCache automatically launches an `executable` on the pool containing the file to be copied. Exactly one instance of the `executable` is started for each file. Multiple instances of the `executable` may run concurrently for different files. The maximum number of concurrent instances of the `executable` per pool as well as the full path of the `executable` can be configured in the 'setup' file of the pool as described in [the section called �The pool �setup� file�.](#the-pool-setup-file)\n\nThe following arguments are given to the executable of a STORE FILE operation on startup.\n\n**put** pnfsID filename -si= storage-information more options\n\nDetails on the meaning of certain arguments are described in [the section called �Summary of command line options�.](#summary-of-command-line-options)\n\nWith the arguments provided the `executable` is supposed to copy the file into the Tertiary Storage System. The `executable` must not terminate before the transfer of the file was either successful or failed.\n\nSuccess must be indicated by a `0` return of the `executable`. All non-zero values are interpreted as a failure which means, dCache assumes that the file has not been copied to tape.\n\nIn case of a `0` return code the `executable` has to return a valid storage URI to dCache in formate:\n\n    hsmType://hsmInstance/?store=<storename>&group=<groupname>&bfid=<bfid>\n\nDetails on the meaning of certain parameters are described [above](#storage-uri).\n\nThe `bfid` can either be provided by the TSS as result of the STORE FILE operation or the `pnfsID` may be used. The latter assumes that the file has to be stored with exactly that `pnfsID` within the TSS. Whatever URI is chosen, it must allow to uniquely identify the file within the Tertiary Storage System.\n\n> **Note**\n>\n> Only the URI must be printed to stdout by the `executable`. Additional information printed either before or after the URI will result in an error. stderr can be used for additional debug information or error messages.\n\n### The EXECUTABLE and the FETCH FILE operation\n\nWhenever a disk file needs to be restored from a Tertiary Storage System dCache automatically launches an `executable` on the pool containing the file to be copied. Exactly one instance of the `executable` is started for each file. Multiple instances of the `executable` may run concurrently for different files. The maximum number of concurrent instances of the `executable` per pool as well as the full path of the `executable` can be configured in the 'setup' file of the pool as described in [the section called �The pool �setup� file�](#the-pool-setup-file).\n\nThe following arguments are given to the executable of a FETCH FILE operation on startup:\n\n**get**  pnfsID filename -si= storage-information-uri= storage-uri more options\n\nDetails on the meaning of certain arguments are described in [the section called �Summary of command line options�](#summary-of-command-line-options). For return codes see [the section called �Summary of return codes�](#summary-of-return-codes).\n\n### The EXECUTABLE and the REMOVE FILE operation\nWhenever a file is removed from the dCache namespace (file system) a process inside dCache makes sure that all copies of the file are removed from all internal and external media. The pool which is connected to the TSS which stores the file is activating the executable with the following command line options:\n\nremove -uri= storage-uri more options\n\nDetails on the meaning of certain arguments are described in [the section called �Summary of command line options.�](#summary-of-command-line-options) For return codes see [the section called �Summary of return codes�.](#summary-of-return-codes)\n\nThe `executable` is supposed to remove the file from the TSS and report a zero return code. If a non-zero error code is returned, the dCache will call the script again at a later point in time.\n\n## Configuring pools to interact with a Tertiary Storage System\n\nThe `executable` interacting with the Tertiary Storage System (TSS), as described in the chapter above, has to be provided to dCache on all pools connected to the TSS. The `executable`, either a script or a binary, has to be made `executable` for the user, dCache is running as, on that host.\n\nThe following files have to be modified to allow dCache to interact with the TSS.\n\n-   The `/var/lib/dcache/config/poolmanager.conf` file (one per system)\n-   The pool layout file (one per pool host)\n-   The pool 'setup' file (one per pool)\n-   The namespaceDomain layout file (one per system)\n\nAfter the layout files and the various 'setup' files have been corrected, the following domains have to be restarted :\n\n-   pool services\n-   dCacheDomain\n-   namespaceDomain\n\n### The dCache layout files\n\n#### The `/var/lib/dcache/config/poolmanager.conf` file\n\nTo be able to read a file from the tape in case the cached file has been deleted from all pools, enable the restore-option. The best way to do this is to log in to the Admin Interface and run the following commands:\n\n    [example.dcache.org] (local) admin > \\c PoolManager\n    [example.dcache.org] (PoolManager) admin > pm set -stage-allowed=yes\n    [example.dcache.org] (PoolManager) admin > save\n\n A restart of the dCacheDomain is not necessary in this case.\n\n Alternatively, if the file `/var/lib/dcache/config/poolmanager.conf` already exists then you can add the entry\n\n    pm set -stage allowed=yes\n\nand restart the DOMAIN-dCache.\n\n> **Warning**\n>\n> Do not create the file `/var/lib/dcache/config/poolmanager.conf` with this single entry! This will result in an error.\n\n#### The pool layout\n\nThe dCache layout file must be modified for each pool node connected\nto a TSS. If your pool nodes have been configured correctly to work\nwithout TSS, you will find the entry lfs=precious in the layout file\n(that is located in `/etc/dcache/layouts` and in the file\n`/etc/dcache/dcache.conf` respectively) for each pool service. This\nentry is a disk-only-option and has to be removed for each pool which\nshould be connected to a TSS. This will default the lfs parameter to\nhsm which is exactly what we need.\n\n#### The pool setup file\n\nThe pool 'setup' file is the file `$poolHomeDir/$poolName/setup`. It\nmainly defines 3 details related to TSS connectivity.\n\n-   Pointer to the `executable` which is launched on storing and fetching files.\n-   The maximum number of concurrent `STORE FILE` requests allowed per pool.\n-   The maximum number of concurrent `FETCH FILE` requests allowed per pool.\n\nDefine the `executable` and Set the maximum number of concurrent `PUT` and `GET` operations:\n\n   hsm create [-key[=value]] ... type [instance] [provider]\n\n    hsm create osm osm -hsmBase=var/pools/tape/ -hsmInstance=osm\n     -command=share/lib/hsmcp.rb -c:puts=1 -c:gets=1 -c:removes=1\n\n    #\n    #  PUT operations\n    # set the maximum number of active PUT operations >= 1\n    #\n    st set max active <numberOfConcurrentPUTS>\n\n    #\n    # GET operations\n    # set the maximum number of active GET operations >= 1\n    #\n    rh set max active `numberOfConcurrentGETs`\n\n       - <hsmType>: the type ot the TSS system. Must be set to �osm� for basic setups.\n       - <hsmInstanceName>: the instance name of the TSS system. Must be set to �osm� for basic setups.\n       - </path/to/executable>: the full path to the executable which should be launched for each TSS operation.\n\nSetting the maximum number of concurrent PUT and GET operations.\n\nBoth numbers must be non zero to allow the pool to perform transfers.\n\nExample:\n\nWe provide a\n[script](#example-of-an-executable-to-simulate-a-tape-backend) to\nsimulate a connection to a TSS. To use this script place it in the\ndirectory `/usr/share/dcache/lib`, and create a directory to simulate\nthe base of the TSS.\n\n```console-root\nmkdir -p /hsmTape/data\n```\n\nLogin to the Admin Interface to change the entry of the pool 'setup' file for a pool named pool\\_1.\n\n    (local) admin > \\c pool_1\n    (pool_1) admin > hsm set osm osm\n    (pool_1) admin > hsm set osm -command=/usr/share/dcache/lib/hsmscript.sh\n    (pool_1) admin > hsm set osm -hsmBase=/hsmTape\n    (pool_1) admin > st set timeout  5\n    (pool_1) admin > rh set timeout 5\n    (pool_1) admin > save\n\n\nAs with mover queues, the flush queue can also be set to behave as either LIFO\n(last-in-first-out) or FIFO (first-in-first-out).  This can be done statically using\nthe property:\n\n```\n    (one-of?fifo|lifo)pool.flush-controller.queue-order=fifo\n```\n\nin the setup file:\n\n```\n    flush set queue order lifo\n```\n\nor by using the admin command itself:\n\n```\n   \\s <pool-name> flush set queue order lifo\n```\n\nWhile neither queue order guarantees fairness, switching to LIFO under heavy\nqueuing where the jobs are long running may provide better throughput to\nlate-coming users.  (Default is FIFO.)\n\n\n#### The namespace layout\n\nIn order to allow dCache to remove files from attached TSSes, `cleaner-hsm` cell must be added:\n\n```ini\n[namespaceDomain]\n...\n[namespaceDomain/cleaner-hsm]\n```\n\n### What happens next\n\nAfter restarting the necessary dCache domains, pools, already containing files, will start transferring them into the TSS as those files only have a disk copy so far. The number of transfers is determined by the configuration in the pool 'setup' file as described above in [the section called �The pool �setup� file�.](#the-pool-setup-file)\n\n## How to Store-/Restore files via the Admin Interface\n\nIn order to see the state of files within a pool, login into the pool in the admin interface and run the command `rep ls`.\n\n    [example.dcache.org] (<poolname>) admin > rep ls\n\nThe output will have the following format:\n\n    PNFSID <MODE-BITS(LOCK-TIME)[OPEN-COUNT]> SIZE si={STORAGE-CLASS}\n\n-   PNFSID: The pnfsID of the file\n-   MODE-BITS:\n\n           CPCScsRDXEL\n           |||||||||||\n           ||||||||||+--  (L) File is locked (currently in use)\n           |||||||||+---  (E) File is in error state\n           ||||||||+----  (X) File is pinned (aka \"sticky\")\n           |||||||+-----  (D) File is in process of being destroyed\n           ||||||+------  (R) File is in process of being removed\n           |||||+-------  (s) File sends data to back end store\n           ||||+--------  (c) File sends data to client (DCAP,FTP...)\n           |||+---------  (S) File receives data from back end store\n           ||+----------  (C) File receives data from client (DCAP,FTP)\n           |+-----------  (P) File is precious, i.e., it is only on disk\n           +------------  (C) File is on tape and only cached on disk.\n\n-   LOCK-TIME: The number of milli-seconds this file will still be locked. Please note that this is an internal lock and not the pin-time (SRM).\n-   OPEN-COUNT: Number of clients currently reading this file.\n-   SIZE: File size\n-   STORAGE-CLASS: The storage class of this file.\n\nExample:\n\n    [example.dcache.org] (pool_1) admin > rep ls\n    00008F276A952099472FAD619548F47EF972 <-P---------L(0)[0]> 291910 si={dteam:STATIC}\n    00002A9282C2D7A147C68A327208173B81A6 <-P---------L(0)[0]> 2011264 si={dteam:STATIC}\n    0000EE298D5BF6BB4867968B88AE16BA86B0 <-C----------L(0)[0]> 1976 si={dteam:STATIC}\n\nIn order to `flush` a file to the tape run the command `flush pnfsid`.\n\n       [example.dcache.org] (<poolname>) admin > flush pnfsid <pnfsid>\n\nExample:\n   [example.dcache.org] (pool_1) admin > flush pnfsid 00002A9282C2D7A147C68A327208173B81A6\nFlush Initiated\n\nA file that has been flushed to tape gets the flag 'C'.\n\nExample:\n\n   [example.dcache.org] (pool_1) admin > rep ls\n    00008F276A952099472FAD619548F47EF972 <-P---------L(0)[0]> 291910 si={dteam:STATIC}\n    00002A9282C2D7A147C68A327208173B81A6 <C----------L(0)[0]> 2011264 si={dteam:STATIC}\n    0000EE298D5BF6BB4867968B88AE16BA86B0 <C----------L(0)[0]> 1976 si={dteam:STATIC}\n\nTo remove such a file from the repository run the command `rep rm`.\n\n   [example.dcache.org] (<poolname>) admin > rep rm <pnfsid>\n\nExample:\n\n    [example.dcache.org] (pool_1) admin > rep rm  00002A9282C2D7A147C68A327208173B81A6\nRemoved 00002A9282C2D7A147C68A327208173B81A6\n\nIn this case the file will be restored when requested.\n\nTo `restore` a file from the tape you can simply request it by initializing a reading transfer or you can fetch it by running the command `rh restore`.\n\n    [example.dcache.org] (<poolname>) admin > rh restore [-block] <pnfsid>\n\nExample:\n\n    [example.dcache.org] (pool_1) admin > rh restore 00002A9282C2D7A147C68A327208173B81A6\n    Fetch request queued\n\n## How to monitor what's going on\n\nThis section briefly describes the commands and mechanisms to monitor the TSS `PUT, GET and REMOVE` operations. dCache provides a configurable logging facility and a Command Line Admin Interface to query and manipulate transfer and waiting queues.\n\n### Log Files\n\nBy default dCache is configured to only log information if something unexpected happens. However, to get familiar with Tertiary Storage System interactions you might be interested in more details. This section provides advice on how to obtain this kind of information.\n\n#### The executable log file\n\nSince you provide the `executable`, interfacing dCache and the TSS, it is in your responsibility to ensure sufficient logging information to be able to trace possible problems with either dCache or the TSS. Each request should be printed with the full set of parameters it receives, together with a timestamp. Furthermore information returned to dCache should be reported.\n\n#### dCache log files in general\n\nIn dCache, each domain (e.g. dCacheDomain, <pool>Domain etc) prints\nlogging information into its own log file named after the domain. The\nlocation of those log files it typically the `/var/log` or\n`/var/log/dCache` directory depending on the individual\nconfiguration. In the default logging setup only errors are\nreported. This behavior can be changed by either modifying\n`/etc/dcache/logback.xml` or using the dCache CLI to increase the log\nlevel of particular components as described\n[below](#obtain-information-via-the-dcache-command-line-admin-interface).\n\n##### Increase the dCache log level by changes in `/etc/dcache/logback.xml`\n\nIf you intend to increase the log level of all components on a\nparticular host you would need to change the `/etc/dcache/logback.xml`\nfile as described below. dCache components need to be restarted to\nactivate the changes.\n\n    <threshold>\n         <appender>stdout</appender>\n         <logger>root</logger>\n         <level>warn</level>\n       </threshold>\n\nneeds to be changed to\n\n    <threshold>\n         <appender>stdout</appender>\n         <logger>root</logger>\n         <level>info</level>\n       </threshold>\n\n> **Important**\n>\n> The change might result in a significant increase in log messages. So don't forget to change back before starting production operation. The next section describes how to change the log level in a running system.\n\n##### Increase the dCache log level via the Command Line Admin Interface\n\nExample:\n\nLogin into the dCache Command Line Admin Interface and increase the log level of a particular service, for instance for the `poolmanager` service:\n\n    [example.dcache.org] (local) admin > \\c PoolManager\n    [example.dcache.org] (PoolManager) admin > log set stdout ROOT INFO\n    [example.dcache.org] (PoolManager) admin > log ls\n    stdout:\n     ROOT=INFO\n     dmg.cells.nucleus=WARN*\n     logger.org.dcache.cells.messages=ERROR*\n     .....\n\n### Obtain information via the dCache Command Line Admin Interface\n\nThe dCache Command Line Admin Interface gives access to information describing the process of storing and fetching files to and from the TSS, as there are:\n\n-   The\n    *Pool Manager Restore Queue*. A list of all requests which have been issued to all pools for a `FETCH FILE` operation from the TSS (rc ls)\n-   The *Pool Collector Queue*. A list of files, per pool and storage group, which will be scheduled for a `STORE FILE` operation as soon as the configured trigger criteria match.\n-   The *Pool STORE FILE*  Queue. A list of files per pool, scheduled for the `STORE FILE` operation. A configurable amount of requests within this queue are active, which is equivalent to the number of concurrent store processes, the rest is inactive, waiting to become active.\n-   The Pool *FETCH FILE* Queue. A list of files per pool, scheduled for the `FETCH FILE` operation. A configurable amount of requests within this queue are active, which is equivalent to the number of concurrent fetch processes, the rest is inactive, waiting to become active.\n\nFor evaluation purposes, the *pinboard* of each component can be used to track down dCache behavior. The *pinboard* only keeps the most recent 200 lines of log information but reports not only errors but informational messages as well.\n\nCheck the pinboard of a service, here the POOLMNGR service.\n\nExample:\n\n    [example.dcache.org] (local) admin > \\c PoolManager\n    [example.dcache.org] (PoolManager) admin > show pinboard 100\n    08.30.45  [Thread-7] [pool_1 PoolManagerPoolUp] sendPoolStatusRelay: ...\n    08.30.59  [writeHandler] [NFSv41-dcachetogo PoolMgrSelectWritePool ...\n    ....\n\nExample:\n\nThe **PoolManager** Restore Queue.  Remove the file `test.root` with the pnfs-ID 00002A9282C2D7A147C68A327208173B81A6.\n\n    [example.dcache.org] (pool_1) admin > rep rm  00002A9282C2D7A147C68A327208173B81A6\n\nRequest the file `test.root`\n\n```console-user\ndccp dcap://example.dcache.org:/data/test.root test.root\n```\n\nCheck the PoolManager Restore Queue:\n\n    [example.dcache.org] (local) admin > \\c PoolManager\n    [example.dcache.org] (PoolManager) admin > rc ls\n    0000AB1260F474554142BA976D0ADAF78C6C@0.0.0.0/0.0.0.0-*/* m=1 r=0 [pool_1] [Staging 08.15 17:52:16] {0,}\n\nExample:\n\n**The Pool Collector Queue.**\n\n  [example.dcache.org] (local) admin > \\c pool_1\n  [example.dcache.org] (pool_1) admin > queue ls -l queue\n                       Name: chimera:alpha\n                  Class@Hsm: chimera:alpha@osm\n     Expiration rest/defined: -39 / 0   seconds\n     Pending   rest/defined: 1 / 0\n     Size      rest/defined: 877480 / 0\n     Active Store Procs.   :  0\n      00001BC6D76570A74534969FD72220C31D5D\n\n\n    [example.dcache.org] (local) admin > \\c pool_1\n    Class                 Active   Error  Last/min  Requests    Failed\n    dteam:STATIC@osm           0       0         0         1         0\n\nExample:\n\n**The pool STORE FILE Queue.**\n\n    [example.dcache.org] (local) admin > \\c pool_1\n    [example.dcache.org] (pool_1) admin > st ls\n    0000EC3A4BFCA8E14755AE4E3B5639B155F9  1   Fri Aug 12 15:35:58 CEST 2011\n\nExample:\n\n**The pool FETCH FILE Queue.**\n\n    [example.dcache.org] (local) admin > \\c pool_1\n    [example.dcache.org] (pool_1) admin >  rh ls\n    0000B56B7AFE71C14BDA9426BBF1384CA4B0  0   Fri Aug 12 15:38:33 CEST 2011\n\nTo check the repository on the pools run the command `rep ls` that is described in the beginning of [the section called �How to Store-/Restore files via the Admin Interface�.](#how-to-store-restore-files-via-the-admin-interface)\n\n\n# Example of an EXECUTABLE to simulate a tape backend\n\n```shell\n#!/bin/sh\n#\n#set -x\n#\nlogFile=/tmp/hsm.log\n#\n################################################################\n#\n#  Some helper functions\n#\n##.........................................\n#\n# print usage\n#\nusage() {\n   echo \"Usage : put|get <pnfsId> <filePath> [-si=<storageInfo>] [-key[=value] ...]\" 1>&2\n}\n##.........................................\n#\n#\nprintout() {\n#---------\n   echo \"$pnfsid : $1\" >>${logFile}\n   return 0\n}\n##.........................................\n#\n#  print error into log file and to stdout.\n#\nprinterror() {\n#---------\nif [ -z \"$pnfsid\" ] ; then\n#      pp=\"000000000000000000000000000000000000\"\n      pp=\"------------------------------------\"\n   else\n      pp=$pnfsid\n   fi\n\n   echo \"$pp : (E) : $*\" >>${logFile}\n   echo \"$pp : $*\" 1>&2\n\n}\n##.........................................\n#\n#  find a key in the storage info\n#\nfindKeyInStorageInfo() {\n#-------------------\n\n   result=`echo $si  | awk  -v hallo=$1 -F\\; '{ for(i=1;i<=NF;i++){ split($i,a,\"=\") ; if( a[1] == hallo )print a[2]} }'`\n   if [ -z \"$result\" ] ; then return 1 ; fi\n   echo $result\n   exit 0\n\n}\n##.........................................\n#\n#  find a key in the storage info\n#\nprintStorageInfo() {\n#-------------------\n   printout \"storageinfo.StoreName : $storeName\"\n   printout \"storageinfo.store : $store\"\n   printout \"storageinfo.group : $group\"\n   printout \"storageinfo.hsm   : $hsmName\"\n   printout \"storageinfo.accessLatency   : $accessLatency\"\n   printout \"storageinfo.retentionPolicy : $retentionPolicy\"\n   return 0\n}\n##.........................................\n#\n#  assign storage info the keywords\n#\nassignStorageInfo() {\n#-------------------\n\n    store=`findKeyInStorageInfo \"store\"`\n    group=`findKeyInStorageInfo \"group\"`\n    storeName=`findKeyInStorageInfo \"StoreName\"`\n    hsmName=`findKeyInStorageInfo \"hsm\"`\n    accessLatency=`findKeyInStorageInfo \"accessLatency\"`\n    retentionPolicy=`findKeyInStorageInfo \"retentionPolicy\"`\n    return 0\n}\n##.........................................\n#\n# split the arguments into the options -<key>=<value> and the\n# positional arguments.\n#\nsplitArguments() {\n#----------------\n#\n  args=\"\"\n  while [ $# -gt 0 ] ; do\n    if expr \"$1\" : \"-.*\" >/dev/null ; then\n       a=`expr \"$1\" : \"-\\(.*\\)\" 2>/dev/null`\n       key=`echo \"$a\" | awk -F= '{print $1}' 2>/dev/null`\n       value=`echo \"$a\" | awk -F= '{for(i=2;i<NF;i++)x=x $i \"=\" ; x=x $NF ; print x }' 2>/dev/null`\n       if [ -z \"$value\" ] ; then a=\"${key}=\" ; fi\n       eval \"${key}=\\\"${value}\\\"\"\n       a=\"export ${key}\"\n       eval \"$a\"\n    else\n       args=\"${args} $1\"\n    fi\n    shift 1\n  done\n  if [ ! -z \"$args\" ] ; then\n     set `echo \"$args\" | awk '{ for(i=1;i<=NF;i++)print $i }'`\n  fi\n  return 0\n}\n#\n#\n##.........................................\n#\nsplitUri() {\n#----------------\n#\n  uri_hsmName=`expr \"$1\" : \"\\(.*\\)\\:.*\"`\n  uri_hsmInstance=`expr \"$1\" : \".*\\:\\/\\/\\(.*\\)\\/.*\"`\n  uri_store=`expr \"$1\" : \".*\\/\\?store=\\(.*\\)&group.*\"`\n  uri_group=`expr \"$1\" : \".*group=\\(.*\\)&bfid.*\"`\n  uri_bfid=`expr \"$1\" : \".*bfid=\\(.*\\)\"`\n#\n  if [  \\( -z \"${uri_store}\" \\) -o \\( -z \"${uri_group}\" \\) -o \\(  -z \"${uri_bfid}\" \\) \\\n     -o \\( -z \"${uri_hsmName}\" \\) -o \\( -z \"${uri_hsmInstance}\" \\) ] ; then\n     printerror \"Illegal URI formal : $1\"\n     return 1\n  fi\n  return 0\n\n}\n#########################################################\n#\necho \"--------- $* `date`\" >>${logFile}\n#\n#########################################################\n#\ncreateEnvironment() {\n\n   if [ -z \"${hsmBase}\" ] ; then\n      printerror \"hsmBase not set, can't continue\"\n      return 1\n   fi\n   BASE=${hsmBase}/data\n   if [ ! -d ${BASE} ] ; then\n      printerror \"${BASE} is not a directory or doesn't exist\"\n      return 1\n   fi\n}\n##\n#----------------------------------------------------------\ndoTheGetFile() {\n\n   splitUri $1\n   [ $? -ne 0 ] && return 1\n\n   createEnvironment\n   [ $? -ne 0 ] && return 1\n\n   pnfsdir=${BASE}/$uri_hsmName/${uri_store}/${uri_group}\n   pnfsfile=${pnfsdir}/$pnfsid\n\n   cp $pnfsfile $filename 2>/dev/null\n   if [ $? -ne 0 ] ; then\n      printerror \"Couldn't copy file $pnfsfile to $filename\"\n      return 1\n   fi\n\n   return 0\n}\n##\n#----------------------------------------------------------\ndoTheStoreFile() {\n\n   splitUri $1\n   [ $? -ne 0 ] && return 1\n\n   createEnvironment\n   [ $? -ne 0 ] && return 1\n\n   pnfsdir=${BASE}/$hsmName/${store}/${group}\n   mkdir -p ${pnfsdir} 2>/dev/null\n   if [ $? -ne 0 ] ; then\n      printerror \"Couldn't create $pnfsdir\"\n      return 1\n   fi\n   pnfsfile=${pnfsdir}/$pnfsid\n\n   cp $filename $pnfsfile 2>/dev/null\n   if [ $? -ne 0 ] ; then\n      printerror \"Couldn't copy file $filename to $pnfsfile\"\n      return 1\n   fi\n\n   return 0\n\n}\n##\n#----------------------------------------------------------\ndoTheRemoveFile() {\n\n   splitUri $1\n   [ $? -ne 0 ] && return 1\n\n   createEnvironment\n   [ $? -ne 0 ] && return 1\n\n   pnfsdir=${BASE}/$uri_hsmName/${uri_store}/${uri_group}\n   pnfsfile=${pnfsdir}/$uri_bfid\n\n   rm $pnfsfile 2>/dev/null\n   if [ $? -ne 0 ] ; then\n      printerror \"Couldn't remove file $pnfsfile\"\n      return 1\n   fi\n\n   return 0\n}\n#########################################################\n#\n#  split arguments\n#\n  args=\"\"\n  while [ $# -gt 0 ] ; do\n    if expr \"$1\" : \"-.*\" >/dev/null ; then\n       a=`expr \"$1\" : \"-\\(.*\\)\" 2>/dev/null`\n       key=`echo \"$a\" | awk -F= '{print $1}' 2>/dev/null`\n         value=`echo \"$a\" | awk -F= '{for(i=2;i<NF;i++)x=x $i \"=\" ; x=x $NF ; print x }' 2>/dev/null`\n       if [ -z \"$value\" ] ; then a=\"${key}=\" ; fi\n       eval \"${key}=\\\"${value}\\\"\"\n       a=\"export ${key}\"\n       eval \"$a\"\n    else\n       args=\"${args} $1\"\n    fi\n    shift 1\n  done\n  if [ ! -z \"$args\" ] ; then\n     set `echo \"$args\" | awk '{ for(i=1;i<=NF;i++)print $i }'`\n  fi\n#\n#\nif [ $# -lt 1 ] ; then\n    printerror \"Not enough arguments : ... put/get/remove ...\"\n    exit 1\nfi\n#\ncommand=$1\npnfsid=$2\n#\n# !!!!!!  Hides a bug in the dCache HSM remove\n#\nif [ \"$command\" = \"remove\" ] ; then pnfsid=\"000000000000000000000000000000000000\" ; fi\n#\n#\nprintout \"Request for $command started `date`\"\n#\n################################################################\n#\nif [ \"$command\" = \"put\" ] ; then\n#\n################################################################\n#\n  filename=$3\n#\n  if [ -z \"$si\" ] ; then\n     printerror \"StorageInfo (si) not found in put command\"\n     exit 5\n  fi\n#\n  assignStorageInfo\n#\n  printStorageInfo\n#\n  if [ \\( -z \"${store}\" \\) -o \\( -z \"${group}\" \\) -o \\( -z \"${hsmName}\" \\) ] ; then\n     printerror \"Didn't get enough information to flush : hsmName = $hsmName store=$store group=$group pnfsid=$pnfsid \"\n     exit 3\n  fi\n#\n  uri=\"$hsmName://$hsmName/?store=${store}&group=${group}&bfid=${pnfsid}\"\n\n  printout \"Created identifier : $uri\"\n\n  doTheStoreFile $uri\n  rc=$?\n  if [ $rc -eq 0 ] ; then echo $uri ; fi\n\n  printout \"Request 'put' finished at `date` with return code $rc\"\n  exit $rc\n#\n#\n################################################################\n#\nelif [ \"$command\" = \"get\"  ] ; then\n#\n################################################################\n#\n  filename=$3\n  if [ -z \"$uri\" ] ; then\n     printerror \"Uri not found in arguments\"\n     exit 3\n  fi\n#\n  printout \"Got identifier : $uri\"\n#\n  doTheGetFile $uri\n  rc=$?\n  printout \"Request 'get' finished at `date` with return code $rc\"\n  exit $rc\n#https://onedio.com/haber/abd-yi-karistiran-kendall-jenner-li-pepsi-reklami-765067\n################################################################\n#\nelif [ \"$command\" = \"remove\" ] ; then\n#\n################################################################\n#\n   if [ -z \"$uri\" ] ; then\n      printerror \"Illegal Argument error : URI not specified\"\n      exit 4\n   fi\n#\n   printout \"Remove uri = $uri\"\n   doTheRemoveFile $uri\n   rc=$?\n#\n   printout \"Request 'remove' finished at `date` with return code $rc\"\n   exit $rc\n#\nelse\n#\n   printerror \"Expected command : put/get/remove , found : $command\"\n   exit 1\n#\nfi\n```\n\n  [EXECUTABLE]: #tss-executable\n  [return code]: #cf-tss-support-return-codes\n  [section\\_title]: #cf-tss-pools-layout-setup\n  [1]: #cf-tss-support-clo\n  [above]: #cf-tss-support-storage-uri\n  [below]: #cf-tss-monitor-log-cli\n  [2]: #cf-tss-pools-admin\n"], "reference": "Enstore is an example of a Tertiary Storage System that provides a simple PUT, GET, and REMOVE interface for file operations. It is used to manage files stored on tape systems.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How should the placeholder value 'EXAMPLESITE-ID' be modified in the info-provider.xml file for proper configuration?", "reference_contexts": ["CHAPTER 20. GLUE INFO PROVIDER\n==============================\n\nThe GLUE information provider supplied with dCache provides the information about the dCache instance in a standard format called GLUE. This is necessary so that WLCG infrastructure (such as FTS) and clients using WLCG tools can discover the dCache instance and use it correctly.\n\nThe process of configuring the info-provider is designed to have the minimum overhead so you can configure it manually; however, you may prefer to use an automatic configuration tool, such as YAIM.\n\n> **NOTE**\n>\n> Be sure you have at least v2.0.8 of glue-schema RPM installed on the node running the info-provider.\n\nThis chapter describes how to enable and test the dCache-internal collection of information needed by the info-provider. It also describes how to configure the info-provider and verify that it is working correctly. Finally, it describes how to publish this information within BDII, verify that this is working and troubleshoot any problems.\n\n> **WARNING**\n>\n> Please be aware that changing information provider may result in a brief interruption to published information. This may have an adverse affect on client software that make use of this information.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Internal collection of information\n\nThe info-provider takes as much information as possible from dCache. To achieve this, it needs the internal information-collecting service, `info`, to be running and a means to collect that information: `httpd`. Make sure that both the `httpd` and `info` services are running within your dCache instance. By default, the `info` service is started on the admin-node; but it is possible to configure dCache so it runs on a different node. You should run only one `info` service per dCache instance.\n\nThe traditional (pre-1.9.7) allocation of services to domains has the `info` cell running in the `infoDomain` domain. A dCache system that has been migrated from this old configuration will have the following fragment in the node's layout file:\n\n```ini\n[infoDomain]\n[infoDomain/info]\n```\n\nIt is also possible to run the `info` service inside a domain that runs other services. The following example show the N domain that hosts the `admin, httpd, topo`  and `info` services.\n\n```ini\n[information]\n[information/admin]\n[information/httpd]\n[information/topo]\n[information/info]\n```\n\nFor more information on configuring dCache layout files, see [the section called �Defining domains and services�](install.md#defining-domains-and-services).\n\nUse the `dcache services` command to see if a particular node is configured to run the `info` service. The following shows the output if the node has an `information`  domain that is configured to run the `info`  cell.\n\n```console-root\ndcache services | grep info\n|information info info /var/log/dCache/information.log\n```\n\nIf a node has no domain configured to host the CELL-INFO service then the above `dcache services` command will give no output:\n\n```console-root\ndcache services | grep info\n```\n\nIf no running domain within *any* node of your dCache instance is running the `info` service then you must add the service to a domain and restart that domain.\n\nExample:\nIn this example, the `info` service is added to the `example` domain. Note that the specific choice of domain (example) is just to give a concrete `example`; the same process may be applied to a different domain.\n\nThe layouts file for this node includes the following definition for the `example` domain:\n\n```ini\n[example]\n[example/admin]\n[example/httpd]\n[example/topo]\n```\n\nBy adding the extra line [example/info] to the layouts file, in future, the example domain will host the info service.\n\n```ini\n[example]\n[example/admin]\n[example/httpd]\n[example/topo]\n[example/info]\n```\n\nTo actually start the CELL-INFO cell, the DOMAIN-EXAMPLE domain must be restarted.\n\n```console-root\ndcache restart example\n|Stopping example (pid=30471) 0 done\n|Starting example done\n```\n\nWith the `example`domain restarted, the `info` service is now running.\n\nYou can also verify both the `httpd` and `info` services are running using the `wget` command. The specific command assumes that you are logged into the node that has the `httpd` service (by default, the admin node). You may run the command on any node by replacing localhost with the hostname of the node running the `httpd` service.\n\nThe following example shows the output from the `wget` when the `info` service is running correctly:\n\n```console-root\nwget -O/dev/null http://localhost:2288/info\n|--17:57:38--  http://localhost:2288/info\n|Resolving localhost... 127.0.0.1\n|Connecting to localhost|127.0.0.1|:2288... connected.\n|HTTP request sent, awaiting response... 200 Document follows\n|Length: 372962 (364K) [application/xml]\n|Saving to: `/dev/null'\n|\n|100%[==============================================================================>] 372,962 --.-K/s in 0.001s\n|\n|17:57:38 (346 MB/s) - `/dev/null' saved [372962/372962]\n```\n\nIf the `httpd` service isn't running then the command will generate\nthe following output:\n\n```console-root\nwget -O/dev/null http://localhost:2288/info\n|--10:05:35--  http://localhost:2288/info\n|            => `/dev/null'\n|Resolving localhost... 127.0.0.1\n|Connecting to localhost|127.0.0.1|:2288... failed: Connection refused.\n```\n\nTo fix the problem, ensure that the `httpd` service is running within your dCache instance. This is the service that provides the web server monitoring within dCache. To enable the service, follow the same procedure for enabling the `info` cell, but add the `httpd` service within one of the domains in dCache.\n\nIf running the `wget` command gives an error message with `Unable to contact the info cell. Please ensure the info cell is running`:\n\n```console-root\nwget -O/dev/null http://localhost:2288/info\n|--10:03:13--  http://localhost:2288/info\n|            => `/dev/null'\n| Resolving localhost... 127.0.0.1\n| Connecting to localhost|127.0.0.1|:2288... connected.\n| HTTP request sent, awaiting response... 503 Unable to contact the info\n| cell.  Please ensure the info cell is running.\n| 10:03:13 ERROR 503: Unable to contact the info cell.  Please ensure\n| the info cell is running..\n```\n\nThis means that the `info` service is not running. Follow the instructions for starting the `info` service given above.\n\n## Configuring the info provider\n\nIn the directory `/etc/dcache` you will find the file\n`info-provider.xml`. This file is where you configure the\ninfo-provider. It provides information that is difficult or impossible\nto obtain from the running dCache directly.\n\nYou must edit the `info-provider.xml` to customise its content to\nmatch your dCache instance. In some places, the file contains\nplace-holder values. These place-holder values must be changed to the\ncorrect values for your dCache instance.\n\n\n> **CAREFUL WITH < AND & CHARATERS**\n>\n> Take care when editing the `info-provider.xml` file! After changing\n> the contents, the file must remain valid, well-formed XML. In\n> particular, be very careful when writing a less-than symbol (`<`) or\n> an ampersand symbol (`&`).\n>\n> -   Only use an ampersand symbol (`&`) if it is part of an entity reference. An entity reference is a sequence that starts with an ampersand symbol and is terminated with a semi-colon (`;`), for example `&gt;` and `&apos;` are entity markups.\n>\n>     If you want to include an ampersand character in the text then you must use the `&amp;` entity; for example, to include the text �me & you� the XML file would include `me\n>     \t    &amp; you`.\n>\n> -   Only use a less-than symbol (`<`) when starting an XML element; for example, `<constant id=\"TEST\">A test\n>     \t    value</constant>`.\n>\n>     If you want to include a less-than character in the text then you must use the `&lt;` entity; for example, to include the text �1 &lt; 2� the XML file would include `1 &lt;\n>     \t    2`.\n> Example:\n> The following example shows the `SE-NAME` constant (which provides a human-readable description of the dCache instance) from a well-formed `info-provider.xml` configuration file:\n>\n>     <constant id=\"SE-NAME\">Simple &amp; small dCache instance for small VOs\n>     (typically &lt; 20 users)</constant>\n>\n> The `SE-NAME` constant is configured to have the value �Simple & small dCache instance for small VOs (typically &lt; 20 users)�. This illustrates how to include ampersand and less-than characters in an XML file.\n\nWhen editing the `info-provider.xml` file, you should *only* edit text\nbetween two elements or add more elements (for lists and\nmappings). You should *never* alter the text inside double-quote\nmarks.\n\nExample:\nThis example shows how to edit the `SITE-UNIQUE-ID` constant. This constant has a default value `EXAMPLESITE-ID`, which is a place-holder value and must be edited.\n\n    <constant id=\"SITE-UNIQUE-ID\">EXAMPLESITE-ID</constant>\n\nTo edit the constant's value, you must change the text between the start- and end-element tags: `EXAMPLESITE-ID`. You *should not* edit the text `SITE-UNIQUE-ID` as it is in double-quote marks. After editing, the file may read:\n\n    <constant id=\"SITE-UNIQUE-ID\">DESY-HH</constant>\n\nThe `info-provider.xml` contains detailed descriptions of all the\nproperties that are editable. You should refer to this documentation\nwhen editing the `info-provider.xml`.\n\n## Testing the info provider\n\nOnce you have configured `info-provider.xml` to reflect your site's\nconfiguration, you may test that the info provider produces meaningful\nresults.\n\nRunning the info-provider script should produce GLUE information in LDIF format; for example:\n\n```console-root\ndcache-info-provider | head -20\n|#\n|#  LDIF generated by Xylophone v0.2\n|#\n|#  XSLT processing using SAXON 6.5.5 from Michael Kay 1 (http://saxon.sf.net/)\n|#   at: 2011-05-11T14:08:45+02:00\n|#\n|\n|dn: GlueSEUniqueID=EXAMPLE-FQDN,mds-vo-name=resource,o=grid\n|objectClass: GlueSETop\n|objectClass: GlueSE\n|objectClass: GlueKey\n|objectClass: GlueSchemaVersion\n|GlueSEStatus: Production\n|GlueSEUniqueID: EXAMPLE-FQDN\n|GlueSEImplementationName: dCache\n|GlueSEArchitecture: multidisk\n|GlueSEImplementationVersion: dCache-##VERSION## (ns=Chimera)\n|GlueSESizeTotal: 86\n```\n\nThe actual values you see will be site-specific and depend on the\ncontents of the `info-provider.xml` file and your dCache\nconfiguration.\n\nTo verify that there are no problems, redirect standard-out to\n`/dev/null` to show only the error messages:\n\n```console-root\ndcache-info-provider >/dev/null\n```\n\nIf you see error messages (which may be repeated several times) of the form:\n\n```console-root\ndcache-info-provider >/dev/null\n|Recoverable error\n|Failure reading http://localhost:2288/info: no more input\n```\n\nthen it is likely that either the `httpd` or `info` service has not been started. Use the above `wget` test to check that both services are running. You can also see which services are available by running the `dcache services` and `dcache status` commands.\n\n## Publishing dCache information\n\nBDII obtains information by querying different sources. One such\nsource of information is by running an info-provider command and\ntaking the resulting LDIF output. To allow BDII to obtain dCache\ninformation, you must allow BDII to run the dCache info-provider. This\nis achieved by symbolically linking the `dcache-info-provider` script\ninto the BDII plugins directory:\n\n```console-root\nln -s /usr/sbin/dcache-info-provider \\\n|/opt/glite/etc/gip/provider/\n```\n\nIf the BDII daemons are running, then you will see the information appear in BDII after a short delay; by default this is (at most) 60 seconds.\n\nYou can verify that information is present in BDII by querying BDII using the `ldapsearch` command. Here is an example that queries for GLUE v1.3 objects:\n\n```console-root\nldapsearch -LLL -x -H ldap://EXAMPLE-HOST:2170 -b o=grid \\\n|'(objectClass=GlueSE)'\n|dn: GlueSEUniqueID=EXAMPLE-FQDN,Mds-Vo-name=resource,o=grid\n|GlueSEStatus: Production\n|objectClass: GlueSETop\n|objectClass: GlueSE\n|objectClass: GlueKey\n|objectClass: GlueSchemaVersion\n|GlueSETotalNearlineSize: 0\n|GlueSEArchitecture: multidisk\n|GlueSchemaVersionMinor: 3\n|GlueSEUsedNearlineSize: 0\n|GlueChunkKey: GlueSEUniqueID=EXAMPLE-FQDN\n|GlueForeignKey: GlueSiteUniqueID=example.org\n|GlueSchemaVersionMajor: 1\n|GlueSEImplementationName: dCache\n|GlueSEUniqueID: EXAMPLE-FQDN\n|GlueSEImplementationVersion: dCache-##VERSION## (ns=Chimera)\n|GlueSESizeFree: 84\n|GlueSEUsedOnlineSize: 2\n|GlueSETotalOnlineSize: 86\n|GlueSESizeTotal: 86\n```\n\n> **CAREFUL WITH THE HOSTNAME**\n>\n> You must replace EXAMPLE-HOST in the URI <ldap://EXAMPLE-HOST:2170/> with the actual hostname of your node.\n>\n> It's tempting to use HOST-LOCALHOST in the URI when executing the `ldapsearch` command; however, BDII binds to the ethernet device (e.g., eth0). Typically, HOST-LOCALHOST is associated with the loopback device (lo), so querying BDII with the URI <ldap://localhost:2170/> will fail.\n\nThe LDAP query uses the `o=grid` object as the base; all reported objects are descendant objects of this base object. The `o=grid` base selects only the GLUE v1.3 objects. To see GLUE v2.0 objects, the base object must be `o=glue`.\n\nThe above `ldapsearch` command queries BDII using the `(objectClass=GlueSE)` filter. This filter selects only objects that provide the highest-level summary information about a storage-element. Since each storage-element has only one such object and this BDII instance only describes a single dCache instance, the command returns only the single LDAP object.\n\nTo see all GLUE v1.3 objects in BDII, repeat the above `ldapsearch`\ncommand but omit the `(objectClass=GlueSE)` filter: `ldapsearch -LLL\n-x -H ldap://EXAMPLE-HOST:2170 -b o=grid`. This command will output\nall GLUE v1.3 LDAP objects, which includes all the GLUE v1.3 objects\nfrom the info-provider.\n\nSearching for all GLUE v2.0 objects in BDII is achieved by repeating\nthe above `ldapsearch` command but omitting the `(objectClass=GlueSE)`\nfilter and changing the search base to `o=glue`: `ldapsearch -LLL -x\n-H ldap://EXAMPLE-HOST:2170 -b o=glue`. This command returns a\ncompletely different set of objects from the GLUE v1.3 queries.\n\nYou should be able to compare this output with the output from running the info-provider script manually: BDII should contain all the objects that the dCache info-provider is supplying. Unfortunately, the order in which the objects are returned and the order of an object's properties is not guaranteed; therefore a direct comparison of the output isn't possible. However, it is possible to calculate the number of objects in GLUE v1.3 and GLUE v2.0.\n\nFirst, calculate the number of GLUE v1.3 objects in BDII and compare\nthat to the number of GLUE v1.3 objects that the info-provider\nsupplies.\n\n```console-root\nldapsearch -LLL -x -H ldap://<dcache-host>:2170 -b o=grid \\\n|'(objectClass=GlueSchemaVersion)' | grep ^dn | wc -l\n|10\ndcache-info-provider | grep -i \"objectClass: GlueSchemaVersion\" | wc -l\n|10\n```\n\nNow calculate the number of GLUE v2.0 objects in BDII describing your\ndCache instance and compare that to the number provided by the\ninfo-provider:\n\n```console-root\nldapsearch -LLL -x -H ldap://<dcache-host>:2170 -b o=glue | perl -p00e 's/\\n //g' | \\\n|grep dn.*GLUE2ServiceID | wc -l\n|27\ndcache-info-provider | perl -p00e 's/\\n //g' | \\\n|grep ^dn.*GLUE2ServiceID | wc -l\n|27\n```\n\nIf there is a discrepancy in the pair of numbers obtains in the above commands then BDII has rejecting some of the objects. This is likely due to malformed LDAP objects from the info-provider.\n\n## Troubleshooting BDII problems\n\nThe BDII log file should explain why objects are not accepted; for\nexample, due to a badly formatted attribute. The default location of\nthe log file is `/var/log/bdii/bdii-update.log`, but the location is\nconfigured by the `BDII_LOG_FILE` option in the\n`/opt/bdii/etc/bdii.conf` file.\n\nThe BDII log files may show entries like:\n\n    2011-05-11 04:04:58,711: [WARNING] dn: o=shadow\n    2011-05-11 04:04:58,711: [WARNING] ldapadd: Invalid syntax (21)\n    2011-05-11 04:04:58,711: [WARNING] additional info: objectclass: value #1 invalid per syntax\n\nThis problem comes when BDII is attempting to inject new\ninformation. Unfortunately, the information isn't detailed enough for\nfurther investigation. To obtain more detailed information from BDII,\nswitch the `BDII_LOG_LEVEL` option in `/opt/bdii/etc/bdii.conf` to\n`DEBUG`. This will provide more information in the BDII log file.\n\nLogging at `DEBUG` level has another effect; BDII no longer deletes\nsome temporary files. These temporary files are located in the\ndirectory controlled by the `BDII_VAR_DIR` option. This is\n`/var/run/bdii` by default.\n\nThere are several temporary files located in the `/var/run/bdii`\ndirectory. When BDII decides which objects to add, modify and remove,\nit creates LDIF instructions inside temporary files `add.ldif`,\n`modify.ldif` and `delete.ldif` respectively. Any problems in the\nattempt to add, modify and delete LDAP objects are logged to\ncorresponding error files: errors with `add.ldif` are logged to\n`add.err`, `modify.ldif` to `modify.err` and so on.\n\nOnce information in BDII has stablised, the only new, incoming objects\nfor BDII come from those objects that it was unable to add\npreviously. This means that `add.ldif` will contain these badly\nformatted objects and `add.err` will contain the corresponding errors.\n\n## Updating information\n\nThe information contained within the `info` service may take a short time to achieve a complete overview of dCache's state. For certain gathered information it may take a few minutes before the information stabilises. This delay is intentional and prevents the gathering of information from adversely affecting dCache's performance.\n\nThe information presented by the LDAP server is updated periodically by BDII requesting fresh information from the info-provider. The info-provider obtains this information by requesting dCache's current status from CELL-INFO service. By default, BDII will query the info-provider every 60 seconds. This will introduce an additional delay between a change in dCache's state and that information propagating.\n\nSome information is hard-coded within the `info-provider.xml` file;\nthat is, you will need to edit this file before the published value(s)\nwill change. These values are ones that typically a site-admin must\nchoose independently of dCache's current operations.\n\n  [???]: #in-install-layout\n"], "reference": "The placeholder value 'EXAMPLESITE-ID' in the info-provider.xml file must be changed to a specific identifier that reflects your dCache instance. For example, you would edit the line containing <constant id=\"SITE-UNIQUE-ID\">EXAMPLESITE-ID</constant> to <constant id=\"SITE-UNIQUE-ID\">DESY-HH</constant>, ensuring that you only modify the text between the start- and end-element tags and do not alter the text inside double-quote marks.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "How does dCache facilitate communication between its cells?", "reference_contexts": ["Message passing\n===============\n\n-----\n[TOC bullet hierarchy]\n-----\n\nThe dCache system is divided into cells which communicate with each other via messages. Cells run inside domains and cells communicate by passing messages to each other. Domains are connected through cell tunnels which exchange messages over TCP.\n\nEach domain runs in a separate Java virtual machine and each cell is run as a separate thread therein. Domain names have to be unique. The domains communicate with each other via `TCP` using connections that are established at start-up. The topology is controlled by the location manager service. When configured, all domains connect with a core domain, which routes all messages to the appropriate domains. This forms a star topology.\n\n> **ONLY FOR MESSAGE COMMUNICATION**\n>\n> The `TCP` communication controlled by the location manager service is for the short control messages sent between cells. Any transfer of the data stored within dCache does not use these connections; instead, dedicated `TCP` connections are established as needed.\n\nWithin this framework, cells send messages to other cells addressing them in the form cellName@domainName. This way, cells can communicate without knowledge about the host they run on. Some cells are [well known](rf-glossary.md#well-known-cell), i.e. they can be addressed just by their name without @domainName. Evidently, this can only work properly if the name of the cell is unique throughout the whole system. If two well known cells with the same name are present, the system will behave in an undefined way. Therefore it is wise to take care when starting, naming, or renaming the well known cells. In particular this is true for pools, which are well known cells.\n\nA domain is started with a shell script **bin/dcache start** domainName. The routing manager and location manager cells are started in each domain and are part of the underlying cell package structure. Each domain will contain at least one cell in addition to them.\n\n## Naming and addressing\n\nDomains must have a name unique through the dCache installation. Each cell has a\nunique name within the domain in which it is running. A fully qualified cell\naddress is formed by combining the cell name and the domain name with an\nat-sign, e.g. `PoolManager@dCacheDomain`. Unqualified addresses either do not\nhave a domain suffix or have a `local` suffix, e.g. `PoolManager@local`. It\nfollows that `local` is an illegal domain name.\n\n## Routing\n\nEach domain has a message routing table. This routing table may be inspected and\nmanipulated through the `System` cell inside that domain. Routing tables are\nmaintained automatically by dCache and there is usually no need to manipulate\nthese manually.\n\nThere are several different types of routes:\n\n| domain  | Routes to other domains                 |\n|---------|-----------------------------------------|\n| topic   | Used for publish-subscribe messaging    |\n| queue   | Used for named queues                   |\n| alias   | A rewriting rule                        |\n| default | Default route if no other route matches |\n\nThere are a few other route types which are not commonly used.\n\n### Cell tunnels\n\nTunnels are cells that establish TCP connections to other tunnels. A tunnel may\neither be listening or connecting. When a connection is established each end\nadds a domain route to the local routing table allowing messages to the other\ndomain to be routed through this tunnel. When the tunnel cell shuts down the\ndomain route is removed.\n\n### Core and satellite domains\n\ndCache domains are designated as either `core` domains or `satellite` domains\nin the configuration. Core domains act as message hubs, forwarding messages on\nbehalf of satellite domains.\n\nCore domains form a fully connected mesh, that is, each core domain has a tunnel\ncell connecting it to a tunnel cell in another core domain. Lexicographic\nordering of the domain name is used to determine which domain connects and which\ndomain listens for a connection.\n\nSatellite domains connect to all core domains.\n\nOther than that, there are no differences between core and satellite domains -\nthey can both host arbitrary dCache services, including none at all.\n\n### Location manager\n\nThe location manager is an implicit service embedded in every dCache domain. The\ncell is called `lm` and one can interact with each instance through the dCache\nadmin shell. Its task is to establish cell tunnels.\n\nOn core domains it will start a cell listening for incoming connections (by\ndefault on TCP 11111 - remember to firewall access to this port). Each core\ndomain registers itself in [Zookeeper](config-zookeeper.md).\n\nOn satellite domains the location manager watches the ZooKeeper state and\nwhenever a core domain registers itself, all satellites will create tunnel cells\nconnecting to the core.\n\n### Routing manager\n\nEach domain has an embedded routing manager service. The cell is called\n`RoutingMgr` and one can interact with each instance through the dCache admin\nshell. Its task is to manage the routing table.\n\nThe routing manager monitors the addition and removal of tunnel cells and domain\nroutes. Whenever a domain route is added in a satellite domain, the routing\nmanager adds a corresponding default route. Similarly, whenever a tunnel cell\ndies, it will remove the installed routes going through that tunnel.\n\nRouting manager instances exchange messages with each other to maintain topic\nand queue routes.\n\n### Queues\n\nThe concept of named queues is borrowed from other messaging systems, even\nthough as we will see in a moment the name is slightly misleading in dCache.\n\nA named queue has an unqualified cell address. Cells writing to a named queue\nare called producers while cells reading from a named queue are called\nconsumers. A named queue can have multiple producers and multiple consumers, but\neach message is only consumed by a single cell.\n\nProducers do not need to do anything special to write a message to a named\nqueue. Consumers however explicitly have to announce that they want to consume\nfrom a queue. When they do this a queue route is installed in the local routing\ntable, allowing messages sent to that queue to be routed to the consumer. The\nrouting manager picks up the new route and forwards this information to other\nrouting managers. In particular a consumer in a satellite domain will cause a\ncorresponding queue route to be installed in all core domains. A consumer in a\ncore domain too will cause queue routes to be installed in satellite domains.\nConsumers in satellite domains do not, however, have queue routes in other\nsatellite domains - messages instead follow a default route to a core domain\nwhich will know where to route the message.\n\nWhen several queue routes apply, one is chosen randomly. In this way a certain\namount of load balancing is achieved. Note that no effort is made to perfectly\nbalance the load. Also note that the address space of a named queue and cells is\nnot separate. A cell with the same name as a named queue is legal and if a local\ncell matches, the message is always delivered locally - i.e. local delivery\ntakes precedence over the routing table.\n\nMany services in dCache consume messages from a named queue that is the same as\ntheir cell name. This way other services do not need to know the fully qualified\naddress of the service and can merely write messages to the named queue.\n\nAs may have been apparent from the description above, a named queue is not\nactually a queue. There is no central store of messages and there is no central\nqueue in a named queue. The latency involved in communication does introduce a\nbuffer capacity allowing a certain number of messages in transit between sender\nand receiver, but this does not constitute a shared queue. A message produced is\nrouted to a consumer and queued locally at the consumer.\n\n### Topics\n\nThe concept of topics is also borrowed from other messaging systems. A topic is\nan unqualified cell address. Cells writing to topic are called publishers, while\ncells receiving messages on a topic are called subscribers. In contrast to a\nnamed queue, messages published to a topic are received by all subscribers. Thus\ntopics provide a multicasting ability to cells.\n\nAs with named queues, publishers do not need to do anything special before\nwriting a message to a topic. Subscribers need to explicitly announce that they\nwant to subscribe to a topic. When they do, a topic route is added to the local\nrouting table. The routing manager picks up the new route and informs other\nrouting managers about the subscription. In particular routing managers in cores\ncontain topic routes to subscribers in other satellite domains and other core\ndomains. Satellite domains however only contain topic routes to local\nsubscribers. Subscribers in other domains are informed through the default\nroute.\n\nThis difference in routing logic with respect to queue routes stems from the\nfact that a message to a topic is delivered along all topic routes - as well as\none of the default routes. Messages to named queues on the other hand are only\nrouted along one of the queue routes chosen at random.\n\n## Configuration\n\nAll domains default to being satellite domains. Unless some domain is explicitly\nmarked as a core domain, domains will be disconnected from each other.\n\nIn a mulit-domain (and multi node) deployments at least one of the domains **must** be configured as `core` domain.\n\n### Explicit configuration of the core domain\n\nOne may alter the default behavior by setting `dcache.broker.scheme` to either\n`core` or `satellite` to designate the domain as either a core or satellite\ndomain.\n\n### Multipath cell communication\n\nA simple star topology obviously makes the central message hub a single point of\nfailure. Since dCache 2.16 it is perfectly valid to have multiple core domains.\nAs mentioned above, the core domains form a fully connected mesh. Satellite\ndomains connect to all core domains such that the distance between two satellite\ndomains is never longer than two hops (i.e. a maximum of one intermediate\ndomain).\n\nThe configuration of such a setup follows the same approach as above: Each core\ndomain sets `dcache.broker.scheme` to `core`. Core domains register themselves\nin ZooKeeper and other domains locate them that way.\n\nCare is taken to cleanly register and unregister core domains in a multipath\nsetup to minimize the effect on the running system. Obviously any service that\nhappened to be running in a core domain being shut down will be unavailable and\nany task that service may have been working on may be lost. Other services will\neventually discover such a situation through a timeout.\n\nThree core domains seem to be a good choice for larger instances.\n\n### Core only deployment\n\nFor small installations it is viable to mark all domains as cores. In such a\nsetup all domains are connected to all other domains. This doesn't scale, but if\nyou don't have more than, say, 10 domains, this should work out just fine. The\nbenefit is that the distance between services is reduced, resulting in lower\nlatency.\n\nA word of warning though: The cells messaging system is deliberately very\nsimple. There is no guaranteed delivery and no guaranteed ordering. Although\ndCache should be robust against such problems, core only deployments will be in\nuncharted territory.\n"], "reference": "dCache facilitates communication between its cells by dividing the system into cells that communicate via messages. Each cell runs inside a domain, and they exchange messages through cell tunnels over TCP. Domains must have unique names, and cells communicate using addresses in the form cellName@domainName. The location manager service controls the TCP communication for short control messages, while dedicated TCP connections are established for data transfer. Additionally, each domain has a message routing table that manages the routing of messages between cells.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is the function of the MissignFiles service in dCache?", "reference_contexts": ["THE MISSING FILES SERVICE\n=========================\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Introduction\n\nWhen a user requests to read a file that doesn't exist, \nthe door will respond to the user with the appropriate error message.\n\nThere are two use-cases where something more sophisticated is\ndesirable.  The first is that dCache should notify one or more\nexternal services that it was requested to read a file that doesn't\nexist.  The second is that dCache will populate the missing file from\nsome external source, allowing the read request to succeed.  There\nmay be other examples where it is desirable for dCache to react to\nfailed read attempts.\n\nThe `MissingFiles` service is a dCache component that perform these tasks.  \nIt exists to allow dCache to react in a coordinated fashion to a user's request to\nread a file that doesn't exist.  This central service instructs the\ndoor to either fail the request or retry (which makes sense only if\nthe file has been fetched from some external source). \nCurrently only the `WebDAV` door provides this supports interaction with \n`MissingFile` service. The interaction is enabled by setting :\n\n```ini\ndcache.enable.missing-files = true\n```\n\nThe `MissingFiles` service is pluggable.  It takes a list of plugins\nand instantiates them.  These plugins are used to determine how\ndCache should react when a user attempts to read a missing file.\nEach plugin is asked in turn what to do until a plugin replies with a\nterminating answer or the list of plugins is exhausted.  A plugin\nreplies saying to fail the request, to retry the request or to ask\nthe next plugin in the chain.  If the last plugin defers the request\nthen then the MissingFiles service will instruct the door to fail the\nrequest.\n\n## Setting up\n\nTo instantiate `MissignFiles` service the following line needs to be added to your \nlayout file.  \n\n```ini\n[<domainName>/missing-files]\n```\n\nAdditionally the service must be enabled:\n\n```ini\ndcache.enable.missing-files = true\n```\n\nThe behavior of `MissignFiles` service is determined by plugins that need \nto be specified as comma separated names:\n\n```ini\nmissing-files.plugins =\n```\n\nCurrently no plugins are available and `MissingFiles` service simply  instructs the door \nto fail the request. \n\n"], "reference": "The MissingFiles service is a dCache component that allows dCache to react in a coordinated fashion to a user's request to read a file that doesn't exist. It instructs the door to either fail the request or retry, depending on whether the file has been fetched from an external source.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What pNFS mean in dCache context?", "reference_contexts": ["dCache as NFSv4.1 Server\n====================================\n\nThis chapter explains how to configure dCache in order to access it via the `NFSv4.1` protocol, allowing clients to mount dCache and perform POSIX IO using standard `NFSv4.1` clients.\n\n> **Important**\n>\n> The `pNFS` mentioned in this chapter is the protocol `NFSv4.1/pNFS` and not the namespace pnfs.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Setting up\n\nTo allow file transfers in and out of dCache using NFSv4.1/pNFS, a new NFSv4.1 door must be started. This door acts then as the mount point for NFS clients.\n\nTo enable the NFSv4.1 door, you have to change the layout file corresponding to your dCache-instance. Enable the NFS within the domain that you want to run it by adding the following line\n\n```ini\n[<domainName>/nfs]\nnfs.version = 4.1\n```\n\nExample:\nYou can just add the following lines to the layout file:\n\n```ini\n[nfs-${host.name}Domain]\n[nfs-${host.name}Domain/nfs]\nnfs.version = 4.1\n```\n\n### Exporting file system\n\nIn addition to run an NFSv4.1 door you need to add exports to the\nexports file. The location of exports file is controlled by\n**nfs.export.file** property and defaults to `/etc/exports`.\n\nAfter reading exports file dCache will read the content of the\ndirectory with additional export tables. The location of directory\ndefined by **nfs.export.dir** property and default to\n`/etc/exports.d`. Only files ending with *.exports* are\nconsidered. Files starting with a dot are ignored. The format of the\nexport tables is similar to the one which is provided by Linux:\n\n    #\n    <path> [host [(options)]]\n\nWhere <options> is a comma separated combination of:\n\n- **ro**\nmatching clients can access this export only in read-only mode. This is the *default* value.\n\n- **rw**\nmatching clients can access this export only in read-write mode\n\n- **noacl**\ndCache ACLs will be ignored; only posix access permissions will be considered. This is the *default* value.\n\n- **acl**\ndCache ACLs will be respected; if present, they override posix permissions. To view the ACLs at the NFS client side, use `nfs4_getfacl` which is in EL7 package `nfs4-acl-tools`.\n\n- **sec=sys**\nmatching clients must access **NFS** using RPCSEC_SYS authentication, where client's local uid and gid is used. This is the *default* value.\n\n- **sec=krb5**\nmatching clients must access **NFS** using RPCSEC_GSS authentication. The Quality of Protection (QOP) is *NONE*, e.g., the data is neither encrypted nor signed when sent over the network. Nevertheless the RPC packets header still protected by checksum.\n\n- **sec=krb5i**\nmatching clients have to access **NFS** using RPCSEC_GSS authentication. The Quality of Protection (QOP) is *INTEGRITY*. The RPC requests and response are protected by checksum.\n\n- **sec=krb5p**\nmatching clients have to access **NFS** using RPCSEC_GSS authentication. The Quality of Protection (QOP) is *PRIVACY*. The RPC requests and response are protected by encryption.\n\n- **all_squash**\nMap all uis and gids to the anonymous user.\n\n- **no_root_squash**\nDo not map requests from the root to anonymous user. This is useful for clients with administrative rights.\n\n- **all_root**\nthe requests from matching clients will be handles as from user root. Additionally, when a new file or directory is created, then parent directory's ownership is inherited.\n\n- **anonuid** and **anongid** specify which uid and gid should be used for  anonymous user.\n\n- **dcap**\nExpose the fact, that nfs server is a dCache instance through special *dot* file. This option is useful when dcap client are used over mounted file system and preferred IO protocol is DCAP. This is the *default* value.\n\n- **no_dcap**\nThe opposite of **dcap** option. Hide the fact, that nfs server is a dCache instance. This option is useful when dcap client are used over mounted file system but nfs protocol still have to be used for IO.\n\n- **pnfs**\nEnable NFSv4.1/pNFS to redirect pnfs capable clients to the dCache pools. This is the *default* value.\n\n- **no_pnfs** and **nopnfs**\nOpposite of **pnfs** option. Enforce clients to send all IO requests to NFS door event independent from client's pNFS capabilities.\n\n- **secure**\nSpecifying whether clients are required to use a privileged port (< 1024). This option typically is used to disallow user-space NFS clients,\nas they might spoof request credentials.\n\n- **insecure**\nThe opposite of **secure** option. The use of privileged port by a client is not required. This is the *default* value.\n\n- **lt=**\nA colon separated ordered list of pnfs layouts that offered to the client. The valid values are:\n\n   - flex_files\n   - nfsv4_1_files\n\nFor modern clients (linux kernel >= 4.10 and RHEL7.4) flex_files is recommended. Default is **nfsv4_1_files**.\n\nFor example:\n\n    /pnfs/dcache.org/data *.dcache.org (rw,sec=krb5i)\n\nNotice, that security flavour used at mount time will be used for client - pool communication as well.\n\nMultiple specifications can be declared like this:\n\n    /pnfs/dcache.org/data *.dcache.org(rw) externalhost.example.org(ro)\n\nIn this example, hosts in the dcache.org may read and write, while host externalhost.example.org may only read.\n\nIf there are multiple path specifications, the shortest matching path wins. If there are multiple host/subnet specifications, the most precise specification wins.\n\n## Configuring NFSv4.1 door with GSS-API support\n\nAdding `sec=krb5` into `/etc/exports` is not sufficient to get\nkerberos authentication to work.\n\nAll clients, pool nodes and node running DOOR-NFS4 must have a valid\nkerberos configuration. Each clients, pool node and node running\nDOOR-NFS4 must have a `/etc/krb5.keytab` with `nfs` service principal:\n\n    nfs/host.domain@<YOUR.REALM>\n\nThe `/etc/dcache/dcache.conf` on pool nodes and node running `NFSv4.1\ndoor` must enable kerberos and RPCSEC_GSS:\n\n```ini\nnfs.rpcsec_gss=true\ndcache.authn.kerberos.realm=<YOUR.REALM>\ndcache.authn.jaas.config=/etc/dcache/gss.conf\ndcache.authn.kerberos.key-distribution-center-list=your.kdc.server\n```\n\nThe `/etc/dcache/gss.conf` on pool nodes and node running `NFSv4.1`\ndoor must configure Java�s security module:\n\n    com.sun.security.jgss.accept {\n    com.sun.security.auth.module.Krb5LoginModule required\n    doNotPrompt=true\n    useKeyTab=true\n    keyTab=\"${/}etc${/}krb5.keytab\"\n    debug=false\n    storeKey=true\n    principal=\"nfs/host.domain@<YOUR.REALM>\";\n    };\n\nNow your NFS client can securely access dCache.\n\n## Configuring principal-id mapping for NFS access\n\nThe `NFSv4.1` uses utf8 based strings to represent user and group names:\n\n   user@DOMAIN.NAME\n\nThis is the case even for non-kerberos based accesses. Nevertheless\nUNIX based clients as well as dCache internally use numbers to\nrepresent uid and gids. A special service, called `idmapd`, takes care\nfor principal-id mapping. On the client nodes the file\n`/etc/idmapd.conf` is usually responsible for consistent mapping on\nthe client side. On the server side, in case of dCache mapping done\nthrough gplazma2. The `identity` type of plug-in required by\nid-mapping service. Please refer to [Chapter 10, Authorization in\ndCache](config-gplazma.md) for instructions about how to configure\n`gPlazma.\n\nFor correct user id mapping nfs4 requires that server and client use the same naming scope, called nfs4domain. This implies a consistent configuration on both sides. To reduce deployment overhead a special auto-discovery mechanism was introduced by SUN Microsystems ( now Oracle) - a [DNS TXT](https://docs.oracle.com/cd/E19253-01/816-4555/epubp/index.html) record. dCache supports this discovery mechanism. When `nfs.domain` property is set, it gets used. If it�s left unset, then DNS TXT record for `_nfsv4idmapdomain` is taken or the default `localdomain` is used when DNS record is absent.\n\nTo avoid big latencies and avoiding multiple queries for the same\ninformation, like ownership of a files in a big directory, the results\nfrom `gPlazma` are cached within `NFSv4.1 door`. The default values\nfor cache size and life time are good enough for typical\ninstallation. Nevertheless they can be overriden in `dcache.conf` or\nlayoutfile:\n\n```ini\n# maximal number of entries in the cache\nnfs.idmap.cache.size = 512\n\n# cache entry maximal lifetime\nnfs.idmap.cache.timeout = 30\n\n# time unit used for timeout. Valid values are:\n# SECONDS, MINUTES, HOURS and DAYS\nnfs.idmap.cache.timeout.unit = SECONDS\n```\n\n## Managing group ids\n\nThe remote procedure call v2 (RPC) that used by nfs protocol defines different\nmechanisms for user authentication. One of them is AUTH_SYS, which majority of\ndeployments are using. Dispire it's popularity, the one downside of using AUTH_SYS\nis the sixteen groups limit. To overcome this limitation dCache nfs server can\nquery gplazma to discover additional groups that the client might have.\nTo enable it the `nfs.idmap.manage-gids` property can be used.\n\n```ini\nnfs.idmap.manage-gids = true\n```\n\nAs enabling it might have negative effects on the performance, the default value\nit false.\n\n> NOTICE: currently only the [ldap](config-gplazma.md#ldap) plugin supports group mapping by uid when `gplazma.ldap.try-uid-mapping` option is enabled.\n\n## Accessing directory tags\n\nThere are two ways to access dCache directory tags over NFS mount: the legacy one as via magic files and as extended attributes.\n\n### Directory tags as magic files\n\nEach directory tag can be accessed via special file name, for example to read or write a tag with the name `foo`, the magic file is\n`.(tag)(foo)`:\n\n```console-user\necho bar > '.(tag)(foo)'\n```\n\nor\n\n```console-user\ncat '.(tag)(foo)'\n|bar\n```\n\nTo list all existing file use `.(tags)()` magic file:\n\n```console-user\n cat '.(tags)()'\n|.(tag)(OSMTemplate)\n|.(tag)(foo)\n|.(tag)(sGroup)\n```\n\n>NOTICE: the directory tags can't be remove via magic files\n\n### Directory tags as extended attributes\n\nThe dCache's NFSv4 door implements *extended attributes over NFS* which is specified in [RFC8276](https://tools.ietf.org/rfc/rfc8276.txt). With compatible NFS client (for the moment only Linux kernel 5.10 and later), the directory tags can be access as extended attributes prefixed with `dcache.tag.`. For example:\n\n```console-user\n attr -l .\n|Attribute \"dcache.tag.OSMTemplate\" has a 15 byte value for .\n|Attribute \"dcache.tag.sGroup\" has a 8 byte value for .\n```\n\nor\n\n```console-user\ngetfattr  .\n|# file: .\n|user.dcache.tag.OSMTemplate\n|user.dcache.tag.sGroup\n```\n\n> NOTICE: the `getfattr` and `setfattr` commands adds an extra `user.` prefix.\n"], "reference": "The pNFS mentioned in this chapter is the protocol NFSv4.1/pNFS and not the namespace pnfs.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "Can you explain what dcache is and how the pinmanager service works in it, especially about pinning and unpinning files?", "reference_contexts": ["THE PINMANAGER SERVICE\n==================================\n\nThe purpose of the `pinmanager` service in dCache is ensuring the presence of a file replica on\ndisk.\n\nIt can be used by explicitly _(un)pinning_ files via the `admin` interface or the `bulk service`,\nbut is also used by the `resilience` component to ensure having a certain number of replicas\navailable. PinManager is also used for keeping a replica _online_ after fetching (staging) it from a\nconnected tertiary storage system (sometimes called HSM - hierarchical storage manager), if staging\nis allowed.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## (Un-)Pinning Concept\n\nA `pin`, also called `sticky`-ness, is a concept describing a file replica on a pool that cannot be\ndeleted for a certain duration. The pin effectively suppresses automatic garbage collection for the\nlifetime of the pin.\n\nPins may have a finite or infinite lifetime. Pins also have an owner, which may be a dCache\nservice (such as `resilience`) or client through a protocol such as `srm`. Only the owner is allowed\nto remove unexpired pins. Several pins (for different users) can exist for the same `pnfsid`, and a\nfile is considered pinned as long as at least one unexpired pin exists.\n\n## The Pin Life Cycle\n\nWhen a pin is created, it will initially appear in state `PINNING`, then transition to\nstate `PINNED` once the attempt is successful.\n\nWhen a pin either has a finite lifetime that has expired or is directly requested to be removed, it\nis put into state `READY_TO_UNPIN`. An 'unpinning' background task runs regularly (default every\nminute), which directly removes all pins in state `READY_TO_UNPIN` that don't require pool contact,\nthen selects a certain number of pins (default 200) in state `READY_TO_UNPIN` for which the\ncorresponding pool needs to be contacted and attempts to remove them as well. During unpinning the\npins are in state `UNPINNING`.\n\nOn success, the pin is deleted from the pool in question as well as the database, on failure the pin\nis put into state `FAILED_TO_UNPIN`. Another background process regularly (default every 2h) resets\nall pins in state `FAILED_TO_UNPIN` back to state `READY_TO_UNPIN` in order to make them eligible to\nbe attempted again.\n\n## Configuring\n\nThe PinManager service can be run in a shared domain. It may also be deployed in high availability (\nHA) mode (coordinated via [ZooKeeper](config-zookeeper.md)) by having several PinManager cells in a\ndCache instance, which then need to share the same database and configuration.\n\n```\npinmanager.db.host=pinman-db-hostname\npinmanager.db.name=dcache\npinmanager.db.password=\npinmanager.db.user=dcache\n```\n\nPins are managed in this central database as well as on the pools containing the replicas.\n\nPin expiration and pin unpinning are background tasks which are executed regularly. The\nproperty `pinmanager.expiration-period` controls how often to execute these tasks. The default value\nis 60 seconds.\n\nThe number of pins that should at most be attempted to be removed and necessitate pool contact per\nunpinning task run can be configured with the property `pinmanager.max-unpins-per-run` and default\nto 200. A value of -1 indicates that there is no limit, which might lead to large CPU and memory\nloads if there are many pending unpin operations.\n\nAnother background task takes care of resetting pins that previously failed to be removed. It can be\nconfigured via `pinmanager.reset-failed-unpins-period` and defaults to 2h.\n"], "reference": "The `pinmanager` service in dCache is responsible for ensuring the presence of a file replica on disk. It allows for explicit (un)pinning of files through the `admin` interface or the `bulk service`, and it is also utilized by the `resilience` component to maintain a certain number of replicas. A `pin`, or `sticky`-ness, prevents a file replica from being deleted for a specified duration, suppressing automatic garbage collection. Pins can have finite or infinite lifetimes and are owned by either a dCache service or a client. When a pin is created, it starts in the `PINNING` state and transitions to `PINNED` upon success. Pins can be removed when they expire or are directly requested to be unpinned, transitioning to `READY_TO_UNPIN`. A background task regularly removes pins in this state, and if successful, the pin is deleted from the pool and database. If it fails, the pin goes to `FAILED_TO_UNPIN`, which is reset back to `READY_TO_UNPIN` by another background process. The PinManager can be configured to run in a shared domain or high availability mode, managing pins in a central database and on the pools containing the replicas.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What role does the PSU play in the pool selection mechanism?", "reference_contexts": ["THE POOLMANAGER SERVICE\n==================================\n\nThe heart of a dCache System is the `poolmanager`. When a user performs an action on a file - reading or writing - a `transfer request` is sent to the dCache system. The `poolmanager` then decides how to handle this request.\n\nIf a file the user wishes to read resides on one of the storage-pools within the dCache system, it will be transferred from that pool to the user. If it resides on several pools, the file will be retrieved from one of the pools determined by a configurable load balancing policy. If all pools the file is stored on are busy, a new copy of the file on an idle pool will be created and this pool will answer the request.\n\nA new copy can either be created by a `pool to pool transfer` (p2p) or by fetching it from a connected tertiary storage system (sometimes called HSM - hierarchical storage manager). Fetching a file from a tertiary storage system is called staging. It is also performed if the file is not present on any of the pools in the dCache system. The pool manager has to decide on which pool the new copy will be created, i.e. staged or p2p-copied.\n\nThe behaviour of the `poolmanager` service is highly configurable. In order to exploit the full potential of the software it is essential to understand the mechanisms used and how they are configured. The `poolmanager` service creates the `PoolManager` cell, which is a unique cell in dCache and consists of several sub-modules: The important ones are the `pool selection unit` (PSU) and the load balancing policy as defined by the partition manager (PM).\n\nThe `poolmanager` can be configured by either directly editing the\nfile `/var/lib/dcache/config/poolmanager.conf` or via the Admin\nInterface. Changes made via the [Admin\nInterface](intouch.md#admin-interface) will be saved in the file\n`/var/lib/dcache/config/poolmanager.conf` by the `save` command. This\nfile will be parsed, whenever the dCache starts up. It is a simple\ntext file containing the corresponding Admin Interface commands. It\ncan therefore also be edited before the system is started. It can also\nbe loaded into a running system with the `reload` command. In this\nchapter we will describe the commands allowed in this file.\n\n> **NOTE**\n>\n> Starting from version 2.16 dCache stores the configuration of\n> `poolmanager` in zookeeper and reads `poolmanager.conf` file only if\n> configuration in zookeeper is missing, e.g. on the first start.\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## The pool selection mechanism\n\nThe PSU is responsible for finding the set of pools which can be used for a specific transfer-request. By telling the PSU which pools are permitted for which type of transfer-request, the administrator of the dCache system can adjust the system to any kind of scenario: Separate organizations served by separate pools, special pools for writing the data to a tertiary storage system, pools in a DMZ which serves only a certain kind of data (e.g., for the grid). This section explains the mechanism employed by the PSU and shows how to configure it with several examples.\n\nThe PSU generates a list of allowed storage-pools for each incoming transfer-request. The PSU configuration described below tells the PSU which combinations of transfer-request and storage-pool are allowed. Imagine a two-dimensional table with a row for each possible transfer-request and a column for each pool - each field in the table containing either �yes� or �no�. For an incoming transfer-request the PSU will return a list of all pools with �yes� in the corresponding row.\n\nInstead of �yes� and �no� the table really contains a *preference* - a non-negative integer. However, the PSU configuration is easier to understand if this is ignored.\n\nActually maintaining such a table in memory (and as user in a configuration file) would be quite inefficient, because there are many possibilities for the transfer-requests. Instead, the PSU consults a set of rules in order to generate the list of allowed pools. Each such rule is called a link because it links a set of transfer-requests to a group of pools.\n\n### Links\n\nA link consists of a set of unit groups and a list of pools. If all the unit groups are matched, the pools belonging to the link are added to the list of allowable pools.\n\nA link is defined in the file `/var/lib/dcache/config/poolmanager.conf` by\n\n      psu create link <link> <unitgroup>\n      psu set link <link> -readpref=<rpref> -writepref=<wpref> -cachepref=<cpref> -p2ppref=<ppref>\n      psu addto link <link> <poolgroup>\n\nFor the preference values see the [section called �Preference Values for Type of Transfer�](#preference-values-for-type-of-transfer).\n\nThe main task is to understand how the unit groups in a link are defined. After we have dealt with that, the preference values will be discussed and a few examples will follow.\n\nThe four properties of a transfer request, which are relevant for the PSU, are the following:\n\n\n**Location of the File**\n\nThe location of the file in the file system is not used directly. Each file has the following two properties which can be set per directory:\n\n-   **Storage Class.** The storage class is a string. It is used by a tertiary storage system to decide where to store the file (i.e. on which set of tapes) and dCache can use the storage class for a similar purpose (i.e. on which pools the file can be stored.). A detailed description of the syntax and how to set the storage class of a directory in the namespace is given in [the section called �Storage Classes�](#storage-classes).\n\n-   **Cache Class.** The cache class is a string with essentially the same functionality as the storage class, except that it is not used by a tertiary storage system. It is used in cases, where the storage class does not provide enough flexibility. It should only be used, if an existing configuration using storage classes does not provide sufficient flexibility.\n\n**IP Address**\nThe IP address of the requesting host.\n\n**Protocol / Type of Door**\nThe protocol respectively the type of door used by the transfer.\n\n**Type of Transfer**\nThe type of transfer is either read, write, p2p request or cache.\n\nA request for reading a file which is not on a read pool will trigger a p2p request and a subsequent read request. These will be treated as two separate requests.\n\nA request for reading a file which is not stored on disk, but has to be staged from a connected tertiary storage system will trigger a `cache` request to fetch the file from the tertiary storage system and a subsequent `read` request. These will be treated as two separate requests.\n\nEach link contains one or more `unit groups`, all of which have to be matched by the transfer request. Each unit group in turn contains several `units`. The unit group is matched if at least one of the units is matched.\n\n#### Types of Units\n\nThere are four types of units: network (`-net`), protocol (`-protocol`), storage class (`-store`) and cache class (`-dcache`) units. Each type imposes a condition on the IP address, the protocol, the storage class and the cache class respectively.\n\nFor each transfer at most one of each of the four unit types will match. If more than one unit of the same type could match the request then the most restrictive unit matches.\n\nThe unit that matches is selected from all units defined in dCache, not just those for a particular unit group. This means that, if a unit group has a unit that could match a request but this request also matches a more restrictive unit defined elsewhere then the less restrictive unit will not match.\n\n**Network Unit**\nA *network unit* consists of an IP address and a net mask written as <IP-address>/<net mask>, say 111.111.111.0/255.255.255.0. It is satisfied, if the request is coming from a host with IP address within the subnet given by the address/netmask pair.\n\n    psu create ugroup <name-of-unitgroup>\n    psu create unit -net <IP-address>/<net mask>\n    psu addto ugroup <name-of-unitgroup> <IP-address>/<net mask>\n\n**Protocol Unit**\nA *protocol unit* consists of the name of the protocol and the version number written as protocol-name/version-number, e.g., `xrootd/3`.\n\n    psu create ugroup <name-of-unitgroup>\n    psu create unit -protocol <protocol-name>/<version-number>\n    psu addto ugroup <name-of-unitgroup> <protocol-name>/<version-number>\n\n**Storage Class Unit**\nA *storage class* unit is given by a storage class. It is satisfied if the requested file has this storage class. Simple wild cards are allowed: for this it is important to know that a storage class must always contain exactly one @-symbol as will be explained in [the section called �Storage Classes�](#storage-classes). In a storage class unit, either the part before the @-symbol or both parts may be replaced by a *-symbol; for example, *@osm and *@* are both valid storage class units whereas `something@*` is invalid. The *-symbol represents a limited wildcard: any string that doesn�t contain an @-symbol will match.\n\n    psu create ugroup <name-of-unitgroup>\n    psu create unit -store <StoreName>:<StorageGroup>@<type-of-storage-system>\n    psu addto ugroup <name-of-unitgroup> <StoreName>:<StorageGroup>@<type-of-storage-system>\n\n**Cache Class Unit**\nA *cache class unit* is given by a cache class. It is satisfied, if the cache class of the requested file agrees with it.\n\n    psu create ugroup <name-of-unitgroup>\n    psu create unit -dcache <name-of-cache-class>\n    psu addto ugroup <name-of-unitgroup> <name-of-cache-class>\n\n#### Preference Values for Type of Transfer\n\nThe conditions for the *type of transfer* are not specified with units. Instead, each link contains four attributes `-readpref`, `-writepref`, `-p2ppref` and `-cachepref`, which specify a preference value for the respective types of transfer. If all the unit groups in the link are matched, the corresponding preference is assigned to each pool the link points to. Since we are ignoring different preference values at the moment, a preference of `0` stands for `no` and a non-zero preference stands for `yes`. A negative value for `-p2ppref` means, that the value for `-p2ppref` should equal the one for the `-readpref`.\n\n##### Multiple non-zero Preference Values\n\n> **NOTE**\n>\n> This explanation of the preference values can be skipped at first reading. It will not be relevant, if all non-zero preference values are the same. If you want to try configuring the pool manager right now without bothering about the preferences, you should only use `0` (for `no`) and, say, `10` (for `yes`) as preferences. You can choose `-p2ppref=-1` if it should match the value for `-readpref`. The first examples below are of this type.\n\nIf several different non-zero preference values are used, the PSU will not generate a single list but a set of lists, each containing pools with the same preference. The Pool Manager will use the list of pools with highest preference and select a pool according to the load balancing policy for the transfer. Only if all pools with the highest preference are offline, the next list will be considered by the Pool Manager. This can be used to configure a set of fall-back pools which are used if none of the other pools are available.\n\n#### Pool Groups\n\nPools can be grouped together to pool groups.\n\n    psu create pgroup <name-of-poolgroup>\n    psu create pool <name-of-pool>\n    psu addto pgroup <name-of-poolgroup> <name-of-pool>\n\nExample:\n\nConsider a host `pool1` with two pools, `pool1_1` and `pool1_2`, and a host `pool2` with one pool `pool2_1`. If you want to treat them in the same way, you would create a pool group and put all of them in it:\n\n    psu create pgroup normal-pools\n    psu create pool pool1_1\n    psu addto pgroup normal-pools pool1_1\n    psu create pool pool1_2\n    psu addto pgroup normal-pools pool1_2\n    psu create pool pool2_1\n    psu addto pgroup normal-pools pool2_1\n\nIf you later want to treat `pool1_2` differently from the others, you would remove it from this pool group and add it to a new one:\n\n    psu removefrom pgroup normal-pools pool1_2\n    psu create pgroup special-pools\n    psu addto pgroup special-pools pool1_2\n\nIn the following, we will assume that the necessary pool groups already exist. All names ending with `-pools` will denote pool groups.\n\nNote that a pool-node will register itself with the `PoolManager:` The pool will be created within the PSU and added to the pool group `default`, if that exists. This is why the dCache system will automatically use any new pool-nodes in the standard configuration: All pools are in `default` and can therefore handle any request.\n\n#### Dynamic Pool Groups\n\nIn some situations it desired that pools dynamically added into groups based on a specific label. Such pool groups have to be created with option `-dynamic` as:\n\n    psu create pgroup -dynamic -tags=key=value\n\nCurrently, Pool Manager only supports pool tag, defined by `pool.tags`, to create such dynamic groups.\n\n    Example:\n\n    psu create pgroup -dynamic -tags=zone=A zone-A-pools\n\nWill create pool group `zone-A-pools` and any existing pool as well as any new pool that configured with tag\n`zone=A` will be automatically included into that pool group.\n\n> **NOTE**\n>\n> Pools can't be manually added into dynamic groups with `psu addto pgroup` admin command.\n\n#### Nested Pool Groups\n\nPool groups can be grouped together into nested pool groups. The special symbol `@` used as prefix\nto `psu addto pgroup` indicates, that the added object is a group:\n\n    Example:\n\n    psu create pgroup group-foo\n    psu create pgroup group-bar\n    psu create pgroup group-foo-bar\n    psu addto pgroup group-foo-bar @group-foo\n    psu addto pgroup group-foo-bar @group-bar\n\nThe pool list of the nested group will be dynamically updated as soon as subgroup is updated.\n\n#### Storage Classes\n\nThe storage class is a string of the form `StoreName:StorageGroup@type-of-storage-system`, where `type-of-storage-system` denotes the type of storage system in use, and `StoreName`:`StorageGroup` is a string describing the storage class in a syntax which depends on the storage system. In general use `type-of-storage-system=osm`.\n\nConsider for example the following setup:\n\n```console-root\nchimera lstag /data/experiment-a\n|Total: 2\n|OSMTemplate\n|sGroup\nchimera readtag /data/experiment-a OSMTemplate\n|StoreName myStore\nchimera readtag /data/experiment-a sGroup\n|STRING\n```\n\nThis is the setup after a fresh installation and it will lead to the\nstorage class `myStore:STRING@osm`. An adjustment to more sensible\nvalues will look like\n\n```console-root\nchimera writetag /data/experiment-a OSMTemplate \"StoreName exp-a\"\nchimera writetag /data/experiment-a sGroup \"run##TODAY_YEAR##\"\n```\n\nand will result in the storage class `exp-a:run##TODAY_YEAR##@osm` for any data\nstored in the `/data/experiment-a` directory.\n\nTo summarize: The storage class depends on the directory the data is stored in and is configurable.\n\n#### Cache Class\n\nStorage classes might already be in use for the configuration of a tertiary storage system. In most cases they should be flexible enough to configure the PSU. However, in rare cases the existing configuration and convention for storage classes might not be flexible enough.\n\nConsider for example a situation, where data produced by an experiment always has the same storage class `exp-a:alldata@osm`. This is good for the tertiary storage system, since all data is supposed to go to the same tape set sequentially. However, the data also contains a relatively small amount of meta-data, which is accessed much more often by analysis jobs than the rest of the data. You would like to keep the meta-data on a dedicated set of dCache pools. However, the storage class does not provide means to accomplish that.\n\nThe cache class of a directory is set by the tag `cacheClass` as follows:\n\n```console-root\nchimera writetag /data/experiment-a cacheClass \"metaData\"\n```\n\nIn this example the meta-data is stored in directories which are\ntagged in this way.\n\nCheck the existing tags of a directory and their content by:\n\n```console-root\nchimera lstag /path/to/directory\n|Total: numberOfTags\n|tag1\n|tag2\n|...\nchimera readtag /path/to/directory tag1\n|contentOfTag1\n```\n\n> **NOTE**\n>\n> A new directory will inherit the tags from the parent directory. But updating a tag will *not* update the tags of any child directories.\n\n#### Define a link\n\nNow we have everything we need to define a link.\n\n    psu create ugroup <name-of-unitgroup>\n    psu create unit - <type> <unit>\n    psu addto ugroup <name-of-unitgroup> <unit>\n\n    psu create pgroup <poolgroup>\n    psu create pool <pool>\n    psu addto pgroup <poolgroup> <pool>\n\n    psu create link <link> <name-of-unitgroup>\n    psu set link <link> -readpref=<10> -writepref=<0> -cachepref=<10>-p2ppref=<-1>\n    psu addto link <link>  <poolgroup>\n\n[return to top](#the-pool-selection-mechanism)\n\n### Examples\n\nFind some examples for the configuration of the PSU below.\n\n#### Separate Write and Read Pools\n\nThe dCache we are going to configure receives data from a running experiment, stores the data onto a tertiary storage system, and serves as a read cache for users who want to analyze the data. While the new data from the experiment should be stored on highly reliable and therefore expensive systems, the cache functionality may be provided by inexpensive hardware. It is therefore desirable to have a set of pools dedicated for writing the new data and a separate set for reading.\n\nExample:\n\nThe simplest configuration for such a setup would consist of two links �write-link� and �read-link�. The configuration is as follows:\n\n    psu create pgroup read-pools\n    psu create pool pool1\n    psu addto pgroup read-pools pool1\n    psu create pgroup write-pools\n    psu create pool pool2\n    psu addto pgroup write-pools pool2\n\n    psu create unit -net 0.0.0.0/0.0.0.0\n    psu create ugroup allnet-cond\n    psu addto ugroup allnet-cond 0.0.0.0/0.0.0.0\n\n    psu create link read-link allnet-cond\n    psu set link read-link -readpref=10 -writepref=0 -cachepref=10\n    psu addto link read-link read-pools\n\n    psu create link write-link allnet-cond\n    psu set link write-link -readpref=0 -writepref=10 -cachepref=0\n    psu addto link write-link write-pools\n\nWhy is the unit group `allnet-cond` necessary? It is used as a condition which is always true in both links. This is needed, because each link must contain at least one unit group.\n\n[return to top](#the-pool-selection-mechanism)\n\n#### Restricted Access by IP Address\n\nYou might not want to give access to the pools for the whole network, as in the previous example ([the section called �Separate Write and Read Pools�](#separate-write-and-read-pools)), though.\n\nExample:\nAssume, the experiment data is copied into the cache from the hosts with IP `111.111.111.201`, `111.111.111.202`, and `111.111.111.203`. As you might guess, the subnet of the site is `111.111.111.0/255.255.255.0`. Access from outside should be denied. Then you would modify the above configuration as follows:\n\n\n    psu create pgroup read-pools\n    psu create pool pool1\n    psu addto pgroup read-pools pool1\n    psu create pgroup write-pools\n    psu create pool pool2\n    psu addto pgroup write-pools pool2\n\n    psu create unit -store *@*\n\n    psu create unit -net 111.111.111.0/255.255.255.0\n    psu create unit -net 111.111.111.201/255.255.255.255\n    psu create unit -net 111.111.111.202/255.255.255.255\n    psu create unit -net 111.111.111.203/255.255.255.255\n\n    psu create ugroup write-cond\n    psu addto ugroup write-cond 111.111.111.201/255.255.255.255\n    psu addto ugroup write-cond 111.111.111.202/255.255.255.255\n    psu addto ugroup write-cond 111.111.111.203/255.255.255.255\n\n    psu create ugroup read-cond\n    psu addto ugroup read-cond 111.111.111.0/255.255.255.0\n    psu addto ugroup read-cond 111.111.111.201/255.255.255.255\n    psu addto ugroup read-cond 111.111.111.202/255.255.255.255\n    psu addto ugroup read-cond 111.111.111.203/255.255.255.255\n\n    psu create link read-link read-cond\n    psu set link read-link -readpref=10 -writepref=0 -cachepref=10\n    psu addto link read-link read-pools\n\n    psu create link write-link write-cond\n    psu set link write-link -readpref=0 -writepref=10 -cachepref=0\n    psu addto link write-link write-pools\n\n> **IMPORTANT**\n>\n> For a given transfer exactly zero or one storage class unit, cache class unit, net unit and protocol unit will match. As always the most restrictive one will match, the IP `111.111.111.201` will match the `111.111.111.201/255.255.255.255` unit and not the `111.111.111.0/255.255.255.0` unit. Therefore if you only add `111.111.111.0/255.255.255.0` to the unit group �read-cond�, the transfer request coming from the IP `111.111.111.201` will only be allowed to write and not to read. The same is true for transfer requests from `111.111.111.202` and `111.111.111.203`.\n\n\n#### Reserving Pools for Storage and Cache Classes\n\nIf pools are financed by one experimental group, they probably do not like it if they are also used by another group. The best way to restrict data belonging to one experiment to a set of pools is with the help of storage class conditions. If more flexibility is needed, cache class conditions can be used for the same purpose.\n\nExample:\n\nAssume, data of experiment A obtained in ##TODAY_YEAR## is written\ninto subdirectories in the namespace tree which are tagged with the\nstorage class `exp-a:run##TODAY_YEAR##@osm`, and similarly for the other\nyears. (How this is done is described in [the section called �Storage\nClasses�](#storage-classes).) Experiment B uses the storage class\n`exp-b:alldata@osm` for all its data. Especially important data is\ntagged with the cache class `important`. (This is described in [the\nsection called �Cache Class�](#cache-class).) A suitable setup would\nbe:\n\n\n\n      psu create pgroup exp-a-pools\n      psu create pool pool1\n      psu addto pgroup exp-a-pools pool1\n\n      psu create pgroup exp-b-pools\n      psu create pool pool2\n      psu addto pgroup exp-b-pools pool2\n\n      psu create pgroup exp-b-imp-pools\n      psu create pool pool3\n      psu addto pgroup exp-b-imp-pools pool3\n\n      psu create unit -net 111.111.111.0/255.255.255.0\n      psu create ugroup allnet-cond\n      psu addto ugroup allnet-cond 111.111.111.0/255.255.255.0\n\n      psu create ugroup exp-a-cond\n      psu create unit -store exp-a:run##LASTYEAR_YEAR##@osm\n      psu addto ugroup exp-a-cond exp-a:run##LASTYEAR_YEAR##@osm\n      psu create unit -store exp-a:run##TODAY_YEAR##@osm\n      psu addto ugroup exp-a-cond exp-a:run##TODAY_YEAR##@osm\n\n      psu create link exp-a-link allnet-cond exp-a-cond\n      psu set link exp-a-link -readpref=10 -writepref=10 -cachepref=10\n      psu addto link exp-a-link exp-a-pools\n\n      psu create ugroup exp-b-cond\n      psu create unit -store exp-b:alldata@osm\n      psu addto ugroup exp-b-cond exp-b:alldata@osm\n\n      psu create ugroup imp-cond\n      psu create unit -dcache important\n      psu addto ugroup imp-cond important\n\n      psu create link exp-b-link allnet-cond exp-b-cond\n      psu set link exp-b-link -readpref=10 -writepref=10 -cachepref=10\n      psu addto link exp-b-link exp-b-pools\n\n      psu create link exp-b-imp-link allnet-cond exp-b-cond imp-cond\n      psu set link exp-b-imp-link -readpref=20 -writepref=20 -cachepref=20\n      psu addto link exp-b-link exp-b-imp-pools\n\n\n\nData tagged with cache class �`important`� will always be written and read from pools in the pool group `exp-b-imp-pools`, except when all pools in this group cannot be reached. Then the pools in `exp-a-pools` will be used.\nNote again that these will never be used otherwise. Not even, if all pools in `exp-b-imp-pools` are very busy and some pools in `exp-a-pools` have nothing to do and lots of free space.\n\nThe central IT department might also want to set up a few pools, which are used as fall-back, if none of the pools of the experiments are functioning. These will also be used for internal testing. The following would have to be added to the previous setup:\n\nExample:\n\n    psu create pgroup it-pools\n    psu create pool pool_it\n    psu addto pgroup it-pools pool_it\n\n    psu create link fallback-link allnet-cond\n    psu set link fallback-link -readpref=5 -writepref=5 -cachepref=5\n    psu addto link fallback-link it-pools\n\nNote again that these will only be used, if none of the experiments\npools can be reached, or if the storage class is not of the form\n`exp-a:run##LASTYEAR_YEAR##@osm`, `exp-a:##TODAY_YEAR##@osm`, or\n`exp-b:alldata@osm`. If the administrator fails to create the unit\n`exp-a:run##TODAY_YEAR##@osm` and add it to the unit group\n`exp-a-cond`, the fall-back pools will be used eventually.\n\n## The partition manager\n\nThe partition manager defines one or more load balancing policies. Whereas the PSU produces a prioritized set of candidate pools using a collection of rules defined by the administrator, the load balancing policy determines the specific pool to use. It is also the load balancing policy that determines when to fall back to lesser prirority links, or when to trigger creation of additional copies of a file.\n\nSince the load balancing policy and parameters are defined per partition, understanding the partition manager is essential to tuning load balancing. This does not imply that one has to partition the dCache instance. It is perfectly valid to use a single partition for the complete instance.\n\nThis section documents the use of the partition manager, how to create partitions, set parameters and how to associate links with partitions. In the following sections the available partition types and their configuration parameters are described.\n\n### Overview\n\nThere are various parameters that affect the load balancing policy. Some of them are generic and apply to any load balancing policy, but many are specific to a particular policy. To avoid limiting the complete dCache instance to a single configuration, the choice of load balancing policy and the various parameters apply to partitions of the instance. The load balancing algorithm and the available parameters is determined by the partition type.\n\nEach PSU link can be associated with a different partion and the policy and parameters of that partition will be used to choose a pool from the set of candidate pools. The only partition that exists without being explicitly created is the partition called `default`. This partition is used by all links that do not explicitly identify a partition. Other partitions can be created or modified as needed.\n\nThe `default` partition has a hard-coded partition type called `classic`. This type implements the one load balancing policy that was available in dCache before version 2.0. The `classic` partition type is described later. Other partitions have one of a number of available types. The system is pluggable, meaning that third party plugins can be loaded at runtime and add additional partition types, thus providing the ultimate control over load balancing in dCache. Documentation on how to develop plugins is beyond the scope of this chapter.\n\nTo ease the management of partition parameters, a common set of shared parameters can be defined outside all partitions. Any parameter not explicitly set on a partition inherits the value from the common set. If not defined in the common set, a default value determined by the partition type is used. Currently, the common set of parameters happens to be the same as the parameters of the `default` partition, however this is only due to compatibility constraints and may change in future versions.\n\n### Managing partitions\n\nFor each partition you can choose the load balancing policy. You do this by chosing the type of the partition.\n\nCurrently four different partition types are supported:\n\n**classic**:\nThis is the pool selection algorithm used in the versions of dCache prior to version 2.0. See [the section called �Classic Partitions�](#classic-partitions) for a detailed description.\n\n**random**:\nThis pool selection algorithm selects a pool randomly from the set of available pools.\n\n**lru**:\nThis pool selection algorithm selects the \"least recently used\" pool, i.e. the one that has not been used the longest.\n\n**wass**:\nThis pool selection algorithm selects pools randomly weighted by available space, while incorporating age and amount of garbage collectible files and information about load.\n\nThis is the partition type of the default partition.\n\nCommands related to dCache partitioning:\n\n-   `pm types`\n\nLists available partition types. New partition types can be added through plugins.\n\n\n-   `pm create`\n\n[-type=partitionType] partitionName\nCreates a new partition. If no partition type is specified, then a `wass` partition is created.\n\n\n-   `pm set`  [partitionName] -parameterName =value|off\nSets a parameter `parameterName` to a new value.\n\n    If `partitionName` is omitted, the common shared set of parameters is updated. The value is used by any partition for which the parameter is not explicitly set.\n\nIf a parameter is set to off then this parameter is no longer defined and is inherited from the common shared set of parameters, or a partition type specific default value is used if the parameter is not defined in the common set.\n\n-   `pm ls`  [-l] [partitionName]\nLists a single or all partitions, including the type of each partition. If a partition name or the `-l` option are used, then the partition parameters are shown too. Inherited and default values are identified as such.\n\n-   `pm destroy` partitionName\nRemoves a partition from dCache. Any links configured to use this partition will fall back to the `default` partition.\n\n\n### Using partitions\n\nA partition, so far, is just a set of parameters which may or may not differ from the default set. To let a partition relate to a part of the dCache, links are used. Each link may be assigned to exactly one partition. If not set, or the assigned partition doesn't exist, the link defaults to the `default` partition.\n\npsu set link [linkName] -section= partitionName [other-options...]\n\nWhenever this link is chosen for pool selection, the associated parameters of the assigned partition will become active for further processing.\n\n> **WARNING**\n>\n> Depending on the way links are setup it may very well happen that more than just one link is triggered for a particular dCache request. This is not illegal but leads to an ambiguity in selecting an appropriate dCache partition. If only one of the selected links has a partition assigned, this partition is chosen. Otherwise, if different links point to different partitions, the result is indeterminate. This issue is not yet solved and we recommend to clean up the poolmanager configuration to eliminate links with the same preferences for the same type of requests.\n\nIn the [Web Interface](intouch.md#the-web-interface-for-monitoring-dcache) you can find a web page listing partitions and more information. You will find a page summarizing the partition status of the system. This is essentially the output of the command `pm ls -l`.\n\nExample:\n\nFor your dCache on dcache.example.org the address is\nhttp://dcache.example.org:2288/poolInfo/parameterHandler/set/matrix/*\n\n\n#### Examples\n\nFor the subsequent examples we assume a basic poolmanager setup :\n\n    #\n    # define the units\n    #\n    psu create unit -protocol   */*\n    psu create unit -protocol   xrootd/*\n    psu create unit -net        0.0.0.0/0.0.0.0\n    psu create unit -net        131.169.0.0/255.255.0.0\n    psu create unit -store      *@*\n    #\n    #  define unit groups\n    #\n    psu create ugroup  any-protocol\n    psu create ugroup  any-store\n    psu create ugroup  world-net\n    psu create ugroup  xrootd\n    #\n    psu addto ugroup any-protocol */*\n    psu addto ugroup any-store    *@*\n    psu addto ugroup world-net    0.0.0.0/0.0.0.0\n    psu addto ugroup desy-net     131.169.0.0/255.255.0.0\n    psu addto ugroup xrootd       xrootd/*\n    #\n    #  define the pools\n    #\n    psu create pool pool1\n    psu create pool pool2\n    psu create pool pool3\n    psu create pool pool4\n\n    #\n    #  define the pool groups\n    #\n    psu create pgroup default-pools\n    psu create pgroup special-pools\n    #\n    psu addto pgroup default-pools pool1\n    psu addto pgroup default-pools pool2\n    #\n    psu addto pgroup special-pools pool3\n    psu addto pgroup special-pools pool4\n    #\n\n##### Disallowing pool to pool transfers for special pool groups based on the access protocol\n\nFor a special set of pools, where we only allow the xroot protocol, we\ndon't want the datasets to be replicated on high load while for the\nrest of the pools we allow replication on hot spot detection.\n\n    #\n    pm create xrootd-section\n    #\n    pm set default        -p2p=0.4\n    pm set xrootd-section -p2p=0.0\n    #\n    psu create link default-link any-protocol any-store world-net\n    psu addto  link default-link default-pools\n    psu set    link default-link -readpref=10 -cachepref=10 -writepref=0\n    #\n    psu create link xrootd-link xrootd any-store world-net\n    psu addto  link xrootd-link special-pools\n    psu set    link xrootd-link -readpref=10 -cachepref=10 -writepref=0\n    psu set    link xrootd-link -section=xrootd-section\n    #\n\n##### Choosing pools randomly for incoming traffic only\n\nFor a set of pools we select pools following the default setting of cpu and space related cost factors. For incoming traffic from outside, though, we select the same pools, but in a randomly distributed fashion. Please note that this is not really a physical partitioning of the dCache system, but rather a virtual one, applied to the same set of pools.\n\n    #\n    pm create incoming-section\n    #\n    pm set default          -cpucostfactor=0.2 -spacecostfactor=1.0\n    pm set incoming-section -cpucostfactor=0.0 -spacecostfactor=0.0\n    #\n    psu create link default-link any-protocol any-store desy-net\n    psu addto  link default-link default-pools\n    psu set    link default-link -readpref=10 -cachepref=10 -writepref=10\n    #\n    psu create link default-link any-protocol any-store world-net\n    psu addto  link default-link default-pools\n    psu set    link default-link -readpref=10 -cachepref=10 -writepref=0\n    #\n    psu create link incoming-link any-protocol any-store world-net\n    psu addto  link incoming-link default-pools\n    psu set    link incoming-link -readpref=10 -cachepref=10 -writepref=10\n    psu set    link incoming-link -section=incoming-section\n    #\n\n### Classic partitions\n\nThe `classic` partition type implements the load balancing policy known from dCache releases before version 2.0. This partition type is still the default. This section describes this load balancing policy and the available configuration parameters.\n\nExample:\n\nTo create a classic partition use the command: `pm create` -type=classic  <partitionName>\n\n#### Load Balancing Policy\n\nFrom the allowable pools as determined by the [pool selection unit](rf-glossary.md#pool-selection-unit), the pool manager determines the pool used for storing or reading a file by calculating a [cost](rf-glossary.md#cost) value for each pool. The pool with the lowest cost is used.\n\nIf a client requests to read a file which is stored on more than one allowable pool, [the performance costs](rf-glossary.md#the-performance_costs) are calculated for these pools. In short, this cost value describes how much the pool is currently occupied with transfers.\n\nIf a pool has to be selected for storing a file, which is either written by a client or [restored](rf-glossary.md#restored) from a [tape backend](rf-glossary.md#tape_backend), this performance cost is combined with a [space cost](rf-glossary.md#space_cost) value to a [total cost](rf-glossary.md#total-cost) value for the decision. The space cost describes how much it �hurts� to free space on the pool for the file.\n\nThe [cost module](rf-glossary.md#cost-module) is responsible for calculating the cost values for all pools. The pools regularly send all necessary information about space usage and request queue lengths to the cost module. It can be regarded as a cache for all this information. This way it is not necessary to send �get cost� requests to the pools for each client request. The cost module interpolates the expected costs until a new precise information package is coming from the pools. This mechanism prevents clumping of requests.\n\nCalculating the cost for a data transfer is done in two steps. First, the cost module merges all information about space and transfer queues of the pools to calculate the performance and space costs separately. Second, in the case of a write or stage request, these two numbers are merged to build the total cost for each pool. The first step is isolated within a separate loadable class. The second step is done by the partition.\n\n#### The Performance Cost\n\nThe load of a pool is determined by comparing the current number of active and waiting transfers to the maximum number of concurrent transfers allowed. This is done separately for each of the transfer types (store, restore, pool-to-pool client, pool-to-pool server, and client request) with the following equation:\n\nperfCost(per Type) = ( activeTransfers + waitingTransfers ) / maxAllowed .\n\nThe maximum number of concurrent transfers (`maxAllowed`) can be configured with the commands [store](rf-glossary.md#to-store),[restore](rf-glossary.md#to-restore), pool-to-pool client, pool-to-pool server, and client request) with the following equation:.\n\nperfCost(per Type) = ( activeTransfers + waitingTransfers ) / maxAllowed .\n\nThe maximum number of concurrent transfers (maxAllowed) can be configured with the commands [st set max active](reference.md#st-set-max-active) (store), [rh set max active](reference.md#rh-set-max-active) (restore), [mover set max] activereference.md#mover-set-max) (client request), [mover set max active -queue=p2p](reference.md#mover-set-max-active-queue-p2p) (pool-to-pool server), and [pp set max active](reference.md#pp-set-max-active) (pool-to-pool client).\n\nThen the average is taken for each mover type where maxAllowed is not zero. For a pool where store, restore and client transfers are allowed, e.g.,\n\nperfCost(total) = ( perfCost(store) + perfCost(restore) + perfCost(client) ) / 3 ,\n\nand for a read only pool:\n\nperfCost(total) = ( perfCost(restore) + perfCost(client) ) / 2 .\n\nFor a well balanced system, the performance cost should not exceed 1.0.\n\n#### The Space Cost\n\nIn this section only the new scheme for calculating the space cost will be described. Be aware, that the old scheme will be used if the [breakeven parameter](rf-glossary.md#breakeven-parameter) of a pool is larger or equal 1.0.\n\nThe cost value used for determining a pool for storing a file depends either on the free space on the pool or on the age of the [least recently used (LRU) file](rf-glossary.md#least-recently-used-lru-file), which whould have to be deleted.\n\nThe space cost is calculated as follows:\n\n<table>\n<colgroup>\n<col width=\"23%\" />\n<col width=\"5%\" />\n<col width=\"29%\" />\n<col width=\"3%\" />\n<col width=\"29%\" />\n<col width=\"9%\" />\n</colgroup>\n<tbody>\n<tr class=\"odd\">\n<td>If</td>\n<td>freeSpace &gt; gapPara</td>\n<td></td>\n<td></td>\n<td>then</td>\n<td>spaceCost = 3 * newFileSize / freeSpace</td>\n</tr>\n<tr class=\"even\">\n<td>If</td>\n<td>freeSpace &lt;= gapPara</td>\n<td>and</td>\n<td>lruAge &lt; 60</td>\n<td>then</td>\n<td>spaceCost = 1 + costForMinute</td>\n</tr>\n<tr class=\"odd\">\n<td>If</td>\n<td>freeSpace &lt;= gapPara</td>\n<td>and</td>\n<td>lruAge &gt;= 60</td>\n<td>then</td>\n<td>spaceCost = 1 + costForMinute * 60 / lruAge</td>\n</tr>\n</tbody>\n</table>\n\nwhere the variable names have the following meanings:\n\nfreeSpace\nThe free space left on the pool\n\nnewFileSize\nThe size of the file to be written to one of the pools, and at least 50MB.\n\nlruAge\nThe age of the [least recently used file](rf-glossary.md#least-recently-used-lru-file) on the pool.\n\ngapPara\nThe gap parameter. Default is 4 GiB. The size of free space below which it will be assumed that the pool is full and consequently the least recently used file has to be removed. If, on the other hand, the free space is greater than `gapPara`, it will be expensive to store a file on the pool which exceeds the free space.\n\nIt can be set per pool with the [set gap](reference.md#set-gap) command. This has to be done in the pool cell and not in the pool manager cell. Nevertheless it only influences the cost calculation scheme within the pool manager and not the bahaviour of the pool itself.\n\ncostForMinute\nA parameter which fixes the space cost of a one-minute-old LRU file to (1 + costForMinute). It can be set with the [set breakeven](reference.md#set-breakeven), where\n\ncostForMinute = breakeven \\* 7 \\* 24 \\* 60.\n\nI.e. the the space cost of a one-week-old LRU file will be (1 + breakeven). Note again, that all this only applies if breakeven &lt; 1.0\n\nThe prescription above can be stated a little differently as follows:\n\n|     |                         |      |                                                             |\n|-----|-------------------------|------|-------------------------------------------------------------|\n| If  | freeSpace &gt; gapPara  | then | spaceCost = 3 \\* newFileSize / freeSpace                    |\n| If  | freeSpace &lt;= gapPara | then | spaceCost = 1 + breakeven \\* 7 \\* 24 \\* 60 \\* 60 / lruAge , |\n\nwhere `newFileSize` is at least 50MB and `lruAge` at least one minute.\n\n##### Rationale\n\nAs the last version of the formula suggests, a pool can be in two states: Either freeSpace &gt; gapPara or freeSpace &lt;= gapPara - either there is free space left to store files without deleting cached files or there isn't.\n\nTherefore, `gapPara` should be around the size of the smallest files which frequently might be written to the pool. If files smaller than `gapPara` appear very seldom or never, the pool might get stuck in the first of the two cases with a high cost.\n\nIf the LRU file is smaller than the new file, other files might have to be deleted. If these are much younger than the LRU file, this space cost calculation scheme might not lead to a selection of the optimal pool. However, in pratice this happens very seldomly and this scheme turns out to be very efficient.\n\n#### The Total Cost\n\nThe total cost is a linear combination of the [performance](rf-glossary.md#performance-cost) and [space cost](rf-glossary.md#space-cost). I.e. totalCost = ccf \\* perfCost + scf \\* spaceCost , where `ccf` and `scf` are configurable with the command [set pool decision](reference.md#set-pool-decision). E.g.,\n\n    (PoolManager) admin > set pool decision -spacecostfactor=3 -cpucostfactor=1\n\nwill give the [space cost](rf-glossary.md#space-cost) three times the weight of the [performance cost](rf-glossary.md#performance-cost).\n\n#### Parameters of Classic Partitions\n\nClassic partitions have a large number of tunable parameters. These parameters are set using the `pm set` command.\n\nExample:\n\nTo set the space cost factor on the `default` partition to `0.3`, use the following command:\n\n    pm set default -spacecostfactor=0.3\n\n\n| Command                                       | Meaning                                                                                                                                                                                                                                                                                                                 | Type    |\n|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|\n| `pm set` partitionName -spacecostfactor=scf   | Sets the `space cost factor` for the partition.\n\n                                                 The default value is `1.0`.                                                                                                                                                                                                                                                                                              | float   |\n| `pm set` partitionName -cpucostfactor=ccf     | Sets the cpu cost factor for the partition.\n\n                                                 The default value is `1.0`.                                                                                                                                                                                                                                                                                              | float   |\n| `pm set` partitionName -idle=idle-value       | The concept of the idle value will be turned on if idle-value &gt; `0.0`.\n\n                                                 A pool is idle if its performance cost is smaller than the idle-value. Otherwise it is not idle.\n\n                                                 If one or more pools that satisfy the read request are idle then only one of them is chosen for a particular file irrespective of total cost. I.e. if the same file is requested more than once it will always be taken from the same pool. This allowes the copies on the other pools to age and be garbage collected.\n\n                                                 The default value is `0.0`, which disables this feature.                                                                                                                                                                                                                                                                 | float   |\n| `pm set` partitionName -p2p=p2p-value         | Sets the static replication threshold for the partition.\n\n                                                 If the performance cost on the best pool exceeds p2p-value and the value for [slope] = `0.0` then this pool is called hot and a pool to pool replication may be triggered.\n\n                                                 The default value is `0.0`, which disables this feature.                                                                                                                                                                                                                                                                 | float   |\n| `pm set` partitionName -alert=value           | Sets the alert value for the partition.\n\n                                                 If the best pool's performance cost exceeds the p2p value and the alert value then no pool to pool copy is triggered and a message will be logged stating that no pool to pool copy will be made.\n\n                                                 The default value is `0.0`, which disables this feature.                                                                                                                                                                                                                                                                 | float   |\n| `pm set` partitionName -panic=value           | Sets the panic cost cut level for the partition.\n\n                                                 If the performance cost of the best pool exceeds the panic cost cut level the request will fail.\n\n                                                 The default value is `0.0`, which disables this feature.                                                                                                                                                                                                                                                                 | float   |\n| `pm set` partitionName -fallback=value        | Sets the fallback cost cut level for the partition.\n\n                                                 If the best pool's performance cost exceeds the fallback cost cut level then a pool of the next level will be chosen. This means for example that instead of choosing a pool with `readpref` = 20 a pool with `readpref` &lt; 20 will be chosen.\n\n                                                 The default value is `0.0`, which disables this feature.                                                                                                                                                                                                                                                                 | float   |\n| `pm set` partitionName -slope=slope           | Sets the dynamic replication threshold value for the partition.\n\n                                                 If slope&gt; `0.01` then the product of best pool's performance cost and slope is used as threshold for pool to pool replication.\n\n                                                 If the performance cost on the best pool exceeds this threshold then this pool is called hot.\n\n                                                 The default value is `0.0`, which disables this feature.                                                                                                                                                                                                                                                                 | float   |\n| `pm set` partitionName -p2p-allowed=value     | This value can be specified if an HSM is attached to the dCache.\n\n                                                 If a partition has no HSM connected, then this option is overridden. This means that no matter which value is set for `p2p-allowed` the pool to pool replication will always be allowed.\n\n                                                 By setting value = `off` the values for `p2p-allowed`, `p2p-oncost` and `p2p-fortransfer` will take over the value of the default partition.\n\n                                                 If value = `yes` then pool to pool replication is allowed.\n\n                                                 As a side effect of setting value = `no` the values for `p2p-oncost` and `p2p-fortransfer` will also be set to `no`.\n\n                                                 The default value is `yes`.                                                                                                                                                                                                                                                                                              | boolean |\n| `pm set` partitionName -p2p-oncost=value      | Determines whether pool to pool replication is allowed if the best pool for a read request is hot.\n\n                                                 The default value is `no`.                                                                                                                                                                                                                                                                                               | boolean |\n| `pm set` partitionName -p2p-fortransfer=value | If the best pool is hot and the requested file will be copied either from the hot pool or from tape to another pool, then the requested file will be read from the pool where it just had been copied to if value = `yes`. If value = `no` then the requested file will be read from the hot pool.\n\n                                                 The default value is `no`.                                                                                                                                                                                                                                                                                               | boolean |\n| `pm set` partitionName -stage-allowed=value   | Set the stage allowed value to `yes` if a tape system is connected and to `no` otherwise.\n\n                                                 As a side effect, setting the value for `stage-allowed` to `no` changes the value for `stage-oncost` to `no`.\n\n                                                 The default value is `no`.                                                                                                                                                                                                                                                                                               | boolean |\n| `pm set` partitionName -stage-oncost=value    | If the best pool is hot, p2p-oncost is disabled and an HSM is connected to a pool then this parameter determines whether to stage the requested file to a different pool.\n\n                                                 The default value is `no`.                                                                                                                                                                                                                                                                                               | boolean |\n| `pm set` partitionName -max-copies=copies     | Sets the maximal number of replicas of one file. If the maximum is reached no more replicas will be created.\n\n                                                 The default value is `500`.                                                                                                                                                                                                                                                                                              | integer |\n\n## Link Groups\n\nThe PoolManager supports a type of objects called link groups. These link groups are used by the [SRM SpaceManager](config-SRM.md#srm-spacemanager) to make reservations against space. Each link group corresponds to a number of dCache pools in the following way: A link group is a collection of [links](#links) and each link points to a set of pools. Each link group knows about the size of its available space, which is the sum of all sizes of available space in all the pools included in this link group.\n\nTo create a new link group login to the [Admin Interface](intouch.md#the-admin-interface) and `\\c` to the PoolManager.\n\n    (local) admin > \\c PoolManager\n    (PoolManager) admin > psu create linkGroup <linkgroup>\n    (PoolManager) admin > psu addto linkGroup <linkgroup> <link>\n    (PoolManager) admin > save\n\n\nWith `save` the changes will be saved to the file\n`/var/lib/dcache/config/poolmanager.conf`.\n\n> **NOTE**\n>\n> You can also edit the file `/var/lib/dcache/config/poolmanager.conf`\n> to create a new link group. Please make sure that it already\n> exists. Otherwise you will have to create it first via the Admin\n> Interface by\n>\n>     (PoolManager) admin > save\n>\n> Edit the file `/var/lib/dcache/config/poolmanager.conf`\n>\n>     psu create linkGroup <linkgroup>\n>     psu addto linkGroup <linkgroup> <link>\n>\n> After editing this file you will have to restart the domain which contains the PoolManager cell to apply the changes.\n\n> **NOTE**\n>\n> Administrators will have to take care, that no pool is present in more than one link group.\n\n**Access latency and retention policy.**\n\nA space reservation has a *retention policy* and an *access latency*, where retention policy describes the quality of the storage service that will be provided for files in the space reservation and access latency describes the availability of the files. See [the section called �Properties of Space Reservation�](config-SRM.md#properties-of-space-reservation) for further details.\n\nA link group has five boolean properties called `replicaAllowed,\noutputAllowed, custodialAllowed, onlineAllowed` and `nearlineAllowed`,\nwhich determine the access latencies and retention policies allowed in\nthe link group. The values of these properties (true or false) can be\nconfigured via the Admin Interface or directly in the file\n`/var/lib/dcache/config/poolmanager.conf`.\n\nFor a space reservation to be allowed in a link group, the the retention policy and access latency of the space reservation must be allowed in the link group.\n\n    (PoolManager) admin > psu set linkGroup custodialAllowed <linkgroup> <true|false>\n    (PoolManager) admin > psu set linkGroup outputAllowed <linkgroup> <true|false>\n    (PoolManager) admin > psu set linkGroup replicaAllowed <linkgroup> <true|false>\n    (PoolManager) admin > psu set linkGroup onlineAllowed <linkgroup> <true|false>\n    (PoolManager) admin > psu set linkGroup nearlineAllowed <linkgroup> <true|false>\n\n> **IMPORTANT**\n>\n> It is up to the administrator to ensure that the link groups' properties are specified correctly.\n>\n> For example dCache will not complain if a link group that does not support a tape backend will be declared as one that supports `custodial` files.\n>\n> It is essential that space in a link group is homogeneous with respect to access latencies, retention policies and storage groups accepted. Otherwise space reservations cannot be guaranteed. For instance, if only a subset of pools accessible through a link group support custodial files, there is no guarantee that a custodial space reservation created within the link group will fit on those pools.\n\n  [Admin Interface]: #intouch-admin\n  [section\\_title]: #cf-pm-psu-pref\n  [1]: #secStorageClass\n  [2]: #secExReadWrite\n  [3]: #secCacheClass\n  [4]: #cf-pm-classic\n  [How to Pick a Pool]: http://www.dcache.org/articles/wass.html\n  [Web Interface]: #intouch-web\n  [???]: #cmd-st_set_max_active\n  [5]: #cmd-rh_set_max_active\n  [6]: #cmd-mover_set_max_active\n  [7]: #cmd-p2p_set_max_active\n  [8]: #cmd-pp_set_max_active\n  [9]: #cmd-set_gap\n  [10]: #cmd-set_breakeven\n  [11]: #cmd-set_pool_decision\n  [slope]: #slope\n  [SRM CELL-SPACEMNGR]: #cf-srm-space\n  [links]: #cf-pm-links\n  [12]: #cf-srm-intro-spaceReservation\n"], "reference": "The PSU is responsible for finding the set of pools that can be used for a specific transfer request. It generates a list of allowed storage pools for each incoming transfer request by consulting a set of rules, known as links, which link a set of transfer requests to a group of pools.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is REST API used for in the QoS service?", "reference_contexts": ["The QoS Service\n======================\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Introduction to the qos service\n\nQuality of service (QoS) is a broad term with a number of different meanings,\nbut for our purposes here it refers to whether a file needs to be\non disk, on tape, or both, as well as to how many copies of the file should be\nmaintained on those media (and, eventually, for how long).\n\nThe implementation of the QoS service is a refactoring of Resilience which\nincorporates and extends the latter's capabilities.  The first version (current) of QoS\nonly manages definitions that are still based on directory tags and storage\nunit attributes.   The intention of the refactoring, however, is to allow\nus to redesign how QoS is specified without having to change in\na significant way the underlying infrastructure which handles it.  Thus\nin the (near) future we hope to be able to support such things as multiple\ntape locations, slow vs. fast disk (e.g. spinning vs SSD), and time-bound\ndefinitions as to when a file would change its QoS status.  This would\nnecessarily mean that QoS definitions would become more properly a part of\nthe file's metadata and not be determined (except perhaps by default) via\nglobal properties such as directory tags or the file's storage unit.\n\n## Configuring the QoS service\n\nThe QoS service is actually made up of four different components.\n\n1. the ``QoS Engine``:  This is the \"frontend\" of the service which receives\n   cache location updates (currently directly from the PnfsManager, as\n   with Resilience), but also requests to modify a file's status (disk, tape, both),\n   either on single files made through the REST frontend, or on file sets through\n   the bulk service (also accessible through the REST API). The engine also\n   manages the logic, or rules, for determining what a file's QoS is.\n1. the ``QoS Verifier``:  This used to be the heart of the Resilience service.\n   It is now responsible only for verification--that is, determining the state of\n   a file and what needs to be done to make it satisfy its specified QoS.\n   The operations are now maintained in a database for reliable survival on restart, and\n   the component assures that enough iterations over the file will occur\n   to complete all the necessary QoS transformations.\n1. the ``QoS Adjuster``:  This is the component that carries out any QoS\n   changes necessary for a given file.  Each adjustment represents\n   a request made by the verifier to do a single thing: change a replica's sticky bit,\n   make a copy, flush the file to an HSM, or stage a file back that is missing\n   from disk.\n1. the ``QoS Scanner``:  is responsible for observing all the pools in the installation\n   and running periodic checks on the files they contain to make sure QoS requirements are\n   met; it also responds, as did Resilience, to pool status changes and pool manager configuration\n   changes, so as to create missing replicas or cache excess ones, and in general maintain\n   the file's QoS, when necessary.\n\n![QoS Component Services](images/qos-components.png)\n\nEach of these services can be run separately as cells in a single domain or in independent\ndomains.  In addition, there is available a singleton/standalone cell configuration\nin which all four components are directly plugged into each other.  The latter eliminates the\nextra overhead of message passing, but requires more memory.   This flexibility of setup options\nshould help in overcoming hardware restrictions or in isolating the components which need,\nfor example, to run over the namespace database (the scanner) or which\nrequire the most heap space (the verifier).\n\nThese services are not yet HA (replicable), but we plan to provide that capability soon.\n\n### Database back-up\n\nResilience ran without an RDBMs backup, using a (lossy) checkpoint file to mitigate long latency\nin recovery (via pool scanning) when the service goes down and restarts.  With QoS, we have\ndecided to back-up the verification operations using a database table.  This not only affords\nmore reliability on restart, but also allows us to cut down the memory footprint of the\nverifier component.\n\nIt is therefore necessary to initialize the verifier database before starting dCache (the\nexample below assumes PostgreSql is installed and the database username is 'dcache'):\n\n```\ncreatedb -U dcache qos\n```\n\nNo further configuration is necessary, as the table will then be automatically built via\nLiquibase on startup.  As usual, the property\n\n```\nqos-verifier.db.host=${dcache.db.host}\n```\n\nor\n\n```\nqos.db.host=${dcache.db.host}\n```\n\nshould be set to point to the host where the PostgreSql instance is running.\n\n### Activating qos\n\nThe service can be run out of the box.  When deployed as separate cells in one or more\ndomains,\n\n\n```\n[qosDomain]\ndcache.java.memory.heap=8192m\n[qosDomain/qos-engine]\n[qosDomain/qos-verifier]\n[qosDomain/qos-adjuster]\n[qosDomain/qos-scanner]\n```\n\nor\n\n```\n[qosVDomain]\ndcache.java.memory.heap=8192m\n[qosVDomain/qos-verifier]\n\n[qosEDomain]\n[qosEDomain/qos-engine]\n\n[qosADomain]\n[qosADomain/qos-adjuster]\n\n[qosSDomain]\nchimera.db.host=<host-where-chimera-runs>\n[qosSDomain/qos-scanner]\n```\n\nthe default value of ``dcache.service.qos`` (**qos-engine**) pertains and does not\nneed to be modified. For the singleton service,\n\n```\n[qosDomain]\ndcache.java.memory.heap=16384m\n[qosDomain/qos]\n```\n\nset the service property to **qos**:\n\n```\ndcache.service.qos=qos\n```\n\nWith respect to the example scanner domain: the scanner communicates directly with Chimera,\nso `chimera.db.host` should be set explicitly if it is not running on the same host\nas the database.\n\n> NOTE:  As of dCache 9.2, the singleton configuration has been removed.  Each of the\n> separate services can be run together in one domain or in separate domains, but it is\n> no longer possible to run the entire QoS component suite as a single service.\n\n### Memory requirements\n\nWhile it is possible to run QoS in the same domain as other services,\nmemory requirements for the verification component should be a bit more generous.\nWhile it no longer requires the amount of heap space that resilience does, it still\ntries to cache a good proportion of the current operations in memory.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n BEST PRACTICE: We recommend at least 8GB of JVM heap be allocated.\n                Be sure to allow enough memory for the entire domain.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhenever feasible, it is also recommended to give QoS its own domain(s).\n\n## QoS Definitions\n\nThe following table summarizes how quality of service is currently implemented.\n\n![QoS Definitions](images/qos-static.png)\n\nThese are what you might call the file defaults, determined, as they were for Resilience,\nby directory tags and storage unit definitions.  The file receives them when it is\nwritten to a particular directory.   See further under the section \"QoS and Resilience\" on\nhow to emulate the behavior of the Resilience service in QoS.\n\n### Changing a file's QoS\n\nThis is currently limited only to transitions involving ``Access Latency`` and ``Retention Policy``.\nThe number of copies or partitioning between types of pools is not currently dynamically modifiable\non a file-by-file basis.  Further, QoS currently only supports a single tape/HSM location.\n\nThe following table lists the available transitions and should help to clarify\nthe effect of QoS modification:\n\n![QoS Transitions](images/qos-available-transitions.png)\n\nThe transition ``tape=>disk+tape`` is probably the most useful from the standpoint\nof user initiated activity, because it corresponds to \"bring on line\" or \"stage\" when\nthe file is not on disk already.  Note, however, that this transition is permanent,\nin the sense that the ``ACCESS LATENCY`` for the file is modified and the replica\nis indefinitely set to \"sticky\".  To change the file back to cached, a second\nmodification request is required. In the future, this may be done via a time-bound\nset of rules given to the engine (not yet implemented).\n\nFile QoS modification can be achieved through the RESTful frontend, either\nfor single files using `/api/v1/namespace`, or in bulk, using `/api/v1/bulk-requests`,\nthe latter communicating with the [dCache Bulk Service](config-bulk.md).  Please\nrefer to the SWAGGER pages at (`https://example.org:3880/api/v1`) for a description\nof the available RESTful resources.  Admins can also submit and control bulk qos\ntransitions directly through the admin shell commands for the Bulk service.\n\n#### QoS file policy (since 9.2)\n\nWith version 9.2, a \"rule engine\" capability has been added to the QoS Engine.  The\nway this works is as follows:\n\n1. A QoS Policy is defined (it is expressed in JSON).\n2. The policy is uploaded through the Frontend REST API (`qos-policy`).  This stores the policy\n   in the namespace.  The API also allows one to list current policies, view a policy's JSON,\n   and remove the policy.  Adding and removal require admin privileges.\n3. Directories can be tagged using the `QosPolicy` tag, which should indicate the\n   name of the policy.  All files written to this directory will be associated with this policy.\n4. The policy defines a set of transitions (media states), each having a specific duration.\n   The QoS Engine keeps track of the current transition and its expiration.  Upon expiration,\n   it consults the policy to see what the next state is, and asks the QoS Verifier to apply it.\n   When a file has reached the final state of its policy, it is no longer checked by the QoS Engine;\n   however, if the file's final state includes `ONLINE` access latency, the QoS Scanner will\n   check it during the periodic online scans; on the other hand, if the file's final state\n   includes `NEARLINE` access latency, but its retention policy is `CUSTODIAL`, the QoS Scanner\n   will check to make sure it has a tape location.  *_Note that there is no requirement for a file\n   in dCache to have a QoS policy._*\n5. The Bulk service's `UPDATE_QOS` activity now allows for transitioning files both by\n   `targetQos` (`disk`, `tape`, `disk+tape`), but also by `qosPolicy` (associate with the\n   file with a policy by this name); in addition, it is possible to skip transitions in that\n   policy using the `qosState` argument to indicate which index of the transition\n   list to begin at (0 by default).\n\nFor more information on policies, with some examples, see the QoS Policy cookbook.\n\n### QoS and \"resilience\"\n\nThe QoS service continues to provide disk resident \"resilience\".  This capability is\nnow conceived of as a specialized case of QoS handling.  It is still defined\nsystem-wide, but eventually this will be augmented by file-based policies.\n\nThe former Resilience service required the following to be in force in order for it\nto act upon a given file:\n\n* have an `AccessLatency` of `ONLINE`\n* belong to a storage unit where the `required` (copies) attribute is set\n* be written to a pool which is a member of at least (but no more than)\n  one pool group marked as ``-resilient``\n\nIn QoS, these requirements have been loosened.  QoS\nno longer restricts itself to looking only at specific pool groups, in the sense\nthat a storage unit which requires more than one replica will be respected and\nhandled whether it is linked to a group which has been marked\n\"resilient\" or not.  In the case that the pool where the file is initially written\ndoes not belong to a single resilient group, a pool is selected\nfrom the global set of pools.\n\nIt is still possible to mark pool groups as preferential (see below).\nWhile a given pool can belong to any number of pool groups, it must still belong\nto only one such preferential group (an exception will be raised when violation of this rule\nis discovered). We have kept this concept of preferential pool group because it can\nbe useful in assuring that copies always land on certain pools, if this is desired.\n\nIt further goes without saying that pools continue to be capable of\nhosting files with different QoS requirements (disk, tape, both) and that\nsegregation or partitioning by type is possible but not necessary.\n\n### Setting up \"resilience\" using the QoS components\n\nThis differs little from what was done for the Resilience service, with\nthe first point now being **optional**:\n\n1.  (Define one or more resilient pool groups).\n\n2.  Define one or more storage units with ``-required`` specified\n    (optionally linking them to a preferential group).\n\n3.  Create the namespace directories where these files will reside, with the\n    necessary tags (for ``ACCESS LATENCY``, ``RETENTION POLICY`` and storage unit).\n\nNote that if a file is ``ONLINE REPLICA``, this is sufficient\nto raise an alarm if its pool goes offline\n(i.e., its storage unit need not also specify ``-required=1``).  This is\ndifferent from before, where online files without the storage unit attribute\ndefined would have been ignored by Resilience.\n\nQoS also periodically checks to make sure that ``CUSTODIAL ONLINE`` files whose\npermanent disk copy may have been erroneously or accidentally purged are staged\nback in (see further below).\n\n#### Defining preferential (\"resilient\") pool groups\n\nThe following definitions in `poolmanager.conf` are now equivalent:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\npsu create pgroup persistent-group -resilient\n\npsu create pgroup persistent-group -primary\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe have kept `-resilient` for backward compatibility, but in the larger QoS context\nit makes a bit more sense simply to call such a group \"primary\".\n\n#### Defining disk requirements via storage unit\n\nThe two attributes which pertain to disk-resident replication are unchanged:\n\n1.  `required` defines the number of copies files of this unit should receive\n    (default is 1).\n\n2.  `onlyOneCopyPer` takes a comma-delimited list of pool tag names; it\n    indicates that copies must be partitioned among pools such that each replica\n    has a distinct value for each of the tag names in the list (default is\n    undefined).\n\nTo create, for instance, a storage unit requiring two copies, each on\na different host, one would do something like this:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\npsu create unit -store test:persistent@osm\n\npsu set storage unit test:persistent@osm -required=2 -onlyOneCopyPer=hostname\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example assumes that the pool layout has also configured the pools with the\nappropriate `hostname` tag and value.\n\n#### Configuration Example\n\nThe following demonstrates the setup for a single pool, primary pool group and\nstorage unit. The usual procedure for linking pools, pool groups and units\ncontinues to apply.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\npsu create unit -store test:persistent@osm\npsu set storage unit test:persistent@osm -required=2 -onlyOneCopyPer=hostname\n\npsu create ugroup persistent-units\npsu addto ugroup persistent-units test:persistent@osm\n\npsu create pool persistent-pool1\n\npsu create pgroup persistent-pools -primary\npsu addto pgroup persistent-pools persistent-pool1\n\npsu create link persistent-link persistent-units ...\npsu addto link persistent-link persistent-pools\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#### Setting the directory tags\n\nTo continue with the above example, the tags which would be minimally required\nin the directories pointing to this storage unit are:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.(tag)(AccessLatency): ONLINE\n.(tag)(sGroup): persistent\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n> **WARNING**\n>\n> Be careful with large file store configuration settings.\n>\n> These are in the process of being deprecated, and can interfere with QoS.\n>\n> In particular,\n>\n> ```ini\n> pool.lfs=volatile\n> ```\n>\n> should not be set on pools where you wish permanent disk copies to reside\n> or where files are to be flushed.\n>\n> If a file which is ```NEARLINE REPLICA``` (i.e., volatile) lands on this\n> pool, and is then transitioned at some point to disk (```ONLINE```), the\n> ``lfs`` setting is invisible to QoS, which will not know enough to\n> migrate the file to another pool; thus the effect of the transition\n> will be negated because the system sticky flag is suppressed.\n\n## QoS home\n\nOn the host(s) where the QoS services are running, you will see several files\nor subdirectories in the `qos.home` directory (`/var/lib/dcache/qos` by default):\n\n**qos-engine**\n\n- backlogged-messages\n- engine-statistics-<date-hour>\n\n**qos-verifier**\n\n- backlogged-messages\n- verifier-statistics-<date-hour>\n\n**qos-adjuster**\n\n- backlogged-messages\n- adjuster-statistics-<date-hour>\n\n**qos-scanner**\n\n- backlogged-messages\n- excluded-pools\n- scanner-statistics-<date-hour>\n\nExplanations of these files follow.\n\n#### backlogged-messages (all components)\n\nIt is possible to enable and disable message handling by the QoS components using the\ncomponent's ``ctrl start/shutdown`` command (for the ``verifier, adjuster`` and ``scanner``),\nor ``enable/disable`` for the ``engine``. Any messages which happen to arrive during the period\nwhen the component is disabled or shut down will be serialized to a file in this directory,\nand then read back in and deleted during enabling/initialization. Disabling message handling\nin the ``engine`` with the ``-drop`` option means all incoming messages are discarded.\n\n#### {component}-statistics-{date-hour} (all components)\n\nThrough an admin command, one can enable and disable the recording of\nstatistical data for each of the four components. When enabled, writing to this file\nis activated.  This will contain detailed timestamped records and timings\nspecific to that component (e.g., of messages, operations, or tasks), written out at 1-minute\nintervals; this logging rolls over every hour until disabled.\n\nThe file can be read from the admin interface by issuing the ``stat`` command\nwithout options.\n\n#### excluded-pools (qos-scanner)\n\nWhen the command `pool exclude <pools>` is used, the pool operation record is\nmarked as `EXCLUDED`. This can be observed when the `pool ls` command is issued.\nPools so marked are recorded to this file after each pool monitor state update\n(every 30 seconds). This file is read in, again if it exists, at startup, so\nthat previously excluded pools will continue to be treated as such.\n\nFor pool exclusion, see the [section below](config-qos-engine.md/#exclude-a-pool-from-qos-handling)\nunder typical scenarios.\n\n## Admin Commands\n\nThere are a number of ways to interact with the qos service through the\nadmin door. As usual, the best guide for each command is to consult its help\ndocumentation (`\\h <command>`).  Tasks are marked according to the component they\nrelate to:  ``engine``, ``pool`` (scanner), ``task`` (adjuster) and ``verify``.  The\n``async`` commands are part of the scanner component and are used to control\ndatabase queries represented by the ``contained`` command that are run asynchronously\nbecause they take a bit of time to complete.\n\nThese commands will not be discussed in detail here. We instead present a brief\noverview and a few examples of output.\n\n### Available actions\n\n```\nqos-adjuster:\n    info [-a] [-l]  # get detailed cell information\n    task cancel [OPTIONS] [<pnfsids>]  # cancel adjuster tasks\n    task ctrl [OPTIONS] [<start>|<shutdown>|<reset>|<run>|<info>]  # control  handling of tasks\n    task details # list diagnostic information concerning tasks by pool\n    task history [OPTIONS] [errors]  # display a history of the most recent terminated file operations\n    task ls [OPTIONS] [<pnfsids>]  # list entries in the adjuster task table\n    task stats [OPTIONS]  # print diagnostic statistics history\nqos-engine:\n    disable [-drop]  # turn off handling\n    enable # turn on handling\n    engine stats [-enable] [-limit=<integer>] [-order=asc|desc]  # print diagnostic statistics\n    info [-a] [-l]  # get detailed cell information\nqos-scanner:\n    async cmd cancel <ids>  # cancel running scans/queries\n    async cmd cleanup [-entry] [-file] <ids>  # remove future entries and/or file\n    async cmd ls # print out a list of running scans/queries\n    async cmd results [OPTIONS] <filename>  # print to screen asynchronous query/scan results\n    contained in <poolexpression>  # count or list pnfsids which have replicas only on the pools in the list\n    info [-a] [-l]  # get detailed cell information\n    pool cancel [OPTIONS] [regular expression for pools]  # cancel pool operations\n    pool details # list diagnostic information concerning scan activity by pool\n    pool exclude [OPTIONS] [<pools>]  # exclude pool operations\n    pool include [OPTIONS] [<pools>]  # include pool operations\n    pool ls [OPTIONS] [<pools>]  # list entries in the pool operation table\n    pool reset [OPTIONS]  # control the processing of pool state changes\n    pool scan [OPTIONS] <pools>  # launch a scan of one or more pools\n    pool stats [-enable] [-limit=<integer>] [-order=asc|desc]  # print diagnostic statistics\n    sys cancel [-id=<string>] [-nearline] [-online]  # cancel background scan operations\n    sys ls [-history] [-order=ASC|DESC]  # list entries in the system scan operation table\n    sys reset [OPTIONS]  # control the periodic check for system scans\n    sys scan [-nearline] [-online]  # initiate an ad hoc background scan.\nqos-verifier:\n    info [-a] [-l]  # get detailed cell information\n    pool info [OPTIONS] [<pools>]  # list tags and mode for a pool or pools\n    verify <pnfsids>  # launch an operation to verify one or more pnfsids\n    verify cancel [OPTIONS] [<pnfsids>]  # cancel verify operations\n    verify details # list diagnostic information concerning verification by pool\n    verify failed # launch operations to rerun verify for all pnfsids currently appearing in the history errors list\n    verify history [OPTIONS] [errors]  # display a history of the most recent terminated operations\n    verify ls [OPTIONS] [<pnfsids>]  # list entries in the operation table\n    verify reset [OPTIONS]  # control the settings for operation handling\n    verify stats [-enable] [-limit=<integer>] [-order=asc|desc]  # print diagnostic statistics\n```\n\n### Info output\n\nOne of the most useful commands is `info`, for diagnostics, viewed all at once\nby doing ``\\s qos-* info``):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (local) admin > \\s qos-* info\nqos-adjuster:\n    --- adjuster-task-map (In-memory queues for running, ready and waiting adjustment tasks.) ---\n    Running since: Fri Apr 29 13:54:15 CDT 2022\n    Uptime 0 days, 0 hours, 0 minutes, 42 seconds\n\n    Last sweep at Fri Apr 29 13:54:52 CDT 2022\n    Last sweep took 0 seconds\n\n    maximum running tasks:     200\n    maximum number of retries: 1\n\n    ACTION                            TOTAL       FAILED\n    CACHE_REPLICA                         0            0\n    COPY_REPLICA                          0            0\n    FLUSH                                 0            0\n    PERSIST_REPLICA                       0            0\n    UNSET_PRECIOUS_REPLICA                0            0\n    WAIT_FOR_STAGE                        0            0\nqos-engine:\n    --- file-status-handler (Processes incoming request messages.) ---\n    Running since: Fri Apr 29 13:54:15 CDT 2022\n    Uptime 0 days, 0 hours, 0 minutes, 42 seconds\n\n\n    MSG TYPE                       RECEIVED\n    ADD_CACHE_LOCATION                    0\n    CLEAR_CACHE_LOCATION                  0\n    CORRUPT_FILE                          0\n    QOS_ACTION_COMPLETED                  1\n    QOS_MODIFIED                          0\n    QOS_MODIFIED_CANCELED                 0\nqos-scanner:\n    --- pool-operation-map (Map of pool-specific operations which run location scans in response to state changes.) ---\n    down grace period 1 HOURS\n    restart grace period 6 HOURS\n    maximum idle time before a running operation is canceled  1 HOURS\n    maximum concurrent operations 5\n    period set to 3 MINUTES\n\n    Running since: Fri Apr 29 13:54:16 CDT 2022\n    Uptime 0 days, 0 hours, 0 minutes, 41 seconds\n\n    Last sweep at Fri Apr 29 13:54:16 CDT 2022\n\n    ACTION                       COMPLETED       FAILED\n    POOL_STATUS_DOWN                     0            0\n    POOL_STATUS_UP                       0            0\n\n    --- system-operation-map (Map of system-wide operations which run scans periodically.) ---\n    system online scan window 24 HOURS\n    system nearline scan is off\n    system nearline scan window 5 DAYS\n    max concurrent operations 5\n    period set to 3 MINUTES\n\n    Running since: Fri Apr 29 13:54:16 CDT 2022\n    Uptime 0 days, 0 hours, 0 minutes, 41 seconds\n\n    Last sweep at Fri Apr 29 13:54:16 CDT 2022\n\n    last online scan start Fri Apr 29 13:54:16 CDT 2022\n    last online scan end Fri Apr 29 13:54:16 CDT 2022\n\n        0 days, 0 hours, 0 minutes, 0 seconds\n\n    last nearline scan start Fri Apr 29 13:54:16 CDT 2022\n    last nearline scan end Fri Apr 29 13:54:16 CDT 2022\n\n        0 days, 0 hours, 0 minutes, 0 seconds\nqos-verifier:\n    --- verify-operation-map (Provides an in-memory API for accessing verification operations.) ---\n    maximum concurrent operations 200\n    maximum number of ready operations in memory 10000\n    maximum retries on failure 1\n\n    sweep interval 1 MINUTES\n\n    MESSAGES                              RECEIVED\n    ADJUST RESPONSE                              0\n    BATCH CANCEL REQUEST                         0\n    BATCH VERIFY REQUEST                         0\n    CANCEL REQUEST                               0\n    LOCATION EXCLUDED                            0\n    VERIFY REQUEST                               0\n\n    Last sweep at Fri Apr 29 13:54:22 CDT 2022\n\n    OPS                                  COMPLETED   OPS/SEC       FAILED\n    ALL                                          0         0            0\n    VOIDED                                       0         0            0\n\n\n    ACTIVE OPERATIONS BY STATE:\n        CANCELED                               1\n\n    ACTIVE OPERATIONS BY MSG_TYPE:\n        QOS_MODIFIED                           1\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe other useful commands are ``verify ls`` and ``task ls``, ``verify history``\nand ``task history``, ``verify details`` and ``task details.``  These can be used\nto give an idea of the running state of QoS: what operations are ongoing (``ls``),\nthe most recent ones to have completed (and any attendant errors, ``history [errors]``),\nas well as pool-by-pool summaries (``details``). See the help for each of these commands\nfor the various options available.\n\nSimilar commands, `pool ls` and ``pool details``, exist for checking the current status\nof any pool scan operations:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (qos@qosDomain) admin > pool ls\n[fndcatemp2] (qos-scanner@qosSDomain) admin > pool ls\ndcatest03-1\t(completed: 0 / 0 : ?%) - (updated: 2022/04/29 13:54:52)(finished: 2022/04/29 13:54:52)(prev UNINITIALIZED)(curr UNINITIALIZED)(IDLE)\ndcatest03-2\t(completed: 0 / 0 : ?%) - (updated: 2022/04/29 13:54:52)(finished: 2022/04/29 13:54:52)(prev UNINITIALIZED)(curr UNINITIALIZED)(IDLE)\ndcatest03-3\t(completed: 0 / 0 : ?%) - (updated: 2022/04/29 13:54:52)(finished: 2022/04/29 13:54:52)(prev UNINITIALIZED)(curr UNINITIALIZED)(IDLE)\ndcatest03-4\t(completed: 0 / 0 : ?%) - (updated: 2022/04/29 13:54:52)(finished: 2022/04/29 13:54:52)(prev UNINITIALIZED)(curr UNINITIALIZED)(IDLE)\n...\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nand for seeing a summary breakdown of scan activity by pool:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (qos@qosDomain) admin > pool details\nNAME                      |        TOTAL       FAILED |           UP         DOWN       FORCED |        FILES          AVGPRD (ms)\ndcatest03-1               |            1            0 |            1            0            1 |            0               152077\ndcatest03-2               |            1            0 |            1            0            1 |            0               152056\ndcatest03-3               |            1            0 |            1            0            1 |            0               152077\ndcatest03-4               |            1            0 |            1            0            1 |            0               152033\n...\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor each operation in the ``ls`` output, the timestamps of the last update\n(change in status) and of the last completed scan are indicated, as well as\npool status (`ENABLED` here), and operation state (`IDLE`, `WAITING`, `RUNNING`).\nFor running scans, the number of file operations completed out of a total (if known)\nis reported.\n\nFinally, the `task ctrl` and `verify / pool / sys reset` commands can be used to verify\nor reset basic configuration values, or to interrupt operation processing or\nforce a sweep to run (for the adjuster).\n\n## Tuning\n\nOnly a few properties can be reset using the `ctrl` and `reset` commands mentioned above. Please\nconsult the documentation in the `qos.properties` defaults for a fuller\nexplanation of the tuning issues which pertain. If adjustments to the preset values\nare made, remember to ensure that enough database connections\nremain available to service both Chimera operations and pool scan operations,\nand that these be properly matched to the number of threads responsible for the\nvarious operations, in order to avoid contention (see again, the explanation in\nthe default properties file).\n\n## QoS's View of Pool Status\n\nIn order to allow for flexibility in configuring door access to pools, the\n`disabled` state on a pool is interpreted this way:\n\n-  `\\s <pool> pool disable -p2p-client` means no p2p can be written to this pool;\nQoS will not use this pool to make copies, though doors can still write\nnew files there.\n\n-  `\\s <pool> pool disable -store` means doors cannot write new copies to the\npool, though it is still available for p2p; hence QoS can still use this\npool to make copies.\n\n-  `\\s <pool> pool disable -rdonly` means the pool cannot be written to either\nby doors or QoS.\n\n-  `\\s <pool> pool disable -strict` indicates not only that the pool is disabled\nfor write, but also for read; QoS will schedule it for a scan so that the\nfiles it contains can be replicated elsewhere if necessary.\n\n## Automatic Staging of Missing CUSTODIAL ONLINE replicas\n\nFiles whose `RETENTION POLICY` is `CUSTODIAL` and whose `ACCESS LATENCY` is `ONLINE`\nconstitute a special case for the purposes of recovery.  For instance,\nif a `REPLICA ONLINE` file with two copies on disk becomes inaccessible because\nboth pools containing those copies went offline at about the same time (not\npermitting QoS enough time to react to make another replica), then an alarm\nconcerning this file's current inaccessibility is raised.  If this file is\n`CUSTODIAL` and has a copy on tertiary storage, however, QoS will first attempt to\nrestage it before considering it inaccessible.\n\n### Pool scan vs Sys scan\n\nFor the scanner component, there are two kinds of scans.  The pool scan runs a query\nby location (= pool) and verifies each of the ``ONLINE`` files that the namespace indicates is\nresident on that pool.  This is generally useful for disk-resident replicas, but\nwill not be able to detect missing replicas (say, from faulty migration, where the\nold pool is no longer in the pool configuration). Nevertheless, a pool scan\ncan trigger the restaging of an ``ONLINE CUSTODIAL`` file that has no readable\nor available replicas.\n\nThe system scan (sys), on the other hand, verifies all files, whatever\ncache locations they may or may not have.  These scans are more costly and can run for days,\nso they are separate from the pool scans and run in the background.  Given that\nthe more usual dCache deployments will have NEARLINE >> ONLINE files, the nearline scan\nis disabled by default, but can be enabled through the ``reset`` command.  ``NEARLINE``\nscans are only useful in catching a qos modification which has changed the status\nof a file and, for instance, requires a flush or a caching which has not occurred.\nThe ``ONLINE`` scan is more useful in that it will detect if such a file is missing a disk\ncopy, regardless of the current available pools, and will stage it back in if it is also\n``CUSTODIAL``.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n SCANNING, QOS vs Resilience\n\n Formerly (in resilience), individual pool scans were both triggered by pool state changes\n and were run periodically; in QoS, they are still triggered by state changes\n (or by an explicit admin command), but there is an option as to how to run ONLINE scans\n periodically.  By enabling 'online' scans (the default), the sys scans will\n touch each file in the natural order of their primary key in the namespace.\n The advantage to this is avoiding scanning the same file more than once if\n it has more than one location.  The disadvantage is that files whose locations\n are currently offline or have been removed from the dCache configuration will\n trigger an alarm.  If 'online' is disabled, the old-style pool scan (more properly,\n location-based scan) will be triggered instead.  This will look at only ONLINE\n files on IDLE pools that are ENABLED, but will end up running redundant checks\n for files with multiple replicas.\n\n With the advent of the rule engine (9.2), the NEARLINE scan has been limited to\n files with a defined qos policy.\n\n NEARLINE is no longer turned off by default, since it no longer necessarily\n encompasses all files on tape, but just the ones for which the policy state\n currently involves state.  Of course, if the majority of files in the dCache\n instance have a policy, then this scan will again involve a much longer run-time\n and thus the window should be adjusted accordingly.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n----------------------\n\n**_The following sections have been carried forward from previous Resilience documentation.\nMost of the same situations and principles continue to apply to QoS inasmuch\nas it is still used to maintain system-wide resilience consistency._**\n\n----------------------\n\n## Some typical scenarios part 1: what happens when ...?\n\n### QoS is initialized (service start)\n\nShould the service go offline, nothing special occurs when it is\nrestarted. That is, it will simply go through the full re-initialization\nprocedures.\n\nInitialization steps are reported at the logging `INFO` level, and can be\nobserved from the pinboard:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n17 Nov 2020 13:06:13 [qos-0] [] Adjuster task map initialized.\n17 Nov 2020 13:06:27 [qos-0] [PoolManager] PoolOpChangeHandler: Received pool monitor update; enabled true, initialized false\n17 Nov 2020 13:06:27 [qos-0] [PoolManager] PoolInfoChangeHandler: Received pool monitor update; enabled true, initialized false\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Waiting for pool monitor refresh notification.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Loading pool operations.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Getting mapped pools.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Eliminating old pools from running.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Eliminating old pools from waiting.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Eliminating old pools from idle.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Adding pools.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] loading excluded pools.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Pool maps reloaded; initializing ...\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Pool maps initialized; delivering backlog.\n17 Nov 2020 13:06:43 [Consumer] [] Backlogged messages consumer exiting.\n17 Nov 2020 13:06:43 [pool-11-thread-1] [] Starting the periodic pool monitor refresh check.\n17 Nov 2020 13:06:43 [pool-19-thread-1] [] Waiting for pool monitor refresh notification.\n17 Nov 2020 13:06:43 [pool-19-thread-1] [] Received pool monitor; loading pool information.\n17 Nov 2020 13:06:44 [pool-19-thread-1] [] Pool maps initialized.\n17 Nov 2020 13:06:44 [pool-19-thread-1] [] Messages are now activated; starting file operation consumer.\n17 Nov 2020 13:06:44 [Reloader] [] Done reloading backlogged messages.\n17 Nov 2020 13:06:44 [pool-19-thread-1] [] File operation consumer is running; activating admin commands.\n17 Nov 2020 13:06:44 [pool-19-thread-1] [] Starting the periodic pool monitor refresh check.\n17 Nov 2020 13:06:44 [pool-19-thread-1] [] Admin access enabled; reloading checkpoint file.\n17 Nov 2020 13:06:44 [pool-19-thread-1] [] Checkpoint file finished reloading.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPoolMonitor state is initially pulled from PoolManager, and thereafter refreshed\nevery 30 seconds by a push from PoolManager to its \"subscribers\" (among which\nthe verifier and scanner). In the ``qos-verifier``, once the initial monitor\nstate is received, pool information is also parsed and loaded into a local data\nstructure (accessed via the `pool info` command). In the ``qos-scanner``, the pool\noperations table is built (accessed via the `pool ls` command).\nThe `excluded-pools` file is also reloaded at this point.\n\n#### What exactly does `UNINITIALIZED` mean for a pool?\n\nIn addition to the usual pool status types (`ENABLED`, `READ_ONLY`, `DOWN`),\n`UNINITIALIZED` serves to indicate incomplete information on that pool from the\nPoolMonitor. The transition from `UNINITIALIZED` to another state occurs when\nthe ``qos-scanner`` comes on line, whether simultaneously with pool initialization\nor not.\n\n>   **The probability of pools going down at initialization is low, but it is\n>   possible that the scanner could go down and then restart to find a number of\n>   pools down. In this case, the** `DOWN` **pools will be handled as usual (see\n>   below). On the other hand, it is preferable not to treat the transition to**\n>   `READ_ONLY` **/**`ENABLED` **as a \"restart\" when coming out of**\n>   `UNINITIALIZED`**, since the majority of pools will most of the time\n>   initialize to a viable readable status, and handling this transition would\n>   unnecessarily provoke an immediate system-wide pool scan.**\n\nDuring this phase of initialization, message handling is still inactive; hence\nany incoming messages are temporarily cached. When the pool info and operation\ntables have been populated, the backlogged messages are handled. At the same\ntime, message handling is also activated. Up until this point, issuing any admin\ncommands requiring access to qos state will result in a message that the\nsystem is initializing:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (qos@qosDomain) admin > pool ls\nService is not yet initialized; use 'show pinboard' to see progress.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAfter this point, admin commands become viable and the pool information is\nperiodically refreshed (every 30 seconds, in response to the PoolMonitor\nmessage). Changes in pool state or pool group and storage unit composition are\nrecorded and appropriate action, if necessary, is taken.\n\n### A pool goes down or is reported as dead\n\nQoS considers pools viable until they become unreadable. If a pool is\nread-only, its files will still be counted as accessible replicas, and will be\nused as potential sources for copying (naturally, the pool is excluded as target\nfor new copies). Once a pool becomes entirely disabled, the scanner will mark it\nas `DOWN` and queue it for an eventual scan. The scan will be activated as soon\nas the expiration of the grace period for that pool has been detected by the\npool \"watchdog\" (which wakes up every three minutes by default).\n\n>   **When we speak of \"scanning\" a pool which is down or inaccessible, this is\n>   shorthand for saying that the scanner runs a query against the namespace to\n>   find all the** `pnfsids` **at the** `location` **representing that pool.\n>   No actual interaction with the pool takes place.**\n\nOnce a scan is completed on a dead pool, no more scans will be done on it until\nits state changes.\n\nUnder most circumstances, no intervention should be required. This is part of\nthe normal functioning of the service.\n\n### A pool is re-enabled or brought back on line\n\nThis is just the counterpart to the previous scenario. When a pool is brought\nback on line, the service queues it for eventual scanning, once again activated\nas soon as the expiration of the grace period is detected. Note that the grace\nperiods for pool down and pool restart are independent properties.\n\n>  **If a scan on the pool is currently running when the pool comes back up, it will\nbe immediately canceled and a new scan rescheduled.**\n\nNote that QoS always does the same thing on a scan, whether triggered by\na change from `UP` to `DOWN` or vice versa: it checks the QoS of each file,\ncompares it to its requirements, and takes the appropriate action.\n\n### Several pools go off line simultaneously\n\nEach will be treated as above. By default, five scans are allowed to run\nsimultaneously. If there are more scans than five, they will continue in the\n`WAITING` state until a slot becomes available.\n\nScans can take some time to complete, depending on how big the pool is. If there\nis a `WAITING` scan which has not run yet when the periodic window for the scan\nexpires, it will simply remain queued until it finally runs.\n\n#### Why don't I see a value for the 'total' on the admin `pool ls` output for a `RUNNING` pool operation?\n\nWhen a pool is scanned, a query is run against Chimera to collect all the\n`pnfsids` of files on the given pool. The scanning processes the results from\nthe query via a cursor, so each file is checked and queued for treatment if necessary,\nin order. This looping through the results can very easily take longer than it does\nfor some of the operations it has created to be run and to complete. But the\ntotal number of files to process is only known at the end of this loop. So it is possible\nto see a value for the number of files processed so far before seeing the total.\nObviously, only when the total exists can the % value be computed.\n\n## Some typical scenarios part 2: how do I ...?\n\nDuring the normal operation a dCache installation, changes to the number of pools,\nalong with the creation and removal of storage classes or pool groups, will undoubtedly\nbe necessary. The following describes the steps to take and what response to expect from\nthe QoS service in each case.\n\n### Add a pool to a group\n\nLet us assume that we have some new disk space available on node `dmsdca24`, and\nthat we want to use it to host some pools. Of course, the first steps are to\nprepare the appropriate partitions for the pools and to create the pool area\n(directory) with the necessary setup file for dCache (see \"Creating and\nconfiguring pools\" in the [Configuring\ndCache](https://www.dcache.org/manuals/Book-2.16/Book-fhs.shtml#in-install-configure)\nsection of the dCache Book). Once we have done that, and have also added the\npool stanza to the layout file\n[(ibid)](https://www.dcache.org/manuals/Book-2.16/Book-fhs.shtml#in-install-configure),\nwe can proceed to add the pool to the `psu` (PoolSelectionUnit):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\sp psu create pool rw-dmsdca24-1 -disabled\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe can then start the pool on the host. It should then appear enabled:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\sp psu ls -a  pool rw-dmsdca24-1\nrw-dmsdca24-1  (enabled=false;active=15;rdOnly=false;links=0;pgroups=0;hsm=[];mode=enabled)\n  linkList   :\n  pGroupList :\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLet us now say that we want to add it to a pool group.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\sp psu addto pgroup res-group rw-dmsdca24-1\n\n\\sp psu ls -a pool rw-dmsdca24-1\nrw-dmsdca24-1  (enabled=false;active=19;rdOnly=false;links=0;pgroups=1;hsm=[];mode=enabled)\n  linkList   :\n  pGroupList :\n    res-group(links=1; pools=35; resilient=true)\n\n\\sp psu ls -a pgroup res-group\nres-group\n resilient = true\n linkList :\n   res-link  (pref=10/10/-1/10;;ugroups=3;pools=1)\n poolList :\n   dmsdca15-1  (enabled=true;active=14;rdOnly=false;links=0;pgroups=1;hsm=[enstore];mode=enabled)\n   dmsdca15-1.1  (enabled=true;active=10;rdOnly=false;links=0;pgroups=1;hsm=[enstore];mode=enabled)\n   ...\n   dmsdca22-8.1  (enabled=true;active=7;rdOnly=false;links=0;pgroups=1;hsm=[enstore];mode=enabled)\n   rw-dmsdca24-1  (enabled=false;active=5;rdOnly=false;links=0;pgroups=1;hsm=[];mode=enabled)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAfter the next monitor refresh, ``qos-verifier`` and ``qos-scanner`` should both\nshow this pool as well:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-verifier pool info rw-dmsdca24-1\nkey           84\nname          rw-dmsdca24-1\ntags          {hostname=dmsdca24.fnal.gov, rack=24-1}\nmode          enabled\nstatus        ENABLED\nlast update   Thu Jun 16 09:24:21 CDT 2016\n\n\\s qos-scanner pool ls rw-dmsdca24-1\nrw-dmsdca24-1   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 09:25:42 CDT 2016)(scanned: Thu Jun 16 09:25:42 CDT 2016)(prev ENABLED)(curr ENABLED)(IDLE)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen the pool is added, a scan is scheduled for it, provided it is in an\ninitialized state. In this case, since the pool is empty, the scan completes\nquickly.\n\n### Remove a pool from a group\n\nWhen a pool is removed from a pool group, the pool needs to be scanned because\nthe loss of the replica on that pool may have an impact on the file QoS.\n\nHere we walk through several steps to show QoS put through its paces.\n\nFirst, let us remove a pool from a pool group which has been marked ``-primary``.\nBecause that group is primary, all files from that pool which have replica requirements\nwill have their other copies on other pools in that group.  What we want to\ntest here is that removing a pool from this group triggers the necessary QoS\nresponse.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\sp psu removefrom pgroup res-group rw-dmsdca24-2\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe observe the pool has been queued for a scan:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-scanner pool ls rw-\nrw-dmsdca24-2   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:17:09 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev ENABLED)(curr ENABLED)(WAITING)\nrw-dmsdca24-3   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:14:33 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev UNINITIALIZED)(curr ENABLED)(IDLE)\nrw-dmsdca24-4   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:14:33 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev UNINITIALIZED)(curr ENABLED)(IDLE)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen the scan starts, we can see it is doing some work:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-scanner pool ls rw-dmsdca24-2\nrw-dmsdca24-2   (completed: 491 / ? : ?%) - (updated: Thu Jun 16 17:17:19 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev ENABLED)(curr ENABLED)(RUNNING)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBecause this is a demo pool, there aren't many files, so the scan completes\nnearly simultaneously with the actual copying; hence there is not enough of a\ntime lag to see a \"% complete\" reported.\n\nUpon termination, there is still a record for this pool in the pool operation table.  The\npool will continue there, in fact, until it is actually removed from the pool\nmanager configuration.\n\nNow, just to spot check that everything is right with the world, let's examine\nthe recent operations, pick a `pnfsid`, and find its locations (replicas).\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-verifier verify history\n � [many other files]\n2020/11/17 13:40:12 (00008F22B12729CE458596DE47E00411D68C)(COPY_REPLICA DONE)(parent none, retried 0)\n  � [many other files]\n\n\\sn cacheinfoof 00008F22B12729CE458596DE47E00411D68C\n dmsdca15-1 dmsdca15-1.1 dmsdca16-2.1 rw-dmsdca24-2\n\n\\sn storageinfoof 00008F22B12729CE458596DE47E00411D68C\n size=1;new=false;stored=false;sClass=resilient-4.dcache-devel-test;cClass=-;hsm=enstore;accessLatency=ONLINE;retentionPolicy=REPLICA;path=<Unknown>;group=resilient-4;family=dcache-devel-test;bfid=<Unknown>;volume=<unknown>;location=<unknown>;\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe storage class/unit for this file (`resilient-4.dcache-devel-test@enstore`)\nhappens to require three copies; there are indeed three, plus the no-longer\nvalid location which we just removed from the primary pool group.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWARNING:  If a pool is moved from one primary pool group to another, the\n          replica counts for the added files will be seen in the context\n          of the new group, leading to the creation of the required number of\n          copies on the pools of the new group (without affecting the previous copies).\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nNOTE:     In the case of removing a pool from a normal pool group, additional\n          replicas may or may not be made.  This is because globally replicated\n          files (where replicas are placed on pools without observing any particular\n          pool group constraint) will still count the replica on the removed pool\n          toward their qos requirement.  In this case, it is not until the pool is\n          entirely removed from the pool manager configuration that a new replica\n          will be made elsewhere.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n**A scan is not triggered when a pool is entirely removed from the configuration.**\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n          It is recommended that a pool be disabled first (see below) before\n          removal from the configuration in order to allow QoS to inspect its files\n          and act appropriately.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe removal of a pool from the `psu` using `psu remove pool <name>` will also\nremove the pool from the pool info listing in the ``qos-verifier``; however,\nif the pool keeps connecting (i.e., it has not been stopped), its entry will\ncontinue to appear in the PoolMonitor, and thus also in the internal qos table.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nBEST PRACTICE:  When either removing a pool from a pool group or from the entire\n                configuration, it is best to disable the pool first in order\n                to allow QoS to handle it as `DOWN`; then when the scan completes,\n                proceed with the removal.\n\n\\s rw-dmsdca24-2 pool disable -strict\n\n...\n\n(after the DOWN scan has completed)\n\n...\n\n\\sp psu remove pool rw-dmsdca24-2\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs stated above, removal of pools from non-primary groups will not trigger\na qos response until the pool actually becomes inaccessible.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nNOTE:   As long as at least one replica of a file continues to reside in a primary\n        pool group, action will be taken (if possible) to maintain the proper\n        number of replicas for that file inside the pool group (a single but\n        currently unreadable copy will raise an alarm, however).\n\n        Three consequences follow from this.\n\n        (1)     If pools are removed serially from a primary group, at some point\n                alarms will begin to be generated for files which need more replicas\n                than the number of pools left in the group.\n\n        (2a)    If a file has replicas only on pools which have been removed from\n                a primary group and which have not been added to another primary\n                group, but which still belong to some other group, those replicas\n                will count toward the file's requirments, since they are now\n                \"at large\" (there are no longer pool group residency restrictions).\n\n        (2b)    If a file has replicas only on pools which have been removed from\n                a primary group and which have not been added to another primary\n                group, and which do not belong to any any other group, those\n                files will become invisible to QoS.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt follows from these two observations that if you want to empty a primary group,\nit is advisable to add all its pools to some other temporary non-primary group\nfirst (if they do not already belong to one), and then remove all the pools\nfrom the group at once.\n\nThis avoids two massive alarm situations:  (a) a flood of alarms about replica\nrequirements not being met as you remove pools one-by-one; (b) a flood of\ninaccessible (orphaned) file alarms for ``ONLINE REPLICA`` files if you remove\nall the pools at once.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nBEST PRACTICE:  In removing all pools from a primary pool group (in preparation\n                for removing the group), first make sure that all pools also\n                belong to a non-primary group, then remove all the pools from\n                the primary group simultaneously.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen removing only selected pools from a primary pool group, the `contained in`\ncommand may come in useful. Running that command over a  set of pools to be\ndrained and removed will provide a list of pnfsids for which some sort of manual\naction/migration will be necessary, since they will be alarmed as 'inaccessible'.\n\n### Add or remove a group\n\nThere is nothing special about adding or removing a group. Doing so will register\nthe group inside qos as well, even though at this point it is empty.\nOnce pools are added to this group, the behavior will be as indicated above.\n\n>   **As mentioned above (see the warning under \"Defining a primary group\"),\n>   it is not possible to use the admin shell to demote a primary pool group\n>   to non-primary status.**\n\n### Exclude a pool from qos handling\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nNOTE:  With version 9.0, the use of 'exclude' is deprecated for migration.\n       See below for the new procedure.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDuring normal operation, QoS should be expected to handle gracefully situations\nwhere a pool with many files, for one reason or another, goes offline. Such an incident,\neven if the \"grace period\" value were set to 0, in initiating a large scan,\nshould not bring the system down. Obversely, should the pool come back on line,\nany such scan should be (and is) immediately canceled, and the pool rescheduled for\na fresh scan. So under most circumstances, no manual intervention or adjustment\nshould be required.\n\nNevertheless, temporary exclusion of one or more pools from being handled by the\nscanner may be desirable in a few situations.\n\nThe original dCache Replica Manager provided for marking pools `OFFLINE` (a state\ndistinct from `DOWN` or `disabled`). Such pools were exempted from pool state change\nhandling, but their files continued to \"count\" as valid replicas.\n\nThis feature was held over into the Resilience service and is kept here as well.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-scanner pool exclude <regular expression>\n\\s qos-scanner pool include <regular expression>\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#### When **NOT** to use `pool exclude`\n\nThe `pool exclude` command interferes with the normal operation of the scanner;\nuse in the wrong circumstances may easily lead to inconsistent state.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWARNING:  Only use 'pool exclude' for temporary situations where the intention\n          is eventually to restore the excluded location(s) to qos management;\n          or when the locations on those pools are actually to be deleted\n          from the namespace.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf, for instance, one set a pool to `EXCLUDED`, then removed the pool,\nthe pool would disappear from the pool operation list (`pool\nls`), but QoS would not react to the change in order to compensate for\nthe missing replicas. One would then have to repair the situation manually,\neither by adding the pool back and then removing it correctly, or by manually\nmigrating the files.\n\n`pool exclude` is useful for doing manual migrations on primary pool groups,\nbut caution should be taken when applying it.\n\n_Note that once a pool is excluded, it can no longer be scanned, even manually,\nuntil it is explicitly included again._\n\n### Rebalance a pool (group)\n\nRebalancing should be required less often on pools belonging to a primary pool\ngroup; but if you should decide to rebalance this kind of pool group, be sure\nto disable qos on all those pools. One could do this by stopping qos altogether,\nbut this of course would stop the processing of other groups not involved\nin the operation. The alternative is to use the `exclude` command one or more times\nwith expressions matching the pools you are interested in:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-scanner pool exclude <exp1>\n\\s qos-scanner pool exclude <exp2>\n...\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNote that the exclusion of a pool will survive a restart of the service because\nexcluded pools are written out to a file (`excluded-pools`; see above) which is\nread back in on initialization.\n\nWhen rebalancing is completed, pools can be set back to active\nqos control:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-scanner pool include .*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n(Note that only `EXCLUDED` pools are affected by the `include` command.)\nRe-inclusion sets the pools back to `IDLE`, and does not schedule them\nautomatically for a scan, so if you wish to scan these pools before the periodic\nwindow elapses, a manual scan is required.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nBEST PRACTICE:  Disable qos on the potential source and target pools\n                by setting them to EXCLUDED before doing a rebalance.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n### Migrating files off of a pool (dCache 9.0+)\n\nInstead of the clumsy and less reliable procedure involved in rebalancing above,\nQoS can now handle the copying of all persistent replicas to other pools\n(whether of a primary/resilient pool group or globally).\n\nTo achieve this, do the following:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s <pool-to-decommission> pool disable -draining\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou will then see that pool scheduled for a scan as if it were in the `DOWN`\nstate.  There will be, however, no issues with using the replicas on that\npool as source for a new copy, because the new pool state `DRAINING` is\nthe same as `READ_ONLY` (`-rdonly`), but with an extra bit to alert QoS to\ntreat it as if it were offline.\n\nWhen the scan has completed, all persistent (but not cached) replicas will\nhave been replicated on other pools, thus leaving the source pool free to\nbe taken offline or to be manually purged of its replicas.  One could even\nconceivably `rep rm -force` all replicas on it, and set it back to enabled,\nwith no issues arising for QoS replica management.\n\n### Manually schedule or cancel a pool scan\n\nA scan can be manually scheduled for any pool, including those in the `DOWN` or\n`EXCLUDED` states.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-scanner pool scan dmsdca18-2\nScans have been issued for:\ndmsdca18-2\ndmsdca18-2.1\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNote that if a pool is in the `WAITING` state as a result of a pool status\nchange from `UP` to `DOWN` or vice versa, calling `pool scan` on it will\noverride the grace period wait so that it will begin to run the next time the\nwatchdog wakes up.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nNOTE:  One can override the grace period for a waiting scan by calling 'pool scan'.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOne could of course also change the global setting for the grace periods using\n`pool reset`:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-scanner pool reset\ndown grace period 6 HOURS\nrestart grace period 6 HOURS\nmaximum concurrent operations 5\nscan window set to 24 HOURS\nperiod set to 3 MINUTES\n\n\\s qos-scanner pool reset -down=1 -unit=MINUTES\ndown grace period 1 MINUTES\nrestart grace period 6 HOURS\nmaximum concurrent operations 5\nscan window set to 24 HOURS\nperiod set to 3 MINUTES\n\n\\s qos-scanner pool reset -restart=1 -unit=MINUTES\ndown grace period 1 MINUTES\nrestart grace period 1 MINUTES\nmaximum concurrent operations 5\nscan window set to 24 HOURS\nperiod set to 3 MINUTES\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAny scan operation, however initiated, can by cancelled manually by issuing the\n`pool cancel` command. Verify operations and adjuster tasks can also be cancelled\nindividually (see below).\n\n### Add or remove a storage unit\n\nAdding a storage unit from scratch would involve creating the unit and\nadding it to a unit group.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\sp psu create unit -store resilient-5.dcache-devel-test@enstore\n\\sp psu set storage unit -required=4 resilient-5.dcache-devel-test@enstore\n\\sp psu addto ugroup resilient resilient-5.dcache-devel-test@enstore\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNone of these actions triggers an immediate response from QoS.\n\nIf this is a new group, the appropriate tag must be set on any target\ndirectories for the files that belong to it.\n\nRemoving a unit also will not trigger an immediate response from QoS,\nthough once again the unit will be unregistered internally.\n\n### Modify a storage unit\n\nThere are two possible modifications to a storage unit with consequences for\nQoS. One would be to change the required number of replicas, and the other,\nthe tag constraints. In the first case, copy or caching operations will be\ntriggered according to whether there are now not enough or too many replicas.\nIn the latter, both caching and copies may occur in order to redistribute\nthe existing files to satisfy the new partitioning by pool tag. In both cases,\nall the pools in all the pool groups to which the storage unit is linked will\nbe scheduled for scans.\n\n>   **In the current implementation, removing the '-required' attribute from\n>   a file's storage unit is equivalent to transitioning that file to cached\n>   status.  If the file is ONLINE, however, one replica will remain sticky.**\n>\n>   **This is a setting change which will also affect future files of this\n>   storage unit. There is currently no way (but there will be in the future)\n>   to change replica counts for individual files.**\n>\n>   **Use the QoS transition to change existing disk+tape files to tape.**\n\n### Changing pool tags\n\nIf a pool's tags are used to determine replica distribution (based on the storage\nunit definition of `onlyOneCopyPer`) and these are changed, QoS will not automatically\nforce the pool to be scanned immediately upon restart (it will just be\nscheduled for a restart scan based on the defined grace period).\n\n>   If it is desirable to rescan a pool's replicas immediately after its\n>   tags have been changed, this must be done manually (see above).\n\n### Troubleshooting operations\n\nIntervention to rectify qos handling should hopefully be needed infrequently;\nyet it is not impossible for tasks not to run to completion (for instance, due to\nlost network connections from which QoS could not recover).\n\nThere are several palliatives available short of restarting the domain(s) in these cases.\n\nOne can try to identify which operations are stuck in the RUNNING state for a\nlong period:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-verifier verify ls -lastUpdateBefore=2016/06/17-12:00:00 -state=RUNNING\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf no progress is being made, one can cancel these operations (and allow them to\nbe retried later):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-verifier verify cancel -lastUpdateBefore=2016/06/17-12:00:00 -state=RUNNING\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n>   **Note that on verify cancellation, the operation will attempt the next pass.\n>   To force immediate cancellation of the entire operation, add `-forceRemoval`\n>   to the command.**\n\nIt is also possible to cancel the individual adjuster tasks in the same way.\nBe aware, however, that this will trigger a retry in the verifier (unless\nthe verify operation is canceled with the `-forceRemoval` option).  Cancellation\nof verify operations also occurs programmatically when a pool scan is canceled,\nor when bulk QoS modification requests are canceled in the Bulk service.  In\nboth of the latter cases, removal of the verify operation is forced.\n\nShould none of the operations be making progress, or if there are `WAITING`\noperations but nothing on the `RUNNING` queue, this is symptomatic of a bug, and\nshould be reported to the dCache team.\n\nAnother potentially useful diagnostic procedure is to pause the handling of new\nor deleted file locations in the \"frontend\" (``qos-engine``):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s qos-engine disable\n...\n\\s qos-engine enable\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWithout the `-strict` flag, `disable` stops the message handling, serializing\nthe incoming messages to disk; when re-enabled, the messages are read\nback in and deleted. This may help to determine if, for instance, pool scanning\nis making any progress by itself."], "reference": "The REST API is used in the QoS service to manage file status modifications, either for single files through the REST frontend or for file sets through the bulk service.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
{"user_input": "What is dCache and how does it relate to resilience?", "reference_contexts": ["The Resilience Service\n======================\n\n-----\n[TOC bullet hierarchy]\n-----\n\n## Configuring the resilience service\n\n### Activating resilience\n\nThe service can be run out of the box. All that is required is to include it in\nsome domain.\n\nResilience communicates directly with Chimera, so `chimera.db.host` should be\nset explicitly if Resilience is not running on the same host as the database.\n\n```ini\n[someDomain/resilience]\nchimera.db.host=<host-where-chimera-runs>\n```\n\n### Memory requirements\n\nWhile it is possible to run Resilience in the same domain as other services,\nmemory requirements for resilience handling are fairly substantial, particularly\nfor large installations.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n BEST PRACTICE: We recommend at least 8GB of JVM heap be allocated; 16GB is preferable.\n                Be sure to allow enough memory for the entire domain.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhenever feasible, it is recommended to give Resilience its own domain.\n\n### Some definitions\n\nA **resilient file** is one whose `AccessLatency` is `ONLINE` and which belongs\nto a **resilient storage unit**.\n\nA **resilient storage unit** is one where the `required` (copies) attribute is\nset to a value greater than 1 (the default).\n\nA **resilient storage unit** must be linked to a **resilient pool group** for it\nto be acted upon. When linked to a pool group which is not resilient, the\nstorage unit requirements will be invisible to Resilience.\n\nResilient replicas (belonging to **resilient storage unit**s) will, for this\nreason, only be found on **resilient pools** (belonging to a **resilient pool\ngroup**), though not all files on resilient pools are necessarily resilient (see\nbelow concerning [shared\npools](#pool-sharing)).\n\nNote that a file's `RetentionPolicy` is not limited to `REPLICA` in order for it\nto qualify as resilient; one may have `CUSTODIAL` files which are also given\npermanent on-disk copies (the 'Own Cloud' dCache instance running at DESY uses\nthis configuration to maintain both hard replicas and a tape copy for each\nfile).\n\n### Setting up resilience\n\nTo have a fully functioning resilient installation, you must take the following\nsteps:\n\n1.  Define one or more resilient pool groups.\n\n2.  Define one or more resilient storage units, and link them to a resilient\n    group or groups.\n\n3.  Create the namespace directories where these files will reside, with the\n    necessary tags.\n\n#### Defining a resilient pool group\n\nTo make a pool group resilient, simply add the '`-resilient`' flag; e.g., in\n`poolmanager.conf`:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\npsu create pgroup resilient-group -resilient\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOnce a pool group is defined as resilient, it will become \"visible\" to the\nresilience service.\n\nA pool may belong to **only one** resilient group, though it can belong to any\nnumber of non-resilient groups as well. When the resilience service selects a\nlocation for an additional copy, it does so from within the resilient pool group\nof the file's source location.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWARNING:  Be careful when redefining a pool group by removing its resilient flag.\n          (This is not possible, as a safety precaution, using an admin command;\n          it requires manual modification and reloading of the poolmanager\n          configuration file.)  Doing so makes all files on all pools in that\n          group no longer considered to be resilient;  they will, however, remain\n          indefinitely \"pinned\" or sticky, and thus not susceptible to garbage\n          collection.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nBEST PRACTICE:  Drain pools in a resilient pool group before removing the\n                group's resilience flag.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#### Defining a resilient storage unit\n\nThere are two attributes which pertain to resilience.\n\n1.  `required` defines the number of copies files of this unit should receive\n    (default is 1).\n\n2.  `onlyOneCopyPer` takes a comma-delimited list of pool tag names; it\n    indicates that copies must be partitioned among pools such that each replica\n    has a distinct value for each of the tag names in the list (default is\n    undefined).\n\nTo create, for instance, a resilient storage unit requiring two copies, each on\na different host, one would do something like this:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\npsu create unit -store test:resilient1@osm\npsu set storage unit test:resilient1@osm -required=2 -onlyOneCopyPer=hostname\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis example assumes that the pool layout has also configured the pools with the\nappropriate `hostname` tag and value.\n\n#### Configuration Example\n\nThe following demonstrates the setup for a single resilient pool, pool group and\nstorage unit. The usual procedure for linking pools, pool groups and units\ncontinues to apply.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\npsu create unit -store test:resilient1@osm\npsu set storage unit test:resilient1@osm -required=2 -onlyOneCopyPer=hostname\n\npsu create ugroup resilient1-units\npsu addto ugroup resilient1-units test:resilient1@osm\n\npsu create pool resilient1-pool1\n\npsu create pgroup resilient1-pools -resilient\npsu addto pgroup resilient1-pools resilient1-pool1\n\npsu create link resilient1-link resilient1-units ...\npsu addto link resilient1-link resilient1-pools\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#### Setting the directory tags\n\nTo continue with the above example, the tags which would be minimally required\nin the directories pointing to this storage unit are:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.(tag)(AccessLatency): ONLINE\n.(tag)(sGroup): resilient1\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n### Pool Sharing\n\nIt is possible to share pools between a resilient group and a group backed by an\nHSM. The files can be mixed, and the HSM files will not be replicated. This\ndiagram illustrates a simple example:\n\n![Pool Sharing](images/resilience-pool-sharing.png)\n\nBecause Resilience checks whether the `AccessLatency` attribute is `ONLINE`, the\n`NEARLINE` files belonging to `hsmunit` are ignored. This pertains to pool scans\nas well. Thus 1A, 2A and 3A can contain both replicas and cached copies.\n\n### Cached files on resilient pools\n\nAs of Golden Release 5.2, the semantics of resilience have changed in the\nfollowing ways:\n\n-    the namespace information is not acted upon until a verification\n     is run against the pool repositories supposedly containing the replicas\n     the database has recorded.\n\n-    full information about whether the replica has the system-owned\n     \"sticky\" flag or is just cached is now gathered by resilience.\n\n-    removal of a given replica is done not by setting the repository state\n     to REMOVED, but by removing the \"sticky\" flag bit.\n\nSeveral important advantages result from these changes.\n\n-    Pool sharing no longer carries any restrictions in the sense that\n     if the pool is resilient, one should not manually p2p a replica of\n     a file to it, or that hot replication should be turned off.  Resilience\n     will now handle such situations seamlessly.\n\n-    Allowing the sweeper to remove unnecessary or excess copies by changing\n     permanent replicas into cached ones is inherently safer.  It also means\n     that resilient pools can also be periodically swept for efficiency.\n\n-    A resilient replica which is marked PRECIOUS (the first/new location of a\n     CUSTODIAL ONLINE file which is resilient) will not be selected for\n     removal in the case of excess copies (this was not guaranteed previously).\n\nThere is still one caveat about resilient pools:\n\n> **WARNING**\n>\n> If a pool is part of a resilient pool group, it should NOT carry the\n> large file store configuration\n> ```ini\n> pool.lfs=volatile\n> ```\n>\n> This is because as a resilient pool group member, this pool could be\n> chosen by the Pool Manager as the location of the original (new)\n> copy, in which case the system \"sticky\" flag required for ONLINE\n> files will actually be suppressed.  This will cause resilience to\n> treat this file as \"missing\" (since it ignores cached replicas in\n> its file count), and to send an alarm before any further replicas\n> can be made.\n\n## Resilience home\n\nOn the host where the resilience service is running, you will see several files\nin the `resilience.home` directory (`/var/lib/dcache/resilience` by default):\n\n-   `pnfs-operation-map`\n\n-   `pnfs-backlogged-messages`\n\n-   `pnfs-operation-statistics`\n\n-   `pnfs-operation-statistics-task-{datetime}`\n\n-   `excluded-pools`\n\nExplanations of these files follow.\n\n### pnfs-operation-map\n\nThis is the snapshot of the main operation table. By default, this checkpoint is\nwritten out (in simple text format) every minute. As mentioned above, this\nserves as a heuristic for immediately reprocessing incomplete operations in case\nof a service crash and restart. It is an approximation of current state, so not\nall file operations in progress at that time may have actually been saved, nor\ncompleted ones removed. As previously stated, the periodic pool scan should\neventually detect any missing replicas not captured at restart.\n\nWhen a new snapshot is taken, the old is first renamed to\n`pnfs-operation-map-old`. Similarly, at restart, before being reread, the file\nis renamed to `pnfs-operation-map-reload` to distinguish it from the current\nsnapshot whose processing may overlap with the reloading.\n\n### pnfs-backlogged-messages\n\nIt is possible to enable and disable message handling inside Resilience. There\nis also a short delay at startup which may also accumulate a backlog. Any\nmessages which happen to arrive during this period are written out to this file,\nwhich is then read back in and deleted during initialization. The lifetime of\nthe file, in the latter case, is therefore rather brief, so you will usually not\nsee one in the directory unless you purposely disable message handling.\n\n### pnfs-operation-statistics and pnfs-operation-statistics-task-{datetime}\n\nThrough an admin command, one can enable and disable the recording of\nstatistical data. When activated, two kinds of files are produced. The first is\nan overview of performance with respect to file location messages and\noperations. The second contains detailed, task-by-task records and timings for\neach terminated file operation, written out at 1-minute intervals; this logging\nrolls over every hour until deactivated.\n\nThe overview file can be accessed directly in the admin door via `diag history`:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nCHECKPOINT                   |          NEWLOC        HZ      CHNG |          FILEOP        HZ      CHNG       FAILED |         CHCKPTD\nTue Jun 07 09:22:21 CDT 2016 |        65592310       512     1.94% |        65595972       516     2.88%           23 |              68\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs can be seen, the time of the snapshot is followed by a breakdown based on\narriving (new) locations and triggered file operations; for each, the total\n(since the start of the resilience service) is given, along with the rate\nsampled over the last checkpoint interval, and the percentage change from the\nprevious interval. The number of failures and the entry count for the last\nsnapshot are also reported.\n\nThe hourly task files must be read locally. The format used is as follows:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nPNFSID | STARTED ENDED | PARENT SOURCE TARGET | VERIFY COPY REMOVE | STATUS\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n*Parent* refers to a pool which is being scanned; new files have no parent. For\neach entry, two timings are reported: *verify* refers to the part of the\noperation which determines whether the file needs handling; then either a copy\nor remove timing is given.\n\n### excluded-pools\n\nWhen the command `pool exclude <pools>` is used, the pool operation record is\nmarked as `EXCLUDED`. This can be observed when the `pool ls` command is issued.\nPools so marked are recorded to this file after each pool monitor state update\n(every 30 seconds). This file is read in, again if it exists, at startup, so\nthat previously excluded pools will continue to be treated as such.\n\nFor pool exclusion, see the [section\nbelow](#exclude-a-pool-from-resilience-handling)\nunder typical scenarios.\n\n## Admin Commands\n\nThere are a number of ways to interact with the resilience service through the\nadmin door. As usual, the best guide for each command is to consult its help\ndocumentation (`\\h <command>`).\n\nThese commands will not be discussed in detail here. We instead present a brief\noverview and a few examples of output.\n\n### Available actions\n\n1.  `enable`, `disable [strict]`: Enable and disable Resilience message\n    handling; enable and disable all of Resilience without having to stop the\n    domain.\n\n1.  `async cmd cancel`, `async cmd cleanup`, `async cmd ls`, `async cmd results`:  For management\n    of jobs (inaccessible files, contained-in queries) run asynchronously; allows\n    for listing, cancelation, and removal of entries, and inspection of the\n    contents of the file where results are printed.\n\n1.  `contained in`:  Print to afile the `pnfsids` whose replicas are\n    entirely contained by (i.e., only found on) the locations/pools indicated\n    by the expression.\n\n1.  `inaccessible`: Print to a file the `pnfsids` for a pool which currently\n    have no readable locations.\n\n1.  `file check`: Run a job to adjust replicas for a given file (i.e., make\n    required copies or remove unnecessary ones).\n\n1.  `file ctrl`, `pool ctrl`: Reset properties controlling internal behavior\n    (such as sweep intervals, grace periods, etc.).\n\n1.  `file ls`: List the current file operations, filtered by attributes or\n    state; or just output the count for the given filter.\n\n1.  `file cancel`: Cancel file operations using a filter similar to the list\n    filter.\n\n1.  `pool info`: List pool information derived from the pool monitor.\n\n1.  `pool group info`: List the storage units linked to a pool group and confirm\n    resilience constraints can be met by the member pools.\n\n1.  `pool ls`: List pool operations, filtered by attributes or state.\n\n1. `pool cancel`: Cancel pool operations using a filter similar to the list\n    filter.\n\n1. `pool scan`: Initiate forced scans of one or more pools.\n\n1. `pool include`, `pool exclude`: Set the status of a pool operation to\n    exclude or include it in replica counting and resilience handling.\n\n1. `diag`: Display diagnostic information, including number of messages\n    received, number of operations completed or failed, and their rates; display\n    detailed transfer information by pool and type (copy/remove), with the pool\n    as source and target.\n\n1. `diag history [enable]`: Enable or disable the collection of statistics;\n    display the contents of the diagnostic history file.\n\n1. `history [errors]`: List the most recent file operations which have\n    completed and are now no longer active; do the same for the most recent\n    terminally failed operations.\n\n### Example output\n\nOne of the most useful commands is `diag`, or diagnostics. This can be called\nwith or without a regular expression (indicating pool names):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > diag .*\nRunning since: Fri Jun 03 14:47:40 CDT 2016\nUptime 3 days, 19 hours, 56 minutes, 36 seconds\n\nLast pnfs sweep at Tue Jun 07 10:44:17 CDT 2016\nLast pnfs sweep took 0 seconds\nLast checkpoint at Tue Jun 07 10:43:24 CDT 2016\nLast checkpoint took 0 seconds\nLast checkpoint saved 86 records\n\nMESSAGE                               received  msgs/sec\n    CLEAR_CACHE_LOCATION                    19         0\n    CORRUPT_FILE                             5         0\n    ADD_CACHE_LOCATION                67898905       515\n    POOL_STATUS_DOWN                         0         0\n    POOL_STATUS_UP                         169         0\n\nOPERATION                            completed   ops/sec       failed\n    FILE                              67901908       517           23\n    POOL_SCAN_DOWN                           0         0            0\n    POOL_SCAN_ACTIVE                       228         0            0\n\nTRANSFERS BY POOL                   from              to          failed         size             removed          failed         size\ndmsdca15-1                        959166          962097               0    939.55 KB                 0               0           0\ndmsdca15-1.1                      972972          973463               0    950.65 KB                 0               0           0\ndmsdca15-2                        967710          970284               1    947.54 KB                 0               0           0\ndmsdca15-2.1                      968995          972802               0    950.00 KB                 0               0           0\ndmsdca15-3                        956909          963618               1    941.03 KB                 0               0           0\ndmsdca15-3.1                      970977          973876               0    951.05 KB                 0               0           0\ndmsdca15-4                        959851          959739               0    937.25 KB                 0               0           0\ndmsdca15-4.1                      926010          926611               0    904.89 KB                 0               0           0\n...\nTOTALS                          67901908        67901908              22     64.76 MB                 0               0           0\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAlong with basic uptime information and operation tracking, the pool expression\ndisplays transfer counts by pool and type (copy, remove).\n\nAlso very useful is the file operation list command. This can display the actual\nrecords for the file operations (here given without any filtering):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > file ls\nTue Jun 07 11:16:12 CDT 2016 (0000C0C4B60020C04B94A8570B0594ACD416 REPLICA)(COPY RUNNING)(parent none, count 1, retried 0)\nTue Jun 07 11:16:12 CDT 2016 (00000447C17631D34ACE98EEDD19A97522F4 REPLICA)(COPY RUNNING)(parent none, count 1, retried 0)\nTue Jun 07 11:16:12 CDT 2016 (00001EE0A9AA9449405890401D5CE5931EB7 REPLICA)(COPY RUNNING)(parent none, count 1, retried 0)\nTue Jun 07 11:16:12 CDT 2016 (00006F319230C7464FEE8BC9BBECFD11636E REPLICA)( RUNNING)(parent none, count 1, retried 0)\n�\nTue Jun 07 11:16:12 CDT 2016 (0000C2F0B0C5240F407E964DA748518C106C REPLICA)( RUNNING)(parent none, count 1, retried 0)\nTue Jun 07 11:16:12 CDT 2016 (000092AF5289517D4C029CDFFBF6D92C769B REPLICA)(COPY RUNNING)(parent none, count 1, retried 0)\nTOTAL OPERATIONS:       89\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can also use '`$`' as an argument to list the count:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > file ls $ -state=WAITING\n0 matching pnfsids\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAppending a '`@`' to the dollar sign will display the counts broken down by\nsource pool:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > file ls $@ -state=RUNNING\n152 matching pnfsids.\n\nOperation counts per pool:\n     dmsdca15-1                             5\n     dmsdca21-6.1                           1\n     dmsdca21-8.1                           5\n     dmsdca17-1.1                           3\n     dmsdca18-1.1                           3\n     dmsdca22-8.1                           2\n     dmsdca22-4.1                          19\n     dmsdca22-2.1                          11\n     �\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA similar command, `pool ls`, exists for checking the current status of any pool\nscan operations:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ls dmsdca17\n dmsdca17-2 (completed: 0 / 0 : ?%) - (updated: Mon Jun 06 14:47:33 CDT 2016)(scanned: Mon Jun 06 14:47:33 CDT 2016)(prev ENABLED)(curr ENABLED)(IDLE)\n dmsdca17-1 (completed: 0 / 0 : ?%) - (updated: Mon Jun 06 15:01:55 CDT 2016)(scanned: Mon Jun 06 15:01:55 CDT 2016)(prev ENABLED)(curr ENABLED)(IDLE)\n dmsdca17-1.1   (completed: 0 / 0 : ?%) - (updated: Mon Jun 06 15:20:32 CDT 2016)(scanned: Mon Jun 06 15:20:32 CDT 2016)(prev ENABLED)(curr ENABLED)(IDLE)\n dmsdca17-2.1   (completed: 0 / 0 : ?%) - (updated: Mon Jun 06 17:53:23 CDT 2016)(scanned: Mon Jun 06 17:53:23 CDT 2016)(prev ENABLED)(curr ENABLED)(IDLE)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor each operation, the timestamps of the last update (change in status) and of\nthe last completed scan are indicated, as well as pool status (`ENABLED` here),\nand operation state (`IDLE`, `WAITING`, `RUNNING`). For running scans, the\nnumber of file operations completed out of a total (if known) is reported.\n\nFinally, the two `ctrl` commands can be used to verify or reset basic\nconfiguration values, or to interrupt operation processing or force a sweep to\nrun. Here is the info output for each:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > file ctrl\nmaximum concurrent operations 200.\nmaximum retries on failure 2\nsweep interval 1 MINUTES\ncheckpoint interval 1 MINUTES\ncheckpoint file path /var/lib/dcache/resilience/pnfs-operation-map\nLast pnfs sweep at Tue Jun 07 11:38:28 CDT 2016\nLast pnfs sweep took 0 seconds\nLast checkpoint at Tue Jun 07 11:38:26 CDT 2016\nLast checkpoint took 0 seconds\nLast checkpoint saved 87 records\n\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ctrl\ndown grace period 1 HOURS\nrestart grace period 6 HOURS\nmaximum concurrent operations 5\nscan window set to 24 HOURS\nperiod set to 3 MINUTE\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n## Tuning\n\nOnly a few properties can be reset using the `ctrl` commands shown above. Please\nconsult the documentation in the `resilience.properties` defaults for a fuller\nexplanation of the tuning issues which pertain to resilience. If adjustments to\nthe preset values are made, remember to ensure that enough database connections\nremain available to service both Chimera operations and Resilience operations,\nand that these be properly matched to the number of threads responsible for the\nvarious operations, in order to avoid contention (see again, the explanation in\nthe default properties file).\n\n## Resilience's View of Pool Status\n\nIn order to allow for flexibility in configuring door access to pools, the\n`disabled` state on a pool is interpreted this way:\n\n-  `\\s <pool> pool disable -p2p-client` means no p2p can be written to this pool;\nResilience will not use this pool to make copies, though doors can still write\nnew files there.\n\n-  `\\s <pool> pool disable -store` means doors cannot write new copies to the\npool, though it is still available for p2p; hence Resilience can still use this\npool to make copies.\n\n-  `\\s <pool> pool disable -rdonly` means the pool cannot be written to either\nby doors or Resilience.\n\n-  `\\s <pool> pool disable -strict` indicates not only that the pool is disabled\nfor write, but also for read; is the pool is resilient, Resilience will schedule\nit for a scan so that the resilient files it contains can be replicated elsewhere.\n\n## Automatic Staging of Missing CUSTODIAL Replicas\n\nFiles whose `retention policy` is `CUSTODIAL` and whose `access latency` is `ONLINE`\nwill be handled by Resilience when their replicas are found in a resilient\npool group and their `required` property is defined.\n\nSuch files constitute a special case for the purposes of recovery.  For instance,\nif a normal resilient file with 2 copies on disk becomes inaccessible because\nboth pools containing those two copies went offline at about the same time (not\npermitting Resilience time to react to make another replica), then an alarm\nconcerning this file's current inaccessibility is raised.  If this file is also\n`CUSTODIAL` and has a copy on tertiary storage, resilience will first attempt to\nrestage it before considering it inaccessible.\n\nThere is no special configuration setup to enable this.  Resilience has been\ntested for restaging of this sort even with a setup where there are special\nstaging pools from which the staged-in replica is then to be p2p'd to a resilient\npool group, and works normally in this case (i.e., provided the proper link\nselection preferences are set).\n\n## Handling of QoS Transitions for Resilient Files\n\nAs of dCache 6.1, resilience also handles changes in ACCESS LATENCY for\nfiles that are in resilient pool groups and belong to a storage unit\nwhich has a non-null '-required' attribute.  This means, for instance, that\na change from ONLINE CUSTODIAL to NEARLINE CUSTODIAL will remove the\npermanent pin on all disk copies.  Changes from NEARLINE CUSTODIAL to\nONLINE CUSTODIAL will also be handled in the usual manner by resilience,\nbecause the service now subscribes to attribute change messages broadcast\nby the PnfsManager.\n\n## Some typical scenarios part 1: what happens when ...?\n\n### Resilience is initialized (service start)\n\nShould the resilience service go offline, nothing special occurs when it is\nrestarted. That is, it will simply go through the full re-initialization\nprocedures.\n\nInitialization steps are reported at the logging `INFO` level, and can be\nobserved from the pinboard:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > show pinboard\n11:50:50 AM [pool-9-thread-1] [] Waiting for pool monitor refresh notification.\n11:50:50 AM [pool-9-thread-1] [] Received pool monitor; loading pool information.\n11:50:50 AM [pool-9-thread-1] [] Loading pool operations.\n11:50:50 AM [pool-9-thread-1] [] Pool maps reloaded; initializing ...\n11:50:50 AM [pool-9-thread-1] [] Pool maps initialized; delivering backlog.\n11:50:50 AM [Consumer] [] Backlogged messages consumer exiting.\n11:50:50 AM [pool-9-thread-1] [] Messages are now activated; starting pnfs consumer.\n11:50:50 AM [pool-9-thread-1] [] Pnfs consumer is running; activating admin commands.\n11:50:50 AM [Reloader] [] Done reloading backlogged messages.\n11:50:50 AM [pool-9-thread-1] [] Starting the periodic pool monitor refresh check.\n11:50:50 AM [pool-9-thread-1] [] Updating initialized pools.\n11:50:50 AM [pool-9-thread-1] [] Admin access enabled; reloading checkpoint file.\n11:50:50 AM [pool-9-thread-1] [] Checkpoint file finished reloading.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPoolMonitor state is initially pulled from PoolManager, and thereafter refreshed\nevery 30 seconds by a push from PoolManager to its \"subscribers\" (of which\nResilience is one). Once the initial monitor state is received, pool information\nis parsed and loaded into a local data structure (accessed via the `pool info`\ncommand). The pool operations table is then built (accessed via the `pool ls`\ncommand). The `excluded-pools` file is also reloaded at this point.\n\n#### What exactly does `UNINITIALIZED` mean for a pool?\n\nIn addition to the usual pool status types (`ENABLED`, `READ_ONLY`, `DOWN`),\n`UNINITIALIZED` serves to indicate incomplete information on that pool from the\nPoolMonitor. The transition from `UNINITIALIZED` to another state occurs when\nthe resilience service comes on line, whether simultaneously with pool\ninitialization or not.\n\n>   **The probability of pools going down at initialization is low, but it is\n>   possible that Resilience could go down and then restart to find a number of\n>   pools down. In this case, the** `DOWN` **pools will be handled as usual (see\n>   below). On the other hand, it is preferable not to treat the transition to**\n>   `READ_ONLY`**/**`ENABLED` **as a \"restart\" when coming out of**\n>   `UNINITIALIZED`**, since the majority of pools will most of the time\n>   initialize to a viable readable status, and handling this transition would\n>   unnecessarily provoke an immediate system-wide scan.**\n\nDuring this phase of initialization, message handling is still inactive; hence\nany incoming messages are temporarily cached. When the pool info and operation\ntables have been populated, the backlogged messages are handled. At the same\ntime, message handling is also activated. Up until this point, issuing any admin\ncommands requiring access to Resilience state will result in a message that the\nsystem is initializing:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > diag\nResilience is not yet initialized; use 'show pinboard' to see progress, or 'enable' to re-initialize if previously disabled.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAfter this point, admin commands become viable and the pool information is\nperiodically refreshed (every 30 seconds, in response to the PoolMonitor\nmessage). Changes in pool state or pool group and storage unit composition are\nrecorded and appropriate action, if necessary, is taken.\n\nThe final step in initialization is to reload any operations from the checkpoint\nfile.\n\n>   **This can be a lengthy procedure if the file is large, but should pose no\n>   particular issues, since the system treats the reloaded operations as just\n>   another set of cache location updates and handles them accordingly.**\n\n### A pool goes down or is reported as dead\n\nResilience considers pools viable until they become unreadable. If a pool is\nread-only, its files will still be counted as accessible replicas, and will be\nused as potential sources for copying (naturally, the pool is excluded as target\nfor new copies). Once a pool becomes entirely disabled, Resilience will mark it\nas `DOWN` and queue it for an eventual scan. The scan will be activated as soon\nas the expiration of the grace period for that pool has been detected by the\npool \"watchdog\" (which wakes up every three minutes by default).\n\n>   **When we speak of \"scanning\" a pool which is down or inaccessible, this is\n>   shorthand for saying that Resilience runs a query against the namespace to\n>   find all the** `pnfsids` **which have** `AccessLatency` **=** `ONLINE` **and\n>   a** `location` **(copy) on that pool. No actual interaction with the pool\n>   takes place.**\n\nOnce a scan is completed on a dead pool, no more scans will be done on it until\nits state changes.\n\nUnder most circumstances, no intervention should be required. This is part of\nthe normal functioning of the service.\n\n### A pool is re-enabled or brought back on line\n\nThis is just the counterpart to the previous scenario. When a pool is brought\nback on line, Resilience queues it for eventual scanning, once again activated\nas soon as the expiration of the grace period is detected. Note that the grace\nperiods for pool down and pool restart are independent properties.\n\nIf a scan on the pool is currently running when the pool comes back up, it will\nbe immediately canceled and a new scan rescheduled.\n\nNote that Resilience always does the same thing on a scan, whether triggered by\na change from `UP` to `DOWN` or vice versa: it checks the countable replicas of\na file, compares them to the required number, and takes the appropriate action\n(either copy or remove).\n\n### Several pools go off line simultaneously\n\nEach will be treated as above. By default, five scans are allowed to run\nsimultaneously. If there are more scans than five, they will continue in the\n`WAITING` state until a slot becomes available.\n\nScans can take some time to complete, depending on how big the pool is. If there\nis a `WAITING` scan which has not run yet when the periodic window for the scan\nexpires, it will simply remain queued until it finally runs.\n\n#### Why don't I see a value for the 'total' on the admin `pool ls` output for a `RUNNING` pool operation?\n\nWhen a pool is scanned, a query is run against Chimera to collect all the\n`pnfsids` associated with `ONLINE` files on the given pool. The scanning\nprocesses the results from the query via a cursor, so each file is checked and\nqueued for treatment if necessary, in order. This looping through the results\ncan very easily take longer than it does for some of the operations it has\ncreated to be run and to complete. But the total number of files to process is\nonly known at the end of this loop. So it is possible to see a value for the\nnumber of files processed so far before seeing the total. Obviously, only when\nthe total exists can the % value be computed.\n\n## Some typical scenarios part 2: how do I ...?\n\nDuring the normal operation of a resilient installation of dCache, changes to\nthe number of pools, along with the creation and removal of storage classes or\npool groups, will undoubtedly be necessary. The following describes the steps to\ntake and what response to expect from the resilience service in each case.\n\n### Add a pool to a resilient group\n\nLet us assume that we have some new disk space available on node `dmsdca24`, and\nthat we want to use it to host some pools. Of course, the first steps are to\nprepare the appropriate partitions for the pools and to create the pool area\n(directory) with the necessary setup file for dCache (see \"Creating and\nconfiguring pools\" in the [Configuring\ndCache](https://www.dcache.org/manuals/Book-2.16/Book-fhs.shtml#in-install-configure)\nsection of the dCache Book). Once we have done that, and have also added the\npool stanza to the layout file\n[(ibid)](https://www.dcache.org/manuals/Book-2.16/Book-fhs.shtml#in-install-configure),\nwe can proceed to add the pool to the `psu` (PoolSelectionUnit):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (PoolManager@dCacheDomain) admin > \\c PoolManager\npsu create pool rw-dmsdca24-1 -disabled\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe can then start the pool on the host. It should then appear enabled:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (PoolManager@dCacheDomain) admin > psu ls -a  pool rw-dmsdca24-1\nrw-dmsdca24-1  (enabled=false;active=15;rdOnly=false;links=0;pgroups=0;hsm=[];mode=enabled)\n  linkList   :\n  pGroupList :\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLet us now say that we want to make this a resilient pool by adding it to a\nresilient pool group.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (PoolManager@dCacheDomain) admin > psu addto pgroup res-group rw-dmsdca24-1\n\n[fndcatemp2] (Resilience@resilienceDomain) admin > \\sp psu ls -a pool rw-dmsdca24-1\nrw-dmsdca24-1  (enabled=false;active=19;rdOnly=false;links=0;pgroups=1;hsm=[];mode=enabled)\n  linkList   :\n  pGroupList :\n    res-group(links=1; pools=35; resilient=true)\n\n[fndcatemp2] (PoolManager@dCacheDomain) admin > psu ls -a pgroup res-group\nres-group\n resilient = true\n linkList :\n   res-link  (pref=10/10/-1/10;;ugroups=3;pools=1)\n poolList :\n   dmsdca15-1  (enabled=true;active=14;rdOnly=false;links=0;pgroups=1;hsm=[enstore];mode=enabled)\n   dmsdca15-1.1  (enabled=true;active=10;rdOnly=false;links=0;pgroups=1;hsm=[enstore];mode=enabled)\n   ...\n   dmsdca22-8.1  (enabled=true;active=7;rdOnly=false;links=0;pgroups=1;hsm=[enstore];mode=enabled)\n   rw-dmsdca24-1  (enabled=false;active=5;rdOnly=false;links=0;pgroups=1;hsm=[];mode=enabled)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAfter the next monitor refresh, Resilience should show this pool as well:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool info rw-dmsdca24-1\nkey           84\nname          rw-dmsdca24-1\ntags          {hostname=dmsdca24.fnal.gov, rack=24-1}\nmode          enabled\nstatus        ENABLED\nlast update   Thu Jun 16 09:24:21 CDT 2016\n\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ls rw-dmsdca24-1\nrw-dmsdca24-1   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 09:25:42 CDT 2016)(scanned: Thu Jun 16 09:25:42 CDT 2016)(prev ENABLED)(curr ENABLED)(IDLE)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen the pool is added, a scan is scheduled for it, provided it is in an\ninitialized state. In this case, since the pool is empty, the scan completes\nquickly.\n\n### Remove a pool from a resilient group\n\nWhen a pool is removed from a resilient group, the pool needs to be scanned,\nbecause the `ONLINE` files it contains constitute replicas, and as such, the\nrespective counts for each will be diminished by 1.\n\nHere we walk through several steps to show Resilience put through its paces.\n\nFirst, let us remove a pool from a resilient pool group.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (PoolManager@dCacheDomain) admin > psu removefrom pgroup res-group rw-dmsdca24-2\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe observe the pool has been queued for a scan:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ls rw-\nrw-dmsdca24-2   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:17:09 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev ENABLED)(curr ENABLED)(WAITING)\nrw-dmsdca24-3   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:14:33 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev UNINITIALIZED)(curr ENABLED)(IDLE)\nrw-dmsdca24-4   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:14:33 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev UNINITIALIZED)(curr ENABLED)(IDLE)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBecause we are impatient and don't want to wait 3 minutes, let's wake up the\nsleeping watchdog:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ctrl run\nForced watchdog scan.\n\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ls rw-dmsdca24-2\nrw-dmsdca24-2   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:17:19 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev ENABLED)(curr ENABLED)(RUNNING)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nShortly thereafter, we can see it is doing some work:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ls rw-dmsdca24-2\nrw-dmsdca24-2   (completed: 491 / ? : ?%) - (updated: Thu Jun 16 17:17:19 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev ENABLED)(curr ENABLED)(RUNNING)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBecause this is a demo pool, there aren't many files, so the scan completes\nnearly simultaneously with the actual copying; hence there is not enough of a\ntime lag to see a \"% complete\" reported.\n\nUpon termination, this pool is no longer resilient; as such, there should no\nlonger be a record for it in the pool operation table; and indeed, only the\nother two \"rw-\" pools still appear:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ls rw-\nrw-dmsdca24-3   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:14:33 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev UNINITIALIZED)(curr ENABLED)(IDLE)\nrw-dmsdca24-4   (completed: 0 / 0 : ?%) - (updated: Thu Jun 16 17:14:33 CDT 2016)(scanned: Thu Jun 16 17:14:33 CDT 2016)(prev UNINITIALIZED)(curr ENABLED)(IDLE)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNow, just to spot check that everything is right with the world, let's examine\nthe recent operations, pick a `pnfsid`, and find its locations (replicas).\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n [fndcatemp2] (Resilience@resilienceDomain) admin > history\n � [many other files]\n Thu Jun 16 17:17:23 CDT 2016 (00008F22B12729CE458596DE47E00411D68C REPLICA)(COPY DONE)(parent 20, retried 0)\n � [many other files]\n\n [fndcatemp2] (Resilience@resilienceDomain) admin > \\sn cacheinfoof 00008F22B12729CE458596DE47E00411D68C\n dmsdca15-1 dmsdca15-1.1 dmsdca16-2.1 rw-dmsdca24-2\n\n [fndcatemp2] (Resilience@resilienceDomain) admin > \\sn storageinfoof 00008F22B12729CE458596DE47E00411D68C\n size=1;new=false;stored=false;sClass=resilient-4.dcache-devel-test;cClass=-;hsm=enstore;accessLatency=ONLINE;retentionPolicy=REPLICA;path=<Unknown>;group=resilient-4;family=dcache-devel-test;bfid=<Unknown>;volume=<unknown>;location=<unknown>;\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe storage class/unit for this file (`resilient-4.dcache-devel-test@enstore`)\nhappens to require three copies; there are indeed three, plus the no-longer\nvalid location which we just removed from resilience handling.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nNOTE:     removing a pool from a resilient pool group will not automatically\n          delete the files it contains from the pool, nor will it remove\n          the system-owned sticky bits for these files (such behavior\n          would be unsafe).\n\nWARNING:  if a pool is moved from one resilient pool group to another, the\n          replica counts for the added files will be seen in the context\n          of the new group, leading to the creation of the required number of\n          copies on the pools of the new group (without affecting the previous copies).\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe removal of a pool from the `psu` using `psu remove pool <name>` will also\nremove the pool from the pool info listing in Resilience; however, if the pool\nkeeps connecting (i.e., it has not been stopped), its entry will continue to\nappear in the PoolMonitor, and thus also in the internal resilience table.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWARNING:  do NOT remove the pool from the psu using 'psu remove pool <name>'\n          until the scan of the pool has completed. Otherwise, its reference\n          will no longer be visible to Resilience, and the scan will partially\n          or completely fail.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOne could also disable the pool, let Resilience handle it as a `DOWN` pool, and\nthen when the scan completes, remove it from the pool group. The removal in this\ncase should not trigger an additional scan, since the pool is `DOWN` and has\nalready been processed. Thus this sequence would also work:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > \\s rw-dmsdca24-2 pool disable -strict\n...\n(after the DOWN scan has completed)\n...\n[fndcatemp2] (PoolManager@dCacheDomain) admin > psu removefrom pgroup res-group rw-dmsdca24-2\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n>   **As long as at least one replica of a file continues to reside in a\n>   resilient pool group, action will be taken by Resilience (if possible) to\n>   maintain the proper number of replicas for that file inside the pool group\n>   (a single but currently unreadable copy will raise an alarm, however). Once\n>   all pools containing replicas of a given file have been removed from the\n>   pool group, however, Resilience becomes agnostic concerning the number of\n>   copies of that file (the file becomes \"invisible\" to Resilience).**\n\n>   **Because of the likelihood of \"orphaned\" files (files all of whose replicas\n>   are inaccessible) when removing or draining pools concurrently, it is\n>   recommended that resilient pools be decommissioned/removed serially while\n>   resilience is running.  If concurrent draining is necessary, then the\n>   `contained in` command may come in useful.  Running that command over\n>   the set of pools to be drained and removed will provide a list of\n>   pnfsids for which some sort of manual action/migration will be necessary,\n>   since they will be alarmed as 'inaccessible' by resilience.**\n\n### Add or remove a resilient group\n\nThere is nothing special about adding or removing a resilient group. Doing so\nwill register the group inside Resilience as well:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > \\sp psu create pgroup r-test-group -resilient\n\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool group info r-test-group\nName         : r-test-group\nKey          : 12\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\neven though at this point it is empty. Once pools are added to this group, the\nbehavior will be as indicated above. To remove the group, remove all the pools\nfirst, let their scans complete, and then remove the group. (See below for how\nto override the grace periods for pool status changes in order to force the scan\nto run.)\n\n>   **As mentioned above (see the warning under \"Defining a resilient group\"),\n>   it is not possible to use the admin shell to demote a resilient pool group\n>   to non-resilient status.**\n\n### Exclude a pool from resilience handling\n\nDuring normal operation, the resilience service should be expected to handle\ngracefully situations where a pool with many files, for one reason or another,\ngoes offline. Such an incident, even if the \"grace period\" value were set to 0,\nin initiating a large scan, should not bring the system down. Obversely, should\nthe pool come back on line, any such scan should be (and is) immediately\ncanceled, and the pool rescheduled for a scan to remove unnecessary copies. So\nunder most circumstances, no manual intervention or adjustment in this regard\nshould be required.\n\nNevertheless, temporary exclusion of one or more pools from being handled by the\nresilience service may be desirable in a few situations.\n\nThe old Replica Manager provided for marking pools `OFFLINE` (a state distinct\nfrom `DOWN` or `disabled`). Such pools were exempted from pool state change\nhandling, but their files continued to \"count\" as valid replicas.\n\nThis feature has been held over in the new resilience service.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s Resilience pool exclude <regular expression>\n\\s Resilience pool include <regular expression>\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#### When **NOT** to use `pool exclude`\n\nThe `pool exclude` command interferes with the normal operation of Resilience;\nuse in the wrong circumstances may easily lead to inconsistent state.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWARNING:  Only use 'pool exclude' for temporary situations where the intention\n          is eventually to restore the excluded location(s) to resilience management;\n          or when the locations on those pools are actually being migrated or\n          deleted from the namespace.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf, for instance, one set a pool to `EXCLUDED`, then removed the pool from a\nresilient group, the pool would disappear from the pool operation list (`pool\nls`), but its replicas would still be counted by Resilience. One would then have\nto repair the situation manually, either by adding the pool back into the\nresilient group and then removing it correctly, or by manually deleting or\nmigrating the files.\n\n`pool exclude` is useful for doing manual migrations on resilient pool groups,\nbut caution should be taken when applying it.\n\n_Note that once a pool is excluded, it can no longer be scanned, even manually,\nuntil it is explicitly included again._\n\n### Rebalance or migrate a resilient pool (group)\n\nRebalancing should be required less often on resilient pools; but if you should\ndecide to rebalance a resilient pool group, or need to migrate files from one\npool group to another, be sure to disable resilience on all those pools. One\ncould do this by stopping resilience altogether,\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s Resilience disable strict\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nbut this of course would stop the processing of other resilient groups not\ninvolved in the operation. The alternative is to use the `exclude` command one\nor more times with expressions matching the pools you are interested in:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s Resilience pool exclude <exp1>\n\\s Resilience pool exclude <exp2>\n...\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNote that the exclusion of a pool inside Resilience will survive a restart of\nthe service because excluded pools are written out to a file (`excluded-pools`;\nsee above) which is read back in on initialization.\n\nWhen rebalancing or migration is completed, pools can be set back to active\nresilience control:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\s Resilience pool include .*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n(Note that only `EXCLUDED` pools are affected by the `include` command.)\nRe-inclusion sets the pools back to `IDLE`, and does not schedule them\nautomatically for a scan, so if you wish to scan these pools before the periodic\nwindow elapses, a manual scan is required.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nBEST PRACTICE:  Disable resilience on the potential source and target pools\n                by setting them to EXCLUDED before doing a rebalance\n                or migration.\n\n                An alternative to this would be to remove all the pools\n                from the pool group(s) in question, and then add them back\n                afterwards.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n### Manually schedule or cancel a pool scan\n\nA scan can be manually scheduled for any resilient pool, including those in the\n`DOWN` or `EXCLUDED` states.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool scan dmsdca18-2\nScans have been issued for:\ndmsdca18-2\ndmsdca18-2.1\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNote that if a pool is in the `WAITING` state as a result of a pool status\nchange from `UP` to `DOWN` or vice versa, calling `pool scan` on it will\noverride the grace period wait so that it will begin to run the next time the\nwatchdog wakes up.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nNOTE:  One can override the grace period for a waiting scan by calling 'pool scan'.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOne could of course also change the global setting for the grace periods using\n`pool ctrl`:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ctrl\ndown grace period 6 HOURS\nrestart grace period 6 HOURS\nmaximum concurrent operations 5\nscan window set to 24 HOURS\nperiod set to 3 MINUTES\n\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ctrl reset -down=1 -unit=MINUTES\ndown grace period 1 MINUTES\nrestart grace period 6 HOURS\nmaximum concurrent operations 5\nscan window set to 24 HOURS\nperiod set to 3 MINUTES\n\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool ctrl reset -restart=1 -unit=MINUTES\ndown grace period 1 MINUTES\nrestart grace period 1 MINUTES\nmaximum concurrent operations 5\nscan window set to 24 HOURS\nperiod set to 3 MINUTES\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAny scan operation, however initiated, can by cancelled manually by issuing the\n`pool cancel` command. This has a number of options, but perhaps the most\nimportant is `-includeChildren`; this indicates that, aside from resetting to\nidle the pool operation in question (and cancelling the underlying database\nquery if it is still running), all of the incomplete child (i.e., file)\noperations scheduled thus far will also be immediately cancelled and removed\nfrom the file operation table. File operations can also be cancelled\nindividually (see below).\n\n### Add or remove a resilient storage unit\n\nAdding a resilient storage unit from scratch would involve creating the unit and\nadding it to a unit group.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\sp psu create unit -store resilient-5.dcache-devel-test@enstore\n\\sp psu set storage unit -required=4 resilient-5.dcache-devel-test@enstore\n\\sp psu addto ugroup resilient resilient-5.dcache-devel-test@enstore\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNone of these actions triggers an immediate response from Resilience, though the\nunit does show up as registered:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool group info -showUnits res-group\nName         : res-group\nKey          : 9\n    resilient-0.dcache-devel-test@enstore\n    resilient-4.dcache-devel-test@enstore\n    resilient-1.dcache-devel-test@enstore\n    resilient-5.dcache-devel-test@enstore\n    resilient-3.dcache-devel-test@enstore\n    resilient-2.dcache-devel-test@enstore\n\n[fndcatemp2] (Resilience@resilienceDomain) admin > pool group info -showUnits rss-group\nName         : rss-group\nKey          : 11\n    resilient-0.dcache-devel-test@enstore\n    resilient-4.dcache-devel-test@enstore\n    resilient-1.dcache-devel-test@enstore\n    resilient-5.dcache-devel-test@enstore\n    resilient-3.dcache-devel-test@enstore\n    resilient-2.dcache-devel-test@enstore\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf this is a new group, the appropriate tag must be set on any target\ndirectories for the files that belong to it.\n\nRemoving a unit also will not trigger an immediate response from Resilience,\nthough once again the unit will be unregistered internally.\n\n### Modify a resilient storage unit\n\nThere are two possible modifications to a resilient storage unit. One would be\nto change the required number of replicas, and the other, the tag constraints.\nIn the first case, copy or remove operations will be triggered according to\nwhether there are now not enough or too many replicas. In the latter, both\nremoves and copies may occur in order to redistribute the existing files to\nsatisfy the new partitioning by pool tag. In both cases, all the pools in all\nthe pool groups to which the storage unit is linked will be scheduled for scans.\n\n### Troubleshooting file operations\n\nIntervention to rectify resilience handling should hopefully be needed\ninfrequently; yet it is not impossible for copy or remove jobs not to run to\ncompletion (for instance, due to lost network connections from which Resilience\ncould not recover).\n\nThere are several palliatives available short of restarting the Resilience\ndomain in these cases.\n\nOne can try to identify which operations are stuck in the RUNNING state for a\nlong period:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nfile ls -lastUpdateBefore=2016/06/17-12:00:00 -state=RUNNING\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf no progress is being made, one can cancel these operations (and allow them to\nbe retried later):\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nfile cancel -lastUpdateBefore=2016/06/17-12:00:00 -state=RUNNING\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n>   **Note that with file cancellation, if the operation count is more than 1\n>   (meaning it is scheduled to make or remove more than once), the operation\n>   will attempt the next pass. To force immediate cancellation of the entire\n>   operation for such cases, add** `-forceRemoval` **to the command.**\n\nShould none of the file operations be making progress, or if there are `WAITING`\noperations but nothing on the `RUNNING` queue, this is symptomatic of a bug, and\nshould be reported to the dCache team. One could try to restart the consumer in\nthis case:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nfile ctrl shutdown\nfile ctrl start\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nbut the bug may present itself again.\n\nThe above `ctrl` command is nevertheless useful for attempting recovery without\nstopping all of Resilience; one can similarly start and stop the pool operation\nconsumer using `pool ctrl`. The resilience service as a whole can also be\nstopped and restarted (should you be running other services in the same domain)\nusing `disable -strict` and `enable`.\n\nAnother potentially useful diagnostic procedure is to pause the handling of new\nor deleted file locations:\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ndisable\n...\nenable\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWithout the `-strict` flag, `disable` stops the message handling, passing off\nthe incoming messages to a file cache; when re-enabled, the messages are read\nback in. This may help to determine if, for instance, pool scanning is making\nany progress by itself.\n"], "reference": "dCache is a storage system that provides resilience through its service, which manages the replication and availability of files across different storage units. It ensures that files are stored in a resilient manner, meaning that they are replicated in multiple locations to prevent data loss.", "synthesizer_name": "single_hop_specifc_query_synthesizer"}
